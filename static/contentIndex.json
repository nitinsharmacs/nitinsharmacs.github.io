{"blogs/index":{"title":"index","links":[],"tags":[],"content":"This is the home page for bitPhile. You will find most of the informative content here."},"blogs/tech/docker/docker-compose-todo-app":{"title":"Docker Compose Todo App","links":["blogs/tech/docker/docker-compose"],"tags":["docker-compose","compose","microservices","networks"],"content":"Greetings, esteemed readers! It is an absolute pleasure to see you all here. I trust that each one of you is in the pink of health and high spirits. In this post, we are going to containerize our todo application using docker compose. If you want to learn about docker compose, refer my post on Docker Compose. Let’s hit the road!\nWhat is there to Cover?\nWe will be covering the following sections here.\n\nSetting up the project\nCreate Dockerfile for the backend\nCreate Dockerfile for the frontend\nCreate a docker compose\nSpin up the compose\nAn introduction to deploys\nConclusion\n\nLet’s follow through with each one of these sections.\nSetting up the application\nPrerequisites\n\nnode\nnpm\nmysql\nand docker of course\n\nCreate your sandbox directory where you would be writing the code. Clone the following project in your sandbox.\n# ssh\n⟩ git clone git@github.com:nitinsharmacs/bitphile-todo-app-docker-compose.git\n \n# https\n⟩ git clone github.com/nitinsharmacs/bitphile-todo-app-docker-compose.git\nListing down the contents of the project, we will see.\n⟩ tree                                                                   (base)\n.\n├── README.md\n├── backend\n│   ├── README.md\n│   ├── index.js\n│   ├── package-lock.json\n│   └── package.json\n└── frontend\n    ├── README.md\n    ├── package-lock.json\n    ├── package.json\n    ├── public\n    │   ├── favicon.ico\n    │   ├── index.html\n    │   ├── logo192.png\n    │   ├── logo512.png\n    │   ├── manifest.json\n    │   └── robots.txt\n    └── src\n        ├── App.css\n        ├── App.js\n        ├── App.test.js\n        ├── components\n        │   ├── CreateTodo.jsx\n        │   ├── Header.jsx\n        │   └── TodoItems.jsx\n        ├── index.css\n        ├── index.js\n        ├── logo.svg\n        ├── reportWebVitals.js\n        └── setupTests.js\n \n5 directories, 25 files\nInstall dependencies in each of those directories.\n⟩ cd frontend\n⟩ npm install\n⟩ cd ../backend\n⟩ npm install\nNow, start both the backend and frontend.\nfrontend\n⟩ npm start\n \nbackend\n⟩ npm start\n\nFacing issues with MySQL db? Please make sure it is running. Also, provide environment variables, DB_USER and DB_PASSWORD while running the backend.\n\nOpen localhost:3000, and you should see an ugly page without CSS. Let’s not worry about the styling of the page.\n\nYou should be able to add a to-do.\nCreate Dockerfile for the backend\nLet’s write Dockerfile for the backend.\nFROM node:12-alpine\n \nCOPY package.json package.json\n \nRUN npm install\n \nENV NODE_ENV=production\n \nCOPY index.js index.js\n \nCMD [&quot;npm&quot;, &quot;start&quot;]\n \nEXPOSE 3001\nCopy and paste this content inside backend/Dockerfile.\nNow, build the image.\n⟩ docker build -t bitphile/todo-backend .\nLet’s try running the image we’ve just built.\n⟩ docker run -d --name todo-backend -p 3001:3001 bitphile/todo-backend\n68dc2b21f5e2b8a6d54c35dc46a56f6d0e117793a2a7c1e80dab42ae91826f93\nSeeing the logs of the container,\n⟩ docker logs -f 68dc2b21f5e2b8a6d54c35dc46a56f6d0e117793a2a7c1e80dab42ae91826f93\n \n&gt; backend@1.0.0 start /\n&gt; node .\n \n(node:18) UnhandledPromiseRejectionWarning: Error: getaddrinfo EAI_AGAIN mysql-server\n    at Object.createConnection (/node_modules/mysql2/promise.js:242:31)\n    at Object.&lt;anonymous&gt; (/index.js:54:4)\n    at Module._compile (internal/modules/cjs/loader.js:999:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10)\n    at Module.load (internal/modules/cjs/loader.js:863:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:708:14)\n    at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:60:12)\n    at internal/main/run_main_module.js:17:47\n(node:18) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)\n(node:18) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\n \n🤯 We are bombarded with an error. If we see it, it is crying because of the MySQL connection. Our container is not able to connect to the MySQL server. And this is straightforward as no MySQL container is running in the network. We will look at it later when we compose our application. For now, let’s move ahead.\nCreate Dockerfile for frontend\nCopy the following Dockerfile content inside frontend/Dockerfile\nFROM node:12-alpine AS builder\n \nWORKDIR /app\n \nCOPY package.json package.json\n \nRUN npm install\n \nCOPY . .\n \nRUN npm run build\n \nFROM nginx:alpine\n \nWORKDIR /usr/share/nginx/html\n \nRUN rm -rf ./*\n \nCOPY --from=builder /app/build .\n \nENTRYPOINT [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]\n\nOption daemon off specifies nginx to run in the foreground so that the container will continue running. If we don’t specify this option, the nginx service would run in the background and the container will stop running as there is nothing to hold it up.\n\nLet’s build the image.\n⟩ docker build -t bitphile/todo-frontend .                               (base)\nSending build context to Docker daemon  478.7kB\nStep 1/11 : FROM node:12-alpine AS builder\n ---&gt; bb6d28039b8c\nStep 2/11 : WORKDIR /app\n ---&gt; Using cache\n ---&gt; 8610e0568d30\nStep 3/11 : COPY package.json package.json\n ---&gt; 2cf4f42b3f97\nStep 4/11 : RUN npm install\n ---&gt; Running in 0606b5b332ec\nperformance.now() and performance.timeOrigin.\n \n&gt; core-js@3.29.1 postinstall /app/node_modules/core-js\n&gt; node -e &quot;try{require(&#039;./postinstall&#039;)}catch(e){}&quot;\n \nThank you for using core-js ( github.com/zloirock/core-js ) for polyfilling JavaScript standard library!\n \nThe project needs your help! Please consider supporting of core-js:\n&gt; opencollective.com/core-js\n&gt; patreon.com/zloirock\n&gt; boosty.to/zloirock\n&gt; bitcoin: bc1qlea7544qtsmj2rayg0lthvza9fau63ux0fstcz\n \nI highly recommend reading this: github.com/zloirock/core-js/blob/master/docs/2023-02-14-so-whats-next.md\n \n \n&gt; core-js-pure@3.29.1 postinstall /app/node_modules/core-js-pure\n&gt; node -e &quot;try{require(&#039;./postinstall&#039;)}catch(e){}&quot;\n \nThank you for using core-js ( github.com/zloirock/core-js ) for polyfilling JavaScript standard library!\n \nThe project needs your help! Please consider supporting of core-js:\n&gt; opencollective.com/core-js\n&gt; patreon.com/zloirock\n&gt; boosty.to/zloirock\n&gt; bitcoin: bc1qlea7544qtsmj2rayg0lthvza9fau63ux0fstcz\n \nI highly recommend reading this: github.com/zloirock/core-js/blob/master/docs/2023-02-14-so-whats-next.md\n \nadded 1489 packages from 684 contributors and audited 1490 packages in 69.985s\n \n233 packages are looking for funding\n  run `npm fund` for details\n \nfound 1 high severity vulnerability\n  run `npm audit fix` to fix them, or `npm audit` for details\nRemoving intermediate container 0606b5b332ec\n ---&gt; e84821d3c273\nStep 5/11 : COPY . .\n ---&gt; 2582c9e1ccf7\nStep 6/11 : RUN npm run build\n ---&gt; Running in 24a2880afcea\n \n&gt; todo-app@0.1.0 build /app\n&gt; react-scripts build\n \nCreating an optimized production build...\nCompiled successfully.\n \nFile sizes after gzip:\n \n  47.56 kB  build/static/js/main.bf9caf07.js\n  1.78 kB   build/static/js/787.2843ca88.chunk.js\n  264 B     build/static/css/main.e6c13ad2.css\n \nThe project was built assuming it is hosted at /.\nYou can control this with the homepage field in your package.json.\n \nThe build folder is ready to be deployed.\nYou may serve it with a static server:\n \n  npm install -g serve\n  serve -s build\n \nFind out more about deployment here:\n \n  cra.link/deployment\n \nRemoving intermediate container 24a2880afcea\n ---&gt; ca6123013213\nStep 7/11 : FROM nginx:alpine\n ---&gt; 2bc7edbc3cf2\nStep 8/11 : WORKDIR /usr/share/nginx/html\n ---&gt; Using cache\n ---&gt; 14150ff7fb4b\nStep 9/11 : RUN rm -rf ./*\n ---&gt; Using cache\n ---&gt; dec51b86101d\nStep 10/11 : COPY --from=builder /app/build .\n ---&gt; bfc76e5e13ec\nStep 11/11 : ENTRYPOINT [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]\n ---&gt; Running in c01fb95e9ca3\nRemoving intermediate container c01fb95e9ca3\n ---&gt; 9ad959056d7a\nSuccessfully built 9ad959056d7a\nSuccessfully tagged bitphile/todo-frontend:latest\n \nLet’s run the image.\n⟩ docker run -d --name todo-frontend -p 4000:80 bitphile/todo-frontend\nNow, if we open http://localhost:4000, we should the to-do page.\nCreate Docker Compose file\nIt is time to take the whole picture of our application. To run our application with all those services, we have to create a docker-compose.yml file. Let’s do create one.\nversion: &#039;3.7&#039;\nname: todo-app\n \nservices:\n  todo-frontend:\n    container_name: todo-frontend\n    build: ./frontend\n    ports:\n      - 4000:80\n    networks:\n      - todo-app\n    depends_on:\n      - todo-backend\n \n  todo-backend:\n    container_name: todo-server\n    build: ./backend\n    ports:\n      - 3001:3001\n    networks:\n      - todo-app\n    environment:\n      - NODE_ENV=production\n      - DB_USER=root\n      - DB_PASSWORD=password\n    depends_on:\n      - mysql\n \n  mysql:\n    container_name: mysql-server\n    image: mysql:5.7\n    volumes:\n      - ./mysql:/var/lib/mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: password\n      MYSQL_DATABASE: todo_app\n    networks:\n      - todo-app\n \nnetworks:\n  todo-app:\n \nLet’s do compose up,\n⟩ docker compose up -d                                                   (base)\n[+] Building 83.3s (22/22) FINISHED\n =&gt; [todo-app_todo-frontend internal] load build definition from Dockerfi  0.0s\n =&gt; =&gt; transferring dockerfile: 305B                                       0.0s\n =&gt; [todo-app_todo-backend internal] load build definition from Dockerfil  0.0s\n =&gt; =&gt; transferring dockerfile: 191B                                       0.0s\n =&gt; [todo-app_todo-frontend internal] load .dockerignore                   0.0s\n =&gt; =&gt; transferring context: 61B                                           0.0s\n =&gt; [todo-app_todo-backend internal] load .dockerignore                    0.0s\n =&gt; =&gt; transferring context: 60B                                           0.0s\n =&gt; [todo-app_todo-frontend internal] load metadata for docker.io/library  0.0s\n =&gt; [todo-app_todo-backend internal] load metadata for docker.io/library/  0.0s\n =&gt; CACHED [todo-app_todo-frontend builder 1/6] FROM docker.io/library/no  0.0s\n =&gt; [todo-app_todo-frontend stage-1 1/4] FROM docker.io/library/nginx:alp  0.0s\n =&gt; [todo-app_todo-frontend internal] load build context                   0.0s\n =&gt; =&gt; transferring context: 458.37kB                                      0.0s\n =&gt; CACHED [todo-app_todo-frontend stage-1 2/4] WORKDIR /usr/share/nginx/  0.0s\n =&gt; [todo-app_todo-frontend stage-1 3/4] RUN rm -rf ./*                    0.5s\n =&gt; CACHED [todo-app_todo-frontend builder 2/6] WORKDIR /app               0.0s\n =&gt; [todo-app_todo-frontend builder 3/6] COPY package.json package.json    0.0s\n =&gt; [todo-app_todo-backend internal] load build context                    0.0s\n =&gt; =&gt; transferring context: 1.60kB                                        0.0s\n =&gt; [todo-app_todo-backend 2/4] COPY package.json package.json             0.0s\n =&gt; [todo-app_todo-frontend builder 4/6] RUN npm install                  69.0s\n =&gt; [todo-app_todo-backend 3/4] RUN npm install                            5.9s\n =&gt; [todo-app_todo-backend 4/4] COPY index.js index.js                     0.0s\n =&gt; [todo-app_todo-frontend] exporting to image                            0.2s\n =&gt; =&gt; exporting layers                                                    0.0s\n =&gt; =&gt; writing image sha256:c30480a00b3ca5464f566693178dd5ae117d60a3a62e5  0.0s\n =&gt; =&gt; naming to docker.io/library/todo-app_todo-backend                   0.0s\n =&gt; =&gt; writing image sha256:28501b3d67ba977a812cfc9e21e91279e52765dc1276a  0.0s\n =&gt; =&gt; naming to docker.io/library/todo-app_todo-frontend                  0.0s\n =&gt; [todo-app_todo-frontend builder 5/6] COPY . .                          0.0s\n =&gt; [todo-app_todo-frontend builder 6/6] RUN npm run build                13.5s\n =&gt; [todo-app_todo-frontend stage-1 4/4] COPY --from=builder /app/build .  0.0s\n[+] Running 4/4\n ⠿ Network todo-app_todo-app  Created                                      0.0s\n ⠿ Container mysql-server     Started                                      0.7s\n ⠿ Container todo-server      Started                                      1.1s\n ⠿ Container todo-frontend    Started                                      1.6s\n \nGreat! Containers seemed to be started. Let’s see,\n⟩ docker compose ps -a                                                   (base)\nNAME                COMMAND                  SERVICE             STATUS              PORTS\nmysql-server        &quot;docker-entrypoint.s…&quot;   mysql               running             3306/tcp, 33060/tcp\ntodo-frontend       &quot;nginx -g &#039;daemon of…&quot;   todo-frontend       running             0.0.0.0:4000-&gt;80/tcp\ntodo-server         &quot;docker-entrypoint.s…&quot;   todo-backend        running             0.0.0.0:3001-&gt;3001/tcp, 3001/tcp\n \nNow, open http://localhost:4000 and add todos.\nAn introduction to deploys\nWe can create multiple containers for the same service using deploy in docker compose.\nversion: &#039;3.7&#039;\nname: todo-app\n \nservices:\n  todo-frontend:\n    container_name: todo-frontend\n    build: ./frontend\n    ports:\n      - 4000:80\n    networks:\n      - todo-app\n    depends_on:\n      - todo-backend\n \n  todo-backend:\n    container_name: todo-server\n    build: ./backend\n    ports:\n      - 3000:3000\n    networks:\n      - todo-app\n    environment:\n      - NODE_ENV=production\n      - DB_USER=root\n      - DB_PASSWORD=password\n    depends_on:\n      - mysql\n    deploy:\n\t    mode: replicated\n\t    replicas: 2\n \n  mysql:\n    container_name: mysql-server\n    image: mysql:5.7\n    volumes:\n      - ./mysql:/var/lib/mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: password\n      MYSQL_DATABASE: todo_app\n    networks:\n      - todo-app\n \nnetworks:\n  todo-app:\nSo, this would create two replicas of the service todo-backend. The routing of which container the request should be sent to would be taken care of by the docker itself.\nRestart policy\nIf the container dies unexpectedly, docker compose can restart the container. Docker compose provides a restart policy for the same that we can define in docker compose file.\nversion: &#039;3.7&#039;\nname: todo-app\n \nservices:\n  todo-backend:\n    container_name: todo-server\n    build: ./backend\n    ports:\n      - 3001:3001\n    networks:\n      - todo-app\n    environment:\n      - NODE_ENV=production\n      - DB_USER=root\n      - DB_PASSWORD=password\n    depends_on:\n      - mysql\n    deploy:\n\t    mode: replicated\n\t    replicas: 2\n\t    restart_policy:\n\t\t    condition: any\n\t\t    delay: 5s\n\t\t    max_attempts: 2\n\t\t    window: 80s\ncondition means the condition to restart the service. delay is the delay in subsequent restart tries and window means how long to wait to decide if the restart is done.\nConclusion\nWell, this is it for now. I hope you find this useful. If there is any problem while setting up the project and running that up, please drop a comment. Please share your feedback and what our next junction topic should be, in the comment section.\nUntil then,\n\nReferences\n\nDocker Compose file reference\n"},"blogs/tech/docker/docker-compose":{"title":"Introduction to Docker Compose","links":["blogs/tech/docker/docker-networking"],"tags":["docker","docker-compose","compose","services","management","orchestration"],"content":"Hello people! Good to see you here. Today we would be learning about docker compose, what is it, why we need it, and its examples. Let’s begin.\nAgenda\n\nCurrent Scenario\nWhat is docker-compose?\nHow does docker compose change the current scenario?\nDefining Services\nVolume and Bind mounts\nNetworking\nEnvironment Variables\nConclusion\n\nCurrent Scenario\nA single application comprises containers for different services responsible for their respective tasks. For example, a backend service, a database service to manage data, a frontend for the app, and so on. All of these services need to be in the same network to communicate with each other. I have explained docker networking in my post Docker Networking.\nManaging all these services and different constituents such as networking, volumes, bind mounts, etc becomes an arduous task.\n\nThe following points summarise the problems in the current scenario:\n\nNetworking. We have to explicitly create and manage networks. We have to be sure that a particular container is connected to a particular network.\nVolumes and bind mounts. There is no centralized way to know what all volumes are connected to all services of an application.\nContainer lifecycle. If any container stops, it becomes our responsibility to restart the container.\n\nThese are some of the major problems. Setting up a multi-container application comprises multiple commands. For example,\n# for running database container\ndocker run -d --name database --network bitphile images/mysql\n \n# for running frontend\ndocker run -d --name frontend -p 3001:3001 --network bitphile bitphile/frontend\n \n# for running backend\ndocker run -d --name backend --network bitphile -mount type=volume,source=bitphile,target=/vol bitphile/backend\nThis is just for a small multi-container application comprising three services. Think about if the application is composing several services.\nDocker Compose\nDocker compose is a tool that helps in defining and running multi-container applications. It enables us to define each container as a service, its interactions with other services, and some other configurations.\nHow does it help to tackle the current Scenario?\nLet’s see each of the points mentioned in the problem statement and scrutinize how docker compose helps in there.\nNetworking\nServices defined inside a docker compose for a particular application are by default in a network for that project. When the docker compose is up, all those services are in a project namespace network.\nSo, there is no need to worry if the frontend sends a request to the backend. Moreover, the networking properties still apply here. A container can be referenced with its name for sending a request instead of its IP address.\nWe will this into action shortly in a subsequent section.\nVolumes and bind mounts\nWe can define volumes and bind mounts for services inside the docker compose definition file. We can specify to delete the volumes when doing compose down.\nContainers Life Cycle\nDocker compose acts like an orchestrator which manages the life cycle of containers. Though it is not much effective for small applications it suffices. It provides ways to specify restart the container if it dies.\nIt becomes very easy to create, run and stop services by using docker compose.\nWhole application picture in a single place\nUsing docker compose, the application becomes fathomable. By seeing the docker compose file itself we can identify what is happening. What all services are there, what is the networking structure, what volumes are in use, etc.\nWell, it is all about theory. Let’s see docker compose in action.\nDefining services\nWe are required to create a docker compose file named docker-compose.yml (you can give any other name but have to specify explicitly in the command, we will see shortly).\nA minimal docker compose file looks like following\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n \nExplanation\n\nversion defines the version of Compose file. It is required to identify the schema used for the docker compose file.\nname is the name of the project. If it is omitted, the project directory name is assigned to name.\nservices defines what all containers the compose comprises. Each item is the name of the service.\n\nBuilding image\nThere are two ways we can define service.\n\nSpecifying how docker can build an image by giving build property.\nDirectly giving the image.\n\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    build:\n\t    dockerfile: Dockerfile\n\t\tcontext: .\n \nThis builds the image and spins up the container.\nDirectly using image\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: node-alpine\nThis will lead to pulling off the image and then spinning up the container.\nVolume and Bind mounts\nIt is fairly simple to mount a volume in docker compose. There are following ways a docker volume can be mounted.\nLet docker create volume for you\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    volumes:\n\t    - /app/volume\n \nNow, if we spin up the compose\n⟩ docker compose up -d\nand listing down the volumes, we get\n⟩ docker volume ls                                                           \nDRIVER    VOLUME NAME\nlocal     a98a19a6d84da93031cb6a331993d2a49bfde5689594a413caddefe4a84d4aef\nSo, docker itself created a volume for you.\nNamed Volume\nWe can name our volume by explicitly specifying volumes section in compose file.\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    volumes:\n\t    - bitphile:/app/volume\n \n \nvolumes:\n\tbitphile:\n\t\tdriver: local\nvolumes can have more properties. You can check those out on the official docs of docker (I have mentioned them in the references section).\nBind a host path\nWe can bind a host path as follows\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    volumes:\n\t    - ./host/path:/app/host\n \n\nNote: The structure of the volumes item is, [source:]target[:mode] where mode can be ro (read only) and rw (read and write).\n\nNetworking\nDocker compose by default creates a default network where all the defined services reside. However, we can explicitly create a network and define the services.\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    network:\n\t    - bitphile\n \nnetworks:\n\tbitphile:\n\t\tdriver: bridge\nWe don’t need to mention driver properties. Docker uses it as default. Just to mention that we can specify more properties, I have specified this property.\nEnvironment Variables\nYou must be already aware of environment variables and their use cases. We can use environment variables in docker compose in the following ways.\nPassing to services\n\nUsing environment\nUsing env_file\n\nEnvironment variables\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    environment:\n\t    - NODE_ENV=production\nEnvironment file\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    env_file: \n     - .env-file\nPopulating Environment variables in compose file\n\nPassing environment file through docker compose command\nUsing .env file\n\nPassing through CLI\nDocker provides --env-file flag to specify the environment file to use.\n⟩ docker compose --env-file .env-file up\nUsing .env\nWe don’t need to specify this file. Docker takes it by default.\nConclusion\nSo, docker compose is a powerful tool helping in creating and managing multi containers applications. It provides several features and options which can’t be covered in this post. If you are interested in deep diving into those features, I would recommend you to go through the official docs. For simple uses, I don’t think you would have to rummage through all those options.\nSo, this is it for now. We have reached our junction and our train has to take a halt. Let’s continue our journey on the next green light with some other interesting topic.\nPlease provide your valuable feedback about what you liked, what has to improve, and what topic should be there for our next junction.\nUntil then,\n\nReferences\n\nDocker compose overview\nDocker CLI reference\nDocker compose file\n"},"blogs/tech/docker/docker-image-layers":{"title":"Docker Image Layers","links":["blogs/tech/docker/getting-started-with-docker","blogs/tech/docker/docker-storage-driver"],"tags":["docker","infra","kubernetes","docker-image","image-layers","image"],"content":"Deep dive into Docker Image Layers\nIn my previous post Getting started with docker, I explained the basics of docker. In this post, we will be deep diving into docker image layers. We will be covering the following topics in this post,\n\nWhat are docker image layers?\nWhy does knowing docker image layers matter?\nWhere do these intermediary images reside and how to see them?\nWhat are the constituents of a docker container size?\n\nAre you ready? Yes! So let’s start…\nWhat are docker image layers?\nWhen we build a docker image using Dockerfile, you must have seen the logs on the terminal console. There you must have seen some sha IDs that it shows. Those sha IDs are nothing but intermediary image layers IDs.\n⟩ docker build -f Dockerfile.new -t bitphile/test-image .      (base)\nSending build context to Docker daemon  7.168kB\nStep 1/4 : FROM alpine\n ---&gt; da7260331371\nStep 2/4 : COPY app.sh ./app.sh\n ---&gt; 198fdf85c15b\nStep 3/4 : RUN chmod +x app.sh\n ---&gt; Running in cf5c357a3254\nRemoving intermediate container cf5c357a3254\n ---&gt; 8e758ee45de3\nStep 4/4 : ENTRYPOINT [&quot;./app.sh&quot;]\n ---&gt; Running in 857700b4b452\nRemoving intermediate container 857700b4b452\n ---&gt; 4d85107b2c59\nSuccessfully built 4d85107b2c59\nSuccessfully tagged bitphile/test-image:latest\nA docker image is made up of several docker image layers. Each of the commands in Dockerfile stacks up a new image layer on top of the previous one.\n\nNote: Commands in Dockerfile are those instructions which do some changes in the file system. Any change made in the file system creates a new image layer. So, commands are responsible for the creation of a new image layer.\nFor example, COPY, RUN, etc are commands.\n\nThe following image shows the docker image layers having ID and their corresponding parent layer ID.\n\n\nNote: Each of the layers coming on top of the existing one depends on their parent layer. Changing the sequence may altogether change the final image. Caching of image layers also depends upon this (upcoming sections).\n\nR/W image layers\nAn image layer can be read-only or read and write. All the intermediate image layers are read-only. One can’t create a file in any of those layers. If anyone of you has worked on docker must be thinking, bitPhile is saying wrong as we can create files in running docker container.\nYour concern is right. We can create files in the docker container. The reason behind this is Container Layer which is explained in the next section.\nContainer Layer\nWhen an image is turned into a container, a new thin layer is stacked on top of the layers. This thin layer is called container layer. This image layer has both Read and Write access. All other image layers beneath this can only be read.\n\nSource: docs.docker.com/storage/storagedriver/images/container-layers.jpg\nWhy does knowing Image Layers matter?\nThere are several reasons why one should know about docker image layers. A few of them are described below.\nImage Size\nAn image is a pack of all dependencies, libraries, and src code required for an application. These images are stored on cloud registries and shared across devices. Size becomes a foremost requirement for an image.\nImage layers size consititute into the size of the final image. Identifying which layer is adding more size can help the developer to find areas of concern.\nLayers are Cached\nImage layers are stored on the host machine for future builds. This helps build docker images faster.\nLet’s see an example. Consider two Dockerfiles, Dockerfile.new-1 and Dockerfile.new-2.\nDockerfile.new-1\n# syntax=docker/dockerfile:1\nFROM bitphile/my-base-image\n \nCOPY app.sh ./app.sh\n \nRUN chmod +x app.sh\n \nENTRYPOINT [&quot;./app.sh&quot;]\nDockerfile.new-2\n# syntax=docker/dockerfile:1\n \nFROM bitphile/my-base-image\n \nCOPY app.sh ./app.sh\nBuilding the first image, the consoles show,\nSending build context to Docker daemon  7.168kB\nStep 1/4 : FROM bitphile/my-base-image\n ---&gt; da7260331379\nStep 2/4 : COPY app.sh ./app.sh\n ---&gt; 198fdf85c15b\nStep 3/4 : RUN chmod +x app.sh\n ---&gt; Running in cf5c357a3254\nRemoving intermediate container cf5c357a3254\n ---&gt; 8e758ee45de3\nStep 4/4 : ENTRYPOINT [&quot;./app.sh&quot;]\n ---&gt; Running in 857700b4b452\nRemoving intermediate container 857700b4b452\n ---&gt; 4d85107b2c59\nSuccessfully built 4d85107b2c59\nSuccessfully tagged bitphile/my-new-image:latest\nThe second, it shows,\nSending build context to Docker daemon  8.192kB\nStep 1/2 : FROM bitphile/my-base-image\n ---&gt; da7260331379\nStep 2/2 : COPY app.sh ./app.sh\n ---&gt; Using cache\n ---&gt; 198fdf85c15b\nSuccessfully built 198fdf85c15b\nSuccessfully tagged bitphile/my-new-image-2:latest\nWe can see it uses caches for the second build. This increases the speed of building the image. Interesting, right?\n\nNote: If any instruction changes, caches are not used for the next instructions. New image layers are created for them from the scratch. Why does this happen? Think about it!\n\nSeeing Image Layers\nOnce the final image is built, we can see intermediary image layers by,\ndocker history &lt;image-name&gt;\nSo, by running the above commands,\ndocker history bitphile/my-new-image\nGives the output,\nIMAGE          CREATED          CREATED BY                                      SIZE      COMMENT\n4d85107b2c59   50 minutes ago   /bin/sh -c #(nop)  ENTRYPOINT [&quot;./app.sh&quot;]      0B\n8e758ee45de3   50 minutes ago   /bin/sh -c chmod +x app.sh                      33B\n198fdf85c15b   50 minutes ago   /bin/sh -c #(nop) COPY file:1179990b720068e4…   33B\nda7260331379   2 hours ago      /bin/sh -c apk add --no-cache bash              2.24MB\nb2aa39c304c2   3 weeks ago      /bin/sh -c #(nop)  CMD [&quot;/bin/sh&quot;]              0B\n&lt;missing&gt;      3 weeks ago      /bin/sh -c #(nop) ADD file:40887ab7c06977737…   7.05MB\n \nThis shows the intermediate image ID, Dockerfile instruction, and size.\n\nNote: &lt;missing&gt; indicates that a particular image is built on a different machine or using builtKit docker builder.\n\nImage Layers on Machine\nWe can find these image layers on the host machine at location /var/lib/docker. If you are using a virtual machine to run a docker daemon, ssh into that machine and access this location.\nListing the contents of /var/lib/docker gives,\ndrwx--x--x   4 root root 4.0K Feb 26 09:14 buildkit\ndrwx--x--x   3 root root 4.0K Feb 26 09:14 containerd\ndrwx--x---  20 root root 4.0K Mar  5 05:51 containers\ndrwx------   3 root root 4.0K Dec 16  2021 image\ndrwxr-x---   3 root root 4.0K Feb 26 09:14 network\ndrwx--x--- 173 root root  24K Mar  5 05:51 overlay2\ndrwx------   4 root root 4.0K Feb 26 09:14 plugins\ndrwx------   2 root root 4.0K Mar  5 02:42 runtimes\ndrwx------   2 root root 4.0K Feb 26 09:14 swarm\ndrwx------   2 root root 4.0K Mar  5 06:21 tmp\ndrwx------   2 root root 4.0K Feb 26 09:14 trust\ndrwx-----x   6 root root 4.0K Mar  5 02:42 volumes\nWe can find these images in images/&lt;storage-driver-name&gt;/imagedb/content/sha25. In my case, overlay2 is storage driver. Refer my post Storage Driver.\nListing content of images/overlay2/imagedb/content/sha25, we see,\n-rw------- 1 root root 1.9K Mar  5 05:10 08bd868662db2cd09a4c7b8e74dad6a9c8fe186964ff31c65d335299d1324ff3\n-rw------- 1 root root 1.9K Mar  5 05:51 198fdf85c15b292a5fff554f3afc26fd1824413822ddd75e72f08937889f1770\n-rw------- 1 root root 2.2K Mar  5 05:11 1d17f325c607e937d89b19de66287aad351fc1abb80bc36591d34b7457ff8316\n-rw------- 1 root root 1.7K Dec 16  2021 25f8c7f3da61c2a810effe5fa779cf80ca171afb0adf94c7cb51eb9a8546629d\n-rw------- 1 root root  16K Mar  1 12:49 2bc7edbc3cf2fce630a95d0586c48cd248e5df37df5b1244728a5c8c91becfe0\n-rw------- 1 root root 2.4K Feb 26 09:51 2f6974482449313e863b0b709841b50da40ce7ef32017f0279671e9b2ee47289\n-rw------- 1 root root  11K Mar  1 06:10 31d22a1554dfca7ad9377b67974ae2e0e3282247fc40fbf2ac3c380102ae772f\n-rw------- 1 root root 4.4K Mar  1 05:26 3c3a210148bab6bc3faedba72ad7de5e58a7cdb6bba0847965211f47fe1c4c21\n-rw------- 1 root root 2.1K Feb 26 09:35 da7260331379a532a15cf3e9bff522627b1574a9ce21bb06c92cea74992613fa\nAs you can see da7260331379 is used as a cache to build the image.\nDocker Container Size\nThere are two things, size and virtual size.\n\nsize is the size of container layer. It comprises of all the files that are written container layer.\n\n\nvirtual size is the sum of size and data of read-only intermediate image layers. If two containers have the same read-only intermediate image layers, will share the same data with separate container layer data.\n\nFor example,\nCONTAINER ID   IMAGE                                          COMMAND                  CREATED         STATUS                   PORTS                  NAMES                                                                                                         SIZE\n7937d5f33e6e   bitphile/my-new-image-2                        &quot;./app.sh&quot;               5 seconds ago   Up 5 seconds                                    bitphile-test-container                                                                                       0B (virtual 9.29MB)\nshows container bitphile-test-container has 0B size and 9.29MB virtual size. If you see the size of the whole image itself is 9.29MB which means it is all size of read-only image layer data.\nNow, adding a new file into the running container should increase the size.\ndocker exec 7937d5f33e6e sh -c &quot;echo hello world &gt; file.txt&quot;\nNow if we see the same container,\nCONTAINER ID   IMAGE                                          COMMAND                  CREATED          STATUS                   PORTS                  NAMES                                                                                                         SIZE\n7937d5f33e6e   bitphile/my-new-image-2                        &quot;./app.sh&quot;               39 seconds ago   Up 39 seconds                                   bitphile-test-container                                                                                       11B (virtual 9.29MB)\ngot 11B of size.\nSo, this is it for now. It’s great that you come reading till here. I will see you in my next post on docker. Till then,\n\nReferences\n\nStorage Drivers \n"},"blogs/tech/docker/docker-networking":{"title":"Docker Networking - How does it work?","links":[],"tags":["docker","networking","ports","bridge","routintg","routing-mesh","overlay"],"content":"Hello folks! Good to see you again on our journey of Docker. In this post, we would be learning about docker networking. So, let’s get started.\nWhat is covered?\n\nAbstract\nProblem Statement\nHow does Docker Network solve the problem?\nSimple example\nTypes of networks based on drivers\nDiving deep into bridge networks\nConclusion\n\nAbstract\nDocker has been a powerful tool for creating and managing independent applications and services. These services are interacting with each other through specified interfaces. However, the isolated environment of docker stymies that interaction. An isolated environment hinders services request-response communications as they have different network namespaces.\nTo solve this problem, docker introduced the concept of networking. Docker Networking.\nProblem without networking\nApplications are running in their containers. These containers have their networking interfaces, namespace, etc. They are not aware of other containers. If there is an application that wants to communicate with another application, sends a request to a specified endpoint of another application. The request never reaches another application as networks are different and isolated.\nFor example, consider a chat application feeberchat (I created this app a long time ago 😄). feeberchat is running inside a container A. The database of feeberchat is getting stored in an instance of mongodb running inside another container B.\nAs both A and B have different network namespaces, they can’t communicate.\n\nHow does Docker Networking solve the problem?\nDocker networking allows to create isolated networks for containers to communicate with each other. It creates a common networking namespace where containers reside and communicate.\n\nAn example\nAssume we have a node (express) application running on port 3000, dockerized with endpoints:\n\n/\n/health\n\nRunning that application in no network using\n⟩ docker run -d --name express-app --network none bitphile/express-app       (base)\nLet’s try to access it from another container.\n⟩ docker run -it --rm --name alpine-shell alpine:latest sh       (base)\nIt opens up the container. Now we have to send a request to container express-app. What is the endpoint? We don’t know as express-app is running without any network. Docker didn’t assign any IP address to express-app container.\nNow, let’s run express-app again differently\n⟩ docker run -d --name express-app bitphile/express-app                (base)\nInspecting network details of express-app gives\n{\n  &quot;bridge&quot;: {\n    &quot;IPAMConfig&quot;: null,\n    &quot;Links&quot;: null,\n    &quot;Aliases&quot;: null,\n    &quot;NetworkID&quot;: &quot;8f7326c66c80c45501e24eb7c29596b9a1065624a81b443ef34f8e5c59abab05&quot;,\n    &quot;EndpointID&quot;: &quot;987f33191fc7843190ac74227c8d062b6a8e2b9c2101e52e289f2f82e6f8c9af&quot;,\n    &quot;Gateway&quot;: &quot;172.17.0.1&quot;,\n    &quot;IPAddress&quot;: &quot;172.17.0.3&quot;,\n    &quot;IPPrefixLen&quot;: 16,\n    &quot;IPv6Gateway&quot;: &quot;&quot;,\n    &quot;GlobalIPv6Address&quot;: &quot;&quot;,\n    &quot;GlobalIPv6PrefixLen&quot;: 0,\n    &quot;MacAddress&quot;: &quot;02:42:ac:11:00:03&quot;,\n    &quot;DriverOpts&quot;: null\n  }\n}\nIt bombarded us with a bunch of details. Let’s not worry about the rest of the details and take IPAddress into consideration. IPAdderss is the IP address of container express-app in the default bridge network (coming in the following sections. Just assume it is as a networking type).\nLet’s send a request to this ip address from alpine-shell container.\n$ curl 172.17.0.3:3000/health\nThis is healthy $\nGreat! We got some results.\nWhat did happen?\nWhen we run express-app container without specifying --network flag, it creates a virtual network (which is called default bridge network). Docker assigns an ip address from that virtual network to the container. That’s why we could see IPAddress in docker inspect.\nIf you try running express-app with --network none flag and inspect the container, you wouldn’t be seeing any network information.\n\nNetwork drivers\nDocker provides various network drivers for different types of requirements. Following is a list of those drivers.\n\nbridge\nhost\noverlay\nmacvlan\nipvlan\n\nEach of these drivers has different capabilities. Select a driver best suits your requirements. For the sake of simplicity and the size of the post, we would be going with bridge network driver.\nBridge Networks\nAligning ourselves with the name, we can think of a bridge as something that connects segments. In networking, it is a Link Layer device that connects network segments.\nIn docker, bridge networks connect containers in a network where they can communicate easily while allowing isolation from the rest of the containers not connected to the same bridge network.\nBridge networks can only be used on the same docker daemon host. For networking among containers from different docker hosts, prefer some other docker networking driver (eg overlay network).\nDefault Bridge Network\nDocker creates this network by default if we don’t specify any network to use.\n⟩ docker network ls                                                              (base)\nNETWORK ID     NAME         DRIVER    SCOPE\n8f7326c66c80   bridge       bridge    local\n1b69478406e4   host         host      local\n9324e36db9ab   none         null      local\nLook bridge network of the driver bridge. This is the default bridge network created and managed by docker. Though we can also configure it but not doing also suffices the requirements in most cases.\n\nNote: Have a look at none network with null driver. This is what we have used for no networking.\n\nUser-defined Bridge Network\nWe can also create a network of driver bridge. This has a few advantages over default bridge network:\n\nThe most important, user-defined bridge network provides automatic DNS resolution between containers. It means, we can directly use container names to send requests instead of IP addresses.\nUser has total control over the configurations on the network.\nOnly containers with this network type can communicate.\n\nUsing user-defined Bridge Network\nCreating a network\nFollowing is the command used to create a docker network\ndocker network create [options] &lt;name&gt;\nLet’s create a network of the name bitphile.\n⟩ docker network create bitphile                                                (base)\n2bfb26180adc42445ca0127f8628d45b71f1871b8b3fdea2a7d3da8b31e9dd17\n⟩ docker network ls                                                             (base)\nNETWORK ID     NAME       DRIVER    SCOPE\n2bfb26180adc   bitphile   bridge    local\n8f7326c66c80   bridge     bridge    local\n1b69478406e4   host       host      local\n9324e36db9ab   none       null      local\nBy default, it creates a network of driver type bridge. We can specify the driver using --driver flag.\n⟩ docker network create --driver macvlan bitphile                               (base)\n237a35cfa290dd59db70b099cf970efd5efbd29f150ae65c7add51632d704a81\nInspecting the network details gives us\n[\n  {\n    &quot;Name&quot;: &quot;bitphile&quot;,\n    &quot;Id&quot;: &quot;2bfb26180adc42445ca0127f8628d45b71f1871b8b3fdea2a7d3da8b31e9dd17&quot;,\n    &quot;Created&quot;: &quot;2023-03-19T09:42:37.48139717Z&quot;,\n    &quot;Scope&quot;: &quot;local&quot;,\n    &quot;Driver&quot;: &quot;bridge&quot;,\n    &quot;EnableIPv6&quot;: false,\n    &quot;IPAM&quot;: {\n      &quot;Driver&quot;: &quot;default&quot;,\n      &quot;Options&quot;: {},\n      &quot;Config&quot;: [\n        {\n          &quot;Subnet&quot;: &quot;172.20.0.0/16&quot;,\n          &quot;Gateway&quot;: &quot;172.20.0.1&quot;\n        }\n      ]\n    },\n    &quot;Internal&quot;: false,\n    &quot;Attachable&quot;: false,\n    &quot;Ingress&quot;: false,\n    &quot;ConfigFrom&quot;: {\n      &quot;Network&quot;: &quot;&quot;\n    },\n    &quot;ConfigOnly&quot;: false,\n    &quot;Containers&quot;: {},\n    &quot;Options&quot;: {},\n    &quot;Labels&quot;: {}\n  }\n]\nRunning the container with this network\n⟩ docker run -d --name express-app --network bitphile bitphile/express-app\n70ae390032cbf2e1e38e749895cf17533c70021168a457480a3dd57f035296fe\nIf we inspect the network bitphile again, we would see a list of containers using this network.\n[\n  {\n    &quot;Name&quot;: &quot;bitphile&quot;,\n    &quot;Id&quot;: &quot;56b7da97479a3a27859e2c62b5a9f84b8389b5b3d5b41b40283b56a40e88d1a6&quot;,\n    &quot;Created&quot;: &quot;2023-03-19T09:46:13.74404902Z&quot;,\n    &quot;Scope&quot;: &quot;local&quot;,\n    &quot;Driver&quot;: &quot;bridge&quot;,\n    &quot;EnableIPv6&quot;: false,\n    &quot;IPAM&quot;: {\n      &quot;Driver&quot;: &quot;default&quot;,\n      &quot;Options&quot;: {},\n      &quot;Config&quot;: [\n        {\n          &quot;Subnet&quot;: &quot;172.22.0.0/16&quot;,\n          &quot;Gateway&quot;: &quot;172.22.0.1&quot;\n        }\n      ]\n    },\n    &quot;Internal&quot;: false,\n    &quot;Attachable&quot;: false,\n    &quot;Ingress&quot;: false,\n    &quot;ConfigFrom&quot;: {\n      &quot;Network&quot;: &quot;&quot;\n    },\n    &quot;ConfigOnly&quot;: false,\n    &quot;Containers&quot;: {\n      &quot;70ae390032cbf2e1e38e749895cf17533c70021168a457480a3dd57f035296fe&quot;: {\n        &quot;Name&quot;: &quot;express-app&quot;,\n        &quot;EndpointID&quot;: &quot;0fda9ef2aa92e5966320c330776986c9cea15210354d2994e3060522619a5290&quot;,\n        &quot;MacAddress&quot;: &quot;02:42:ac:16:00:02&quot;,\n        &quot;IPv4Address&quot;: &quot;172.22.0.2/16&quot;,\n        &quot;IPv6Address&quot;: &quot;&quot;\n      }\n    },\n    &quot;Options&quot;: {},\n    &quot;Labels&quot;: {}\n  }\n]\nQuick introduction to subnet and gateway\nThis won’t be thorough. It would just be for understanding networking in the context of docker.\nSubnet\nIt is used to divide the larger network into the smaller network to make network management easy. For example, subnet 172.22.0.0/16 means the first 16 bites will be used for networks, and the rest of 16 bits will be used for hosts in the sub-network.\n \n172    22    0    0\n -------      ----\n network      host\nIn the context of docker, the subnet makes sure that the network interface has a unique IP address (gateway IP address) and the containers are assigned unique IP addresses. It also helps in routing traffic.\nGateway\nAs the name suggests, it is the entry point (and exit point) of a network segment.\nIn the context of docker, we have gateway 172.22.0.1 which is an entry point for network bitphile.\nSpecifying subnet and gateway\nIt is possible to provide a subnet and gateway while creating a network.\n⟩ docker network create --subnet=172.20.0.0/16 --gateway=172.20.0.1 bitphile  (base)\n340e9f5c82185940df2a3a2009e6ef5276c1c6b440a9a8dc5376c5c5df448b90\nTry running the same command again and see what happens.\nYou can’t create another network with the subnet already used.\nDelete a network\nDeleting something gives such a relaxation that can’t be expressed. Specially deleting the code (don’t delete a working code 😂).\nWe can delete the network using\n⟩ docker network rm bitphile                                                    (base)\nbitphile\nConclusion\nFinally, we are at the home stretch of this post. Hope you found this useful and you got something important out of this. Please reach out to me if you have any feedback. I have given my mail below. See you at the next junction of our journey on docker.\nUntil then,\n\nReferences\n\nNetworking overview\nContainer networking\nDocker bridge networking\n\n📨Mail to me"},"blogs/tech/docker/docker-storage-driver":{"title":"docker-storage-driver","links":[],"tags":[],"content":""},"blogs/tech/docker/docker-volume":{"title":"Docker Volume","links":["blogs/tech/docker/docker-image-layers"],"tags":["docker","infra","kubernetes","docker-volume","volume","storage","bind-mounts","tmpfs"],"content":"Understanding Docker Volume Thoroughly\nAgenda\n\nAbstract\nContainer layer and its data on docker host machine\nWhat are the flaws of storing data in the container layer?\nWhat is the solution?\nUsing docker volume\nConclusion\n\nAbstract\nA Docker container is an independent process comprising the application and its dependencies. Container has its processes, file system, and networks independent of the host machine.\nCreating a resource in the running container stores that resource in the Read-Write layer or Container layer. This is a temporary storage mechanism. It is accessible until the container is running. Once the container is removed, all the data goes.\nDocker provides a few mechanisms to tackle this problem and volume is one of them. Let’s dig deeper into docker volumes.\n\nNote: All the examples are being done by using minikube for running docker daemon. Any mention of docker host machine refers to minikube node (You can go inside minikube node using, minikube ssh).\n\nContainer Layer\nI have explained about container layer in my previous post on Docker at Docker Image Layers. In this section, we would be seeing where actually the container layer is stored on the docker host machine. Excited? Seems like yes 😄. Let’s start.\nWhere does the container layer reside?\nContainer layer is a read-write layer stacked on top of read-only image layers. We can’t modify read-only filesystem of read-only image layers. We can modify the contents in the file system of the container layer.\nLet’s spin up an alpine container with sh command.\ndocker run --rm -it alpine sh\nCreating a file as\n/ # echo &quot;Docker is awesome - Nitin&quot; &gt; testimonials.txt\n/ # ls\nbin               home              mnt               root              srv               tmp\ndev               lib               opt               run               sys               usr\netc               media             proc              sbin              testimonials.txt  var\n\nNow, let’s see where this data is stored on the docker host machine.\nInspecting the above container of ID 385f23abffa3 gives the following\n$ docker inspect --format &#039;{{ .GraphDriver.Data }}&#039; 385f23abffa3\nmap[LowerDir:/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032-init/diff:/var/lib/docker/overlay2/71129bf0c1fb93d386d1abd9390e68eb08b64e65cb31186a0e710931359adb72/diff MergedDir:/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/merged UpperDir:/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/diff WorkDir:/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/work]\n$\nWoo! It gives a map with a bunch of paths. Let me example you each of these paths. The following descriptions are based on overlay2 storage driver.\n\nLowerDir is the read-only file system for lower image layers. Any changes made to the file system are reflected in the new file system of the read-write container layer. LowerDir acts as a base file system.\nUpperDir is the read-write file system for the container layer where we can create and delete files and directories. Container layer changes are stored in this location.\nMergedDir is the merge of LowerDir and UpperDir. It gives a unified view of the file system for the container.\nWorkDir is something overlay2 driver uses for its internal operations such as copy-on-write process.\n\nAs you can see LowerDir has two paths separated by :.\nFirst one is the path of the lower image layer read-only file system.\n/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032-init/diff\n\nSecond one is the path of the lower image layer read-only file system after changes made by previous running containers.\n/var/lib/docker/overlay2/71129bf0c1fb93d386d1abd9390e68eb08b64e65cb31186a0e710931359adb72/diff\n\nThese two directories serve as a base read-only file system for the container layer read-write file system.\nAs we create a new file testimonials.txt in the container, let’s peek into UpperDir to see this new file. Remember, this is on the docker host machine.\n$ sudo ls -la /var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/diff\ntotal 16\ndrwxr-xr-x 3 root root 4096 Mar 12 06:45 .\ndrwx--x--- 5 root root 4096 Mar 12 06:44 ..\ndrwx------ 2 root root 4096 Mar 12 06:44 root\n-rw-r--r-- 1 root root   26 Mar 12 06:45 testimonials.txt\n$\nYay! We have testimonials.txt on our docker host machine. If we cat we see,\n$ pwd\n/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/diff\n$ cat testimonials.txt\nDocker is awesome - Nitin\nGreat. If you have noticed when we do ls inside the container, it gives more directories like,\n$:/ ls\nbin               home              mnt               root              srv               tmp\ndev               lib               opt               run               sys               usr\netc               media             proc              sbin              testimonials.txt  var\nHowever, we didn’t see these directories on UpperDir. Guess why?\nThe reason is, UpperDir only has the changes made to the container file system. This is where MergedDir comes into existence which provides a unified view of the file system. MergedDir is the merge of LowerDir and UpperDir to give a unified file system.\nLet’s see what is inside MergedDir.\n$ sudo ls /var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/merged\nbin  etc   lib    mnt  proc  run   srv  testimonials.txt  usr\ndev  home  media  opt  root  sbin  sys  tmp               var\n$\nAs we can see it is the same as the file system of the container. You can go ahead and try creating files in this location and see if it turns up in the container.\nThe Flaws of storing data in the container layer\n\n\nFirst thing first, storing data in the container layer is not permanent. Once you delete the container, all the data stored goes away.\n\n\nstorage-driver is getting used to dealing with the file system on the container. Using container file system for an IO-intensive application is always not a good idea. It causes performance issues and increases latency when accessing files stored on a container file system.\n\n\nContainer size gets increase when we start to store files on the container file system. I would give the example of a use case. Sometimes we get the need to create an image from the container itself. And if we have a huge amount of data inside the container, the resulting image would be big.\n\n\nSharing of data across containers is hard as it is highly coupled with the container itself.\n\n\nThese are the problems we are considering here. There may be different other problems.\nThen what is the solution, huh?\nSolution - Using Docker volume\nVolume is not the only solution. It is one of the solutions docker provides. Some other solutions are bind mound, tmpfs etc.\nWhat is Docker Volume?\nThis is the way of storing and managing persistent data outside of container file system. It is stored on a host file system managed by Docker. These volumes can be shared across different containers.\nCool, Huh?\nLet’s create a volume\nVolumes are not tied to containers. So, we can create and manage volumes without even touching containers.\ndocker volume command is used to manage volumes.\n$ docker volume\n \nUsage:  docker volume COMMAND\n \nManage volumes\n \nCommands:\n  create      Create a volume\n  inspect     Display detailed information on one or more volumes\n  ls          List volumes\n  prune       Remove all unused local volumes\n  rm          Remove one or more volumes\n \nRun &#039;docker volume COMMAND --help&#039; for more information on a command.\n$\nLet’s create a volume of the name bitphile, 😁.\n$ docker volume create bitphile\nbitphile\n$ docker volume ls\nDRIVER    VOLUME NAME\nlocal     bitphile\n$\nInspecting this volume gives us,\n$ docker volume inspect bitphile\n[\n    {\n        &quot;CreatedAt&quot;: &quot;2023-03-12T07:38:38Z&quot;,\n        &quot;Driver&quot;: &quot;local&quot;,\n        &quot;Labels&quot;: {},\n        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/bitphile/_data&quot;,\n        &quot;Name&quot;: &quot;bitphile&quot;,\n        &quot;Options&quot;: {},\n        &quot;Scope&quot;: &quot;local&quot;\n    }\n]\n$\nMountpoint is where this volume is mounted. That is the location on the host machine where volume data will be stored.\nLet’s peek inside that location and see what’s there.\n$ sudo ls -la /var/lib/docker/volumes/bitphile/_data\ntotal 8\ndrwxr-xr-x 2 root root 4096 Mar 12 07:38 .\ndrwx-----x 3 root root 4096 Mar 12 07:38 ..\n$\nNOTHING? Well, we just created it 😂!\nMount Volume to container\nThere are two methods of mounting volume to the container.\n\n--mount option\n-v option\n\n-v option is like shorthand. --mount is verbose and explicit, which is why we going to use --mount throughout this post.\nLet’s create a container and mount it to this volume we have just created.\n$ docker run --rm -it --mount type=volume,source=bitphile,target=/bitphile-vol alpine sh\n/ #\n--mount option takes parameters are key=value pairs separated by ,.\n\ntype specifies the volume as type. --mount is being used by bind mounts, and tmpfs also. By default, type is volume. But for the sake of being explicit, I have used it.\nsource is the volume name.\ntarget is the path in the container file system that will be mounted with the volume. There is alias of this option as dest, destination etc.\n\nListing contents in the container file system root we see,\n/ # ls\nbin           home          opt           sbin          usr\nbitphile-vol  lib           proc          srv           var\ndev           media         root          sys\netc           mnt           run           tmp\n/ #\n\nWe have bitphile-vol directory mounted to volume bitphile.\nLet’s create a file inside that directory.\n/ # echo Good morning &gt; bitphile-vol/greetings.txt\n/ # cat bitphile-vol/greetings.txt\nGood morning\n/ #\n\nPeeking on the volume mount location on the host machine, we see,\n$ sudo ls -la /var/lib/docker/volumes/bitphile/_data\ntotal 12\ndrwxr-xr-x 2 root root 4096 Mar 12 07:48 .\ndrwx-----x 3 root root 4096 Mar 12 07:38 ..\n-rw-r--r-- 1 root root   13 Mar 12 07:48 greetings.txt\n$\nCool! We have greetings.txt.\nMount the same volume to another container\nLet’s spin up a new container with the same volume.\n$ docker run --name bitphile-second -it --mount source=bitphile,target=/bit-vol alpine sh\n/ $ ls\nbin      etc      media    proc     sbin     tmp\nbit-vol  home     mnt      root     srv      usr\ndev      lib      opt      run      sys      var\n/ $\nLet’s see what bit-vol has,\n/ # ls bit-vol\ngreetings.txt\n/ # cat bit-vol/greetings.txt\nGood morning\n/ #\n\nNice.\nConclusion\nFinally, we are at the end (the happiest moment when the zoom meeting ends 😂).\nSo, docker volumes are used as one of the methods to store data permanently. There are some other methods of doing the same. We will have look at those sometime later.\nUntil then,\n\nReferences\n\nDocker Volumes\n"},"blogs/tech/docker/getting-started-with-docker":{"title":"Getting Started With Docker","links":["blogs/tech/docker/docker-image-layers"],"tags":["docker","infra","kubernetes","container","containerisation","containerization"],"content":"Introduction\nDeveloping software and applications is getting more complex day by day. This introduces the complexity of running and deploying the applications in different machines or environments. It also includes making sure it works everywhere no matter what the architecture of the host machine is.\nApplications running in the same environment can conflict with other applications. This can cause malfunctions and make application development less effective.\nTo solve this problem we need isolation and a virtual machine is one way to achieve this. We can have different virtual machines to run the applications that will be independent of other applications environments.\nHowever, using a virtual machine has its own pros and cons. Some of these are the followings:\nVirtual Machine Pros\n\nStronger isolation and security as it provides hardware-level isolation which makes them more secure and isolated.\n\nVirtual Machine Cons\n\nRequires more resources. Virtual machine requires guest system operation to run which consumes more resources.\nStartup time is more in virtual machines as it has to boot the whole operating system.\nPortability is less as virtual machines are tied to specific hardware and require additional configurations to run.\n\nSo, what alternative do we have to achieve this isolated environment? The answer is containerization.\nContainerization\nJust to get started with this concept, I would like to give an example. We have seen containers that are used on ships to transport things. These containers have isolated environments and temperatures specific to the items they are containing.\nSimilarly, containers can contain applications and supporting libraries required to run the application. This helps in running applications in any environment no matter what host system OS and architecture are.\nDocker\nDocker is a containerization tool that is used to create and run containers. There are several other alternatives to docker like Podman, openVZ, etc.\nArchitecture\nDocker uses client-server architecture where the docker client talks with the docker daemon which can run on either a host machine, a virtual machine, or a machine on the cloud. Docker clients can connect to multiple daemons.\n\nSource: docs.docker.com/engine/images/architecture.svg\nDocker client\nDocker client is an interface that lets users create and run containers. Client talks with docker daemon using Docker API requests. It is not more than an application that can be web-based or cli that sends requests to the server and gets responses back.\nThe server or daemon performs the requested job.\nDocker daemon\nA Docker daemon is the server that accepts requests from the docker client and performs the respective job. It can run on a host machine or virtual machine. Docker clients can connect to multiple docker daemons and change the daemon using docker client-provided commands.\nFor example,\ndocker context ls\nIt shows available contexts or daemons to which a client can connect to.\nNAME                DESCRIPTION                               DOCKER ENDPOINT                                     KUBERNETES ENDPOINT   ORCHESTRATOR\ndefault             Current DOCKER_HOST based configuration   tcp://localhost:2376                                                      swarm\ndesktop-linux                                                 unix:///Users/test/.docker/run/docker.sock\nrancher-desktop *   Rancher Desktop moby context              unix:///Users/test/.rd/docker.sock\n\nTo change context, use\ndocker context use rancher-desktop\n\nNote: Rancher desktop is an alternative to Docker desktop you can use. Docker desktop requires a license. Rancher desktop creates a lightweight virtual machine on the host system which contains docker components. Docker daemon runs inside that virtual machine.\n\nDocker Container and Image\nContainer\nAs I mentioned before, a container is a self-contained unit having all the necessary things required for the item container is containing. In this context, a container is a self-contained runnable unit that comprises the application and supporting libraries that are required to run that application.\nImage\nAn image is a blueprint of a container that defines a container. An image contains all the instructions and resources required to create a container.\nIf you have worked on OOPs, you must have heard about classes and objects. You can think of images as classes and objects as containers.\nYou can create your image by using other images as a base. I will show you shortly how to do it.\nRegistry\nThis is the place where you can push your docker images. It’s like GitHub where you push source code onto GitHub and here you would be pushing docker images. You can also pull docker images stored on the docker registry.\nDocker provides a docker hub register where you can get almost all docker images pushed by other people. Visit hub.docker.com for more details.\nDockerfile\nDockerfile is used to build docker images. It contains all the instructions required to build an image. Following is an example of a Dockerfile.\n\nFROM fedora:latest\nRUN dnf install figlet -y\nENTRYPOINT [&quot;figlet&quot;]\nCMD [&quot;bitPhile&quot;]\nExplanation\n\nFROM specifies to use fedora:latest as the base image. The base image is the parent image on which your image is based. A new image will be built on top of this base image.\nRUN instruction is used to run the given command while building the image.\nENTRYPOINT is the starting script that will run when the container is started.\nCMD specifies the default command to run when the container is started.\n\n\nThere is a difference between ENTRYPOINT and CMD. When ENTRYPOINT is provided, whatever argument CMD instruction has will be append as arguments to ENTRYPOINT. For instance, if we will see the above example, the final command will be figlet hello world.\n\nBuilding Docker Image\nSave the above file and run the following docker command.\ndocker build -t docker-fedora-test .\nbuild is the subcommand that tells to build a docker image. -t is a tag where docker-fedora-test is passed as a tag name. . is the context.\n\nWhat is context? Context the part of the host file system that is required to build a docker image. For example, there may be some file or data which is required in image building. So, we pass those files using context.\nWe can only pass one context.\n\nA docker image is built through several layers of images. Each of the instructions in Dockerfile runs in a separate image layer. To learn more about docker image layers, refer to my blog docker image layers.\nRunning Docker Image\nTo run the image we’ve just built, run the following docker command.\ndocker run docker-fedora-test\nThis gives output as,\n _     _ _   ____  _     _ _\n| |__ (_) |_|  _ \\| |__ (_) | ___\n| &#039;_ \\| | __| |_) | &#039;_ \\| | |/ _ \\\n| |_) | | |_|  __/| | | | | |  __/\n|_.__/|_|\\__|_|   |_| |_|_|_|\\___|\nAwesome, right?\nWe can also pass arguments while we are running the container.\ndocker run docker-fedora-test hello world\nhello world gets append as an argument to fedora entrypoint command.\nFolks, this is it for now. There are lots of things that come in docker and it is hard to put everything in one place both for you and me. So, see you in the next post on docker. Stay tuned.\nPlease provide your feedback as it helps me improve the posts and get more information to you.\nReferences\n\ndocs.docker.com/engine/reference/builder/ for Dockerfile\ndocs.docker.com/get-started/overview/ for overview and architecture\n"},"blogs/tech/programming/scala/chat-app-in-scala-armeria":{"title":"Chat App in Scala Armeria","links":[],"tags":["rest-api","microservices","armeria","websocket","socket"],"content":"Contents\n\nAbstract\nWhat is Armeria and its features?\nProject details\nProject setup requirements\nSetting up Armeria and service handler\nSetting up the web socket handler\nShow example in the browser console\nSocket service for frontend\nCode snippet of the chat screen\nWorking example Screenshots etc\nConclusion\nAcknowledgements\nReferences\n\nAbstract\nAs a part of E4R (Engineering for Research) retreat Hackathon, we explored Armeria and created a small PoC single group chat application using HTTP request polling and websockets. We are using the Armeria framework to develop one of the services in our project. So, it became the inspiration for the exploration.\nThis blog constitutes building a small chat application using REST API framework Armeria, Scala, and JavaScript. It won’t have end-to-end code but only contains code snippets. You can find code on here.\nWhat is Armeria?\nArmeria is a REST API microservice framework that becomes a backbone of powerful, fast, and asynchronous web services. It is completely asynchronous and built on top of reactive streams and Java 8 CompletableFuture. It is built in Java and compatible with any language that runs on JVM such as Scala, Kotlin, Clojure, etc.\nIt lets you develop services with technologies like gRPC, Thrift, HTTP, etc. running on the same application and port.\nI think this introduction to Armeria would be enough to get a basic understanding. If you are more interested in Armeria’s features, please have a look at its official website. Now let’s see Armeria in action with the help of a small chat application.\nAbout PoC\nWe are going to build a single group chat application comprising a backend service in Scala using the Armeria framework and a frontend service using React/Javascript.\nPrerequisites\nTo get started with the project, we have to have some required prerequisites.\n\nJava 17 or more installed\nScala installed and sbt\nNode v17 or more\nAnd an IDE of your choice.\n\nSetting up the project\nAssuming all the prerequisites done, create a new project using sbt.\nmkdir chat-app\ncd chat-app\nsbt\nIt setups the project. Now we need to create build.sbt and write the following inside it.\nThisBuild / version := &quot;0.1.0-SNAPSHOT&quot;\n \nThisBuild / scalaVersion := &quot;3.3.0&quot;\n \nlazy val root = (project in file(&quot;.&quot;))\n  .settings(\n    name := &quot;chat-app-retreat&quot;,\n    libraryDependencies ++= Seq(\n      &quot;com.linecorp.armeria&quot;  &quot;upickle&quot; % &quot;3.0.0&quot;\n    )\n  )\nCreate src directory with the following basic structure.\nsrc\n├── main\n│   ├── resources\n│   └── scala\n└── test\n    └── scala\n\nNow reload the project using sbt reload.\nCreate a service handler in Main.scala\nCreating a simple server in Armeria is very simple as below code snippet shows. For now, we only have a single handler on path &quot;/&quot; that responds with OK.\nobject Main:  \n  def main(args: Array[String]): Unit =\n    val port: Int      = 8000\n    val server: Server = newServer(port)\n    val future         = server.start()\n    future.join()\n    logger.info(s&quot;Server is running on port $port&quot;)\n \n  private def newServer(port: Int) =\n    val serverBuilder: ServerBuilder = Server.builder()\n \n    serverBuilder\n      .http(port)\n      .service(\n        &quot;/&quot;,\n        (ctx, req) =&gt; HttpResponse.of(HttpStatus.OK)\n      )\n      .requestTimeout(Duration.ofSeconds(1000))\n      .build()\nA handler or a service is attached to the server using service method. We can also have an annotated controller similar to Spring Boot that can be attached using annotatedService method.\nRunning this and sending request to our server, we get 200 OK. Cool, our server works.\n⟩ curl http://localhost:8000/\n200 OK⏎\nCreating message endpoints\nWe require to create two endpoints for creating and fetching all the messages.\n\nGET /e4r/message\nPOST /e4r/create-message\n\nSupposing we have a ChatController which handlers messages services. Adding it to server would like as shown below.\nprivate def newServer(port: Int) =\n    val serverBuilder: ServerBuilder = Server.builder()\n \n    serverBuilder\n      .http(port)\n      .service(\n        &quot;/&quot;,\n        (ctx, req) =&gt; HttpResponse.of(HttpStatus.OK)\n      )\n\t  .annotatedService(\n        &quot;/e4r&quot;,\n        ChatController(ChatService(new ChatRepository))\n      )\n      .requestTimeout(Duration.ofSeconds(1000))\n      .build()\nAdding two handlers in ChatController\nclass ChatController(chatService: ChatService):\n  @Get(&quot;/message&quot;)\n  def getMessages: HttpResponse =\n    val result = chatService.getMessages\n    HttpResponse.ofJson(HttpStatus.OK, messages)\n \n  @Post(&quot;/create-message&quot;)\n  @RequestConverter(classOf[JacksonRequestConverterFunction])\n  def createMessage(message: MessageDTO): HttpResponse =\n    chatService\n      .createMessage(message.toMessage)\n      HttpResponse.of(HttpStatus.CREATED)\n \nSending request on /e4r/message returns all the messages in group. /e4r/create-message creates a new message.\n⟩ curl -X POST --header &quot;content-type: application/json&quot; -d &#039;{&quot;sender&quot;:&quot;@atul&quot;, &quot;text&quot;: &quot;Hey there!&quot;, &quot;createdAt&quot;: &quot;2023-08-03T09:30:37.258Z&quot;}&#039; http://localhost:8000/e4r/create-message\n201 Created\n \n⟩ curl -q http://localhost:8000/e4r/message | jq\n[\n  {\n    &quot;id&quot;: &quot;d69f836c-eac7-48dc-9138-9d2d40b38f4c&quot;,\n    &quot;sender&quot;: &quot;@atul&quot;,\n    &quot;text&quot;: &quot;Hey there!&quot;,\n    &quot;createdAt&quot;: 1692114770.973873\n  },\n  {\n    &quot;id&quot;: &quot;50ca044a-0f2b-4904-902f-297946c30413&quot;,\n    &quot;sender&quot;: &quot;@atul&quot;,\n    &quot;text&quot;: &quot;Hey there!&quot;,\n    &quot;createdAt&quot;: 1691055037.258\n  }\n]\nImplementing Websocket handler\nTo make the chat experiment real-time, we are required to use web sockets. Web socket is a protocol that creates a full duplex connection among devices. Want to deep dive into web sockets? You can have a read about it over this.\nTalking about Armeria, it has got web socket support in recent versions and that’s why there is no rich documentation on it. However, we managed to get some implementation of web sockets by going through the code and tests.\nLet’s attach a socket service to our server.\n \nval socketService = new SocketService()\nsocketService.on(&quot;new-message&quot;, (data) =&gt; println(data))\nsocketService.on { (data) =&gt;\n    println(&quot;from broadcast&quot;)\n    println(data)\n}\n    \nprivate def newServer(port: Int) =\n    val serverBuilder: ServerBuilder = Server.builder()\n \n    serverBuilder\n      .http(port)\n      .service(\n        &quot;/&quot;,\n        (ctx, req) =&gt; HttpResponse.of(HttpStatus.OK)\n      )\n      .service(\n        &quot;/chat/:clientId&quot;,\n        WebSocketService.builder(socketService).allowedOrigins(&quot;*&quot;).build()\n      )\n\t  .annotatedService(\n        &quot;/e4r&quot;,\n        ChatController(ChatService(new ChatRepository))\n      )\n      .requestTimeout(Duration.ofSeconds(1000))\n      .build()\nWebSocketService builder takes socketService and builds a websocket handler. Method allowOrigins let’s it accept connection from any origin.\nFollowing code snippet shows SocketService.\nclass SocketService extends WebSocketServiceHandler:\n \n  override def handle(ctx: ServiceRequestContext, in: WebSocket): WebSocket =\n\tval writer: WebSocketWriter = WebSocket.streaming()\n    in.subscribe(new Subscriber[WebSocketFrame]:\n      def onSubscribe(s: Subscription): Unit =\n        s.request(Long.MaxValue)\n \n      def onNext(webSocketFrame: WebSocketFrame): Unit =\n        try\n          val frame = webSocketFrame\n          try\n            frame.`type` match\n              case WebSocketFrameType.TEXT =&gt;\n\t\t\t\t// call handlers\n\t\t\t\tgetAllHanders.foreach((cb) =&gt; cb(frame.text()))\n              case WebSocketFrameType.CLOSE =&gt;\n                val closeFrame = frame.asInstanceOf[CloseWebSocketFrame]\n                writer.close(closeFrame.status, closeFrame.reasonPhrase)\n \n              case _ =&gt;\n              // do nothing\n          catch\n            case jsonException: ujson.ParseException =&gt;\n              println(jsonException)\n            case t: Throwable =&gt;\n              writer.close(t)\n          finally println(&quot;finally&quot;)\n        catch\n          case t: Throwable =&gt;\n            println(t)\n \n      def onError(t: Throwable): Unit =\n        writer.close(t)\n \n      def onComplete(): Unit =\n        println(&quot;CLOSED&quot;)\n        writer.close\n    )\n    writer\n \n  def emit(namespace: String, data: String): Unit =\n    this.socketWriters.values.foreach((writer) =&gt;\n      writer.write(this.createPayload(namespace, data, &quot;namespaced&quot;))\n    )\n \nSocketService extends WebSocketServiceHandler. The important method to implement is handle which triggers every time any client connects with the server.\nArmeria implements Websocket using reactive streams. So, a subscriber is attached to the web socket reactive stream in on every new connection. The writer stream is used to write data to the connected client socket.\nwriter.write(&quot;Hello world&quot;)\nemit is the method that takes a namespace and data. Any handler on the client side listening on this namespace will get that data.\nSo, whenever a new message is created, all the connected clients have to be notified about the message. This can be accomplished using,\n  class ChatController(chatService: ChatService, socket: SocketService):\n  // ...\n  @Post(&quot;/create-message&quot;)\n  @RequestConverter(classOf[JacksonRequestConverterFunction])\n  def createMessage(message: MessageDTO): HttpResponse =\n\tsocket.emit(&quot;new-message&quot;, message.toJson)\n    chatService\n      .createMessage(message.toMessage)\n      HttpResponse.of(HttpStatus.CREATED)\n \nnew-message is namespace and message.toJson is data.\nLet’s try creating a message and see if we get new message on client socket.\n\nSo, the client is connected to the server. Creating a new message triggers a message event on client socket. The below picture shows the message it got from the server.\n\nOur simple chat application (without any UI 😂) is working.\nImplementing Socket on UI\nSimilar to backend, we have written a wrapper on top of WebSocket. This simplistic wrapper manages namespaces and invokes handlers listening on respective namespaces.\ntype fn = { (data: string): void };\n \nclass ChatSocket {\n  handlers: { [key: string]: fn[] };\n  //...\n  connect() {\n    this.socket = new WebSocket(this.url + `/${this.username}`);\n    this.socket.addEventListener(&#039;open&#039;, () =&gt; {\n      console.log(&#039;socket connected&#039;);\n    });\n \n    this.socket.addEventListener(&#039;message&#039;, (event: MessageEvent) =&gt; {\n      const { namespace, data } = JSON.parse(event.data);\n      this.handlers[namespace].forEach((cb) =&gt; cb(data));\n    });\n \n    this.socket.addEventListener(&#039;close&#039;, (event) =&gt;\n      console.log(&#039;Closed&#039;, event)\n    );\n \n    return this;\n  }\n  //...\n}\nTo register a handler, wrapper implements an on method.\nclass ChatSocket {\n  //...\n  on(namespace: string, cb: fn): void {\n    const handlers = this.handlers[namespace];\n \n    if (handlers) {\n      handlers.push(cb);\n      return;\n    }\n \n    this.handlers[namespace] = [cb];\n  }\nUsing Socket wrapper in Chat component\nAs of now, we are only listening on new messages. We are not sending messages using socket. Messages are created using HTTP REST request only.\nFollowing shows usages of socket wrapper in a chat component.\nconst Chats = ({ chatService, username }: ChatAppProps) =&gt; {\n  //...\n \n  useEffect(() =&gt; {\n    //...\n    const chatSocket = new ChatSocket(\n      &#039;ws://localhost:8000/chat&#039;,\n      username\n    ).connect();\n \n    chatSocket.on(&#039;new-message&#039;, (message: Message) =&gt;\n      setMessages((prevMessages: Message[]) =&gt; [...prevMessages, message])\n    );\n  }, []);\n  //...\n};\nuseEffect hook initialises the socket and socket starts listening on new-message namespace. Whenever a new message comes, setMessages updates the messages state and the component updates.\nConclusion\nArmeria is a newly developed framework which is currently in development. It provides several powerful features such as allowing to run different services within a single application and on the same port. It also provides asynchronous execution out of the box. So, it can handle multiple requests without us implementing concurrency and get our hands dirty in threads and concurrency. If you are looking for a lightweight, fast, asynchronous, and ready-to-use REST API framework, it is a good choice to go with.\nHowever, being a new framework, the documentation is undeveloped. We tried to find the implementation of web sockets by going inside the tests they’ve created.\nAcknowledgements\nWe would like to thank Satya and the Retreat team for organising such an interesting Retreat. Thanks to all the team members (Shubham Chauhan and Atul) who have chosen this trivial problem statement to explore.\nReferences\n\nArmeria Official Docs. URL: armeria.dev/docs/\nArmeria Github Repo. URL: github.com/line/armeria\nTrustin Lee — Armeria: A microservice framework well-suited everywhere. Youtube. URL: www.youtube.com/watch%2CJoker%D0%B8JUGru\nArmeria: A microservice framework well-suited everywhere. URL: speakerdeck.com/trustin/armeria-a-microservice-framework-well-suited-everywhere\n"},"index":{"title":"index","links":[],"tags":[],"content":"This is the home page for bitPhile. You will find most of the informative content here."},"notes/CPython/Asyc-Server":{"title":"Asyc Server","links":[],"tags":[],"content":"Asyc Server\nDescription\nThis article goes through the thorogh theory and practical information of creating an async server that can handle multiple clients at the same time. We would start with simple socket server that handles only one client and then gradually improve our server.\nLet’s get started…\nSimple echo server\n# server.py\n \nimport socket\n \ndef handle_connection(sock: socket.socket):\n    while True:\n        received_data = sock.recv(4096)\n        if not received_data:\n            break\n        sock.sendall(received_data)\n \n    print(&#039;Client disconnected&#039;, sock.getpeername())\n    sock.close()\n \ndef run_server(host: str, port: int) -&gt; None:\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&#039;Server is listening on {}:{}&#039;.format(host, port))\n    while True:\n        client_sock, addr = sock.accept()\n        print(&#039;Connetion from&#039;, addr)\n        handle_connection(client_sock)\n \nif __name__ == &#039;__main__&#039;:\n    run_server(&#039;localhost&#039;, 3000)\nNow, when we run it and connect with it, only one connection can be made at single point of time. For next connection the already existing connection has to end first.\npython3 server.py\n \n# client\nnc localhost 3000\nhello\nhello\nIntroducing Threads\nLet’s use threads for multiple connections:\n \n# server.py\n \nimport socket\nimport threading\n \ndef handle_connection(sock: socket.socket):\n    while True:\n        received_data = sock.recv(4096)\n        if not received_data:\n            break\n        sock.sendall(received_data)\n \n    print(&#039;Client disconnected&#039;, sock.getpeername())\n    sock.close()\n \ndef run_server(host: str, port: int) -&gt; None:\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&#039;Server is listening on {}:{}&#039;.format(host, port))\n    while True:\n        client_sock, addr = sock.accept()\n        print(&#039;Connetion from&#039;, addr)\n\t\t\t\tthread = threading.Thread(target=handle_connection, args=[client_sock])\n\t      thread.start()\n \nif __name__ == &#039;__main__&#039;:\n    run_server(&#039;localhost&#039;, 3000)\n \nSo, this let’s us connect multiple clients but still for each connection you need to have a thread which is expensive.\nUsing Thread pools\nTo control the number of threads being used, we can use thread pools\nfrom concurrent.futures import ThreadPoolExecutor\n \nsockets = set()\n \npool = ThreadPoolExecutor(max_workers=20)\n \ndef handle_connection(sock: socket.socket):\n    while True:\n        received_data = sock.recv(4096)\n        if not received_data:\n            break\n        sock.sendall(received_data)\n \n    print(&#039;Client disconnected&#039;, sock.getpeername())\n    sock.close()\n \ndef run_server(host: str, port: int) -&gt; None:\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&#039;Server is listening on {}:{}&#039;.format(host, port))\n    while True:\n        client_sock, addr = sock.accept()\n        print(&#039;Connetion from&#039;, addr)\n\t\t\t\tpool.submit(handle_connection, client_sock)\n \nif __name__ == &#039;__main__&#039;:\n    run_server(&#039;localhost&#039;, 3000)\nIf you create 20 connections, to make one more connection, you need to wait for the atleast one to get end.\nLet’s use IO multiplexing\nThe major problem with our socket is that it has to wait for any read or write that blocks the program execution. How cool that would be if someone notifies us that there is some update on the socket and run a callback? Cool huh…\nPython provides IO multiplexing module selectors  that we can use. It provides different selectors and one of them is DefaultSelector .\nTo register a socket, we do\nsel = selectors.DefaultSelector()\n \nsel.register(sock, selectors.EVENT_READ, data)\nsel.select() returns a list of (key, events) tuple and each tuple describes a readu socket.\n\nkey is an object with socket object key.filobj and auxiliary data key.data.\nevents are bitmasks of events ready on the socket.\n\nSo, [server.py](server.py) with selectors\n# server.py\n \nimport selectors\nimport socket\n \nsockets = set()\n \ndef broadcast(broadcaster, message):\n    for sock in sockets:\n        if broadcaster is not sock:\n            sock.sendall(message)\n \ndef handle_connection(sock: socket.socket):\n    received_data = sock.recv(4096)\n    if received_data:\n        broadcast(sock, received_data)\n    else:\n        print(&#039;Client disconnected:&#039;, sock.getpeername())\n        sel.unregister(sock)\n        sock.close()\n \nsel = selectors.DefaultSelector()\n \ndef accept(sock: socket.socket):\n    client_sock, addr = sock.accept()\n    print(&#039;Client connected from {}&#039;.format(addr))\n    sockets.add(client_sock)\n    sel.register(client_sock, selectors.EVENT_READ, handle_connection)\n \ndef setup_server_socket(host: str, port: int):\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&#039;Server is listening on {}:{}&#039;.format(host, port))\n    sel.register(sock, selectors.EVENT_READ, accept)\n \ndef run_event_loop():\n    while True:\n        for key, _ in sel.select():\n            cb = key.data\n            sock = key.fileobj\n            cb(sock)\n \nif __name__ == &#039;__main__&#039;:\n    setup_server_socket(&#039;localhost&#039;, 3000)\n    run_event_loop()\nThis approach also has  a problem. We are writing to the socket and if the writing queue is full, it will block the execution. Which means, handle_connection should be broken into two functions to write and read from the socket.\nThis works for the simple applications but for the larger applications this doesn’t work.\nGenerators as Coroutines\nWe can use yield return the control back to the caller. This way we can achieve context switching as run code irrespective of blocking statements. The easiest thing we can do it to put yield above every blocking statements.\nAbout yield\nThis is used to create generators. What it does is returns the control back to the caller if yield gets executed. For example,\n1. def create_counter():\n2. \t\tfor i in range(10):\n3. \t\t\tyield i\n4.       \n \ncounter = create_counter()\n \nnext(counter) # gives 0\nnext(counter) # gives 1\n#...\nSo, it returns the control back to the caller if yield gets executed. In the context of generator, if generator is called again, it resumes where it left of. For examle, next(counter) returns 0 and controls goes back to calling location. When next(counter) called again generator start executing from line number 4 . It goes through the loop, i becomes 1 and again yield happens and it returns 1.\nLet’s look another example for more details\ndef generator():\n\t\tprint(&#039;started&#039;)\n\t\tprint(&#039;before yielding 1&#039;)\n\t\tyield 1\n\t\tprint(&#039;before yielding 2&#039;)\n\t\tyield 2\n \ng = generator()\nnext(g) # prints &#039;started&#039;, &#039;before yielding 1&#039; and returns 1\nnext(g) # prints &#039;before yielding 2&#039; and returns 2\nnext(g) # raises StopIteration exception and there is no more yield statement\nUsing generator in our problem\nSo, we can put yield wherever there is blocking code. Also, we need an event loop the runs all the generators (or coroutines).\n# server.py\nfrom collections import deque\nimport socket\n \nclass EventLoop:\n    def __init__(self):\n        self.tasks_to_run = deque([])\n \n    def create_task(self, coro):\n        self.tasks_to_run.append(coro)\n \n    def run(self):\n        while self.tasks_to_run:\n            task = self.tasks_to_run.popleft()\n            try:\n                print(&quot;running task&quot;, task)\n                next(task)\n            except StopIteration:\n                print(&#039;STOPITERATION FOR ==&gt; &#039;, task)\n                continue\n            self.create_task(task)\n \ndef run_server(host=&#039;127.0.0.1&#039;, port=55555):\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&quot;Server is running on {}:{}&quot;.format(host, port))\n    while True:\n        print(&quot;in run_server while loop&quot;)\n        yield\n        client_sock, addr = sock.accept()\n        print(&#039;Connection from&#039;, addr)\n        loop.create_task(handle_client(client_sock))\n \ndef handle_client(sock):\n    print(sock)\n    while True:\n        yield\n        received_data = sock.recv(4096)\n        print(received_data)\n        if not received_data:\n            break\n        yield\n        sock.sendall(received_data)\n \n    print(&#039;Client disconnected:&#039;, sock.getpeername())\n    sock.close()\n \nloop = EventLoop()\n \nif __name__ == &#039;__main__&#039;:\n    loop.create_task(run_server(&#039;localhost&#039;, 3000))\n    loop.run()\nSo, there is an event loop which has two methods of creating tasks and running all of them sequentially. It’s using deque .\nFlow\n\nWe create event loop loop and create task where run_server() returns a generator that gets added to the queue. Then we start the loop.\nLoop takes the leftmost task from the queue, runs it using next() that executes the generator until next yield comes. For the first time when run_server generator is getting executed, socket gets setup, we goes into the loop and we encouters our first yield that returns control back to the loop.\nLoop again takes the same task as there is no task, continue executing the task. As before it was yield , now it executes accept() that blocks the execution and waits for the connection.\nOnce a client is connected, control moves forward. One more task is created of connection handler. Execution continues there in run_server loop. It goes to the loop begining and prints in run_server while loop where again yield comes and control goes back to the event loop.\nNext task is then executed which is handler. It goes like that.\n\nDid you see the problem here?\nThere is a problem. Though there is some sort of concorrency but still another task has to wait for the previous task to get completed (or until next yield comes in the task).\nSo, if a client is connected and there is handler task get control, if client writes something on the socket, it won’t be written back as the\nyield\n        sock.sendall(received_data)\nmakes control go back to the event loop and run_server takes control and it waits for the new connection. When a new connection is made then control goes back to handler and then data is written back.\nSolving this issue with IO multiplexing\nWe can combine both generators and IO multiplexing to improve our solution. We have modify our EventLoop class.\n# event loop class\n \nclass EventLoop():\n    def __init__(self):\n        self.tasks = deque([])\n        self.sel = selectors.DefaultSelector()\n \n    def create_task(self, task):\n        self.tasks.append(task)\n \n    def run(self):\n        while True:\n            print(&#039;tasks&#039;, self.tasks)\n            if self.tasks:\n                task = self.tasks.popleft()\n                try:\n                    op, sock = next(task)\n                except StopIteration:\n                    continue\n \n                if op == &#039;read&#039;:\n                    self.sel.register(sock, selectors.EVENT_READ, task)\n                elif op == &#039;write&#039;:\n                    self.sel.register(sock, selectors.EVENT_WRITE, task)\n                else:\n                    raise ValueError(&#039;Invalid event loop operation type&#039;, op)\n            else:\n                for key, _ in self.sel.select():\n                    sock = key.fileobj\n                    task = key.data\n                    self.sel.unregister(sock)\n                    self.create_task(task)\nSo, what change we made in EventLoop class is that we get the task, run next. Now, yield in generator returns some information. It returns the operation ( read or write) and the particular socket.\nWhat we do is, according the operation code, we register the socket with selectors. So, if there is no task in the queue, we wait for some update on the socket using sel.select() method. We get the socket, and the task, unregister the socket and put back the same task into the queue where it gets executed.\ndef run_server(host=&#039;127.0.0.1&#039;, port=55555):\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&quot;Server is running on {}:{}&quot;.format(host, port))\n    while True:\n        print(&quot;in run_server while loop&quot;)\n        yield &#039;read&#039;, sock\n        client_sock, addr = sock.accept()\n        print(&#039;Connection from&#039;, addr)\n        sockets.add(client_sock)\n        loop.create_task(handle_client(client_sock))\n \ndef broadcast(sender_sock, message):\n    for sock in sockets:\n        if sock is not sender_sock:\n            sock.sendall(message)\n \ndef handle_client(sock):\n    print(sock)\n    while True:\n        yield &#039;read&#039;, sock\n        received_data = sock.recv(4096)\n        print(received_data)\n        if not received_data:\n            break\n        yield &#039;write&#039;, sock\n        broadcast(sock, received_data)\n        # sock.sendall(received_data)\n \n    print(&#039;Client disconnected:&#039;, sock.getpeername())\n    sock.close()\n \nloop = EventLoop()\n \nif __name__ == &#039;__main__&#039;:\n    loop.create_task(run_server(&#039;localhost&#039;, 3000))\n    loop.run()"},"notes/CPython/Async-IO":{"title":"Async IO","links":[],"tags":[],"content":"Async IO\nIt’s single threaded, single process design. It uses cooperative multitasking.\nCoroutines\nCoroutines are function that can suspend its execution and pass control to another coroutine.\nRead More"},"notes/CPython/Pandas":{"title":"Pandas","links":[],"tags":[],"content":"Pandas\nPandas is a python library used for data structures and data analysis.\nLoading CSV files\nimport pandas\ndf1 = pandas.read_csv(&quot;filename.csv&quot;)\ndf1\npandas.read_csv() returns a DataFrame.\nCSV file without header\ndf = pandas.read_csv(&quot;csv_without_header.csv&quot;, header=None)\ndf.columns = [&quot;col1&quot;, &quot;col2&quot;,...]\ndf\nSetting index\ndf.set_index(&quot;ID&quot;)\n# this will create a new data frame with ID set as index\n \ndf.set_index(&quot;ID&quot;, inplace=True)\n# this won&#039;t create new data frame. It will modify the same frame\nBut the second statement has a problem. If another index is set of it, the old index column would be deleted i.e. ID column be will be dropped. This can be solved using\ndf.set_index(&quot;ID&quot;, inplace=True, drop=False)\nAccessing DataFrames\nUsing labels\ndf.loc([row_start:row_end, col_start:col_end])\n \ndf1.loc[2:3, &quot;Address&quot;:&quot;City&quot;]\nUsing indexing\n# syntax df1.iloc[row_index_start: row_index_end, col_index_start: col_index_end]\n \ndf1.loc[2:3, 3:5]"},"notes/CPython/Python":{"title":"Python","links":["projects/tech/CPython/Python","notes/CPython/multi-threading","notes/CPython/Async-IO","projects/tech/CPython/References","notes/CPython/Pandas","notes/CPython/opencv-python","notes/CPython/Asyc-Server","notes/CPython/poetry"],"tags":[],"content":"Python\nTable Of contents\nCpython\nA type of implementation of python programming language. It’s written in\nDeclaring and initializing variable\nname = &#039;nitin&#039;;\nname, age = &#039;nitin&#039;, 21;\n&gt;&gt;&gt; name, age = &#039;nitin&#039;, 21, 23\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nValueError: too many values to unpack (expected 2)\n&gt;&gt;&gt; name, age, status = &#039;nitin&#039;, 21\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nValueError: not enough values to unpack (expected 3, got 2)\n&gt;&gt;&gt;\nOperators\n\n// : floor division\n** : power\n\nData types\n\nint\nfloat\ndecimal\nfraction\ncomplex number\nstrings\n\nprint()\nr for printing raw string\nprint(&#039;hello world\\nbye world&#039;); # prints with new line\nprint(r&#039;hello world\\nbye world&#039;); # prints in single line with &#039;\\n&#039;\nend keyword for specifying what to insert at the end\nprint(&#039;hello world&#039;, end=&#039;,&#039;); #prints &#039;hello world,&#039; without new line\nString\ngreeting = &#039;good morning&#039;;\ngreeting = &quot;good morning&quot;;\nMulti-line string\ngreetings = &#039;&#039;&#039;\n\tGood morning\n\tWelcome everyone\n&#039;&#039;&#039;;\n \ngreetings = &quot;&quot;&quot;\n\tGood morning\n\tWelcome everyone\n&quot;&quot;&quot;;\n \ngreetings = &quot;&quot;&quot;\\\n\tGood morning\n\tWelcome everyone\n&quot;&quot;&quot;; # &#039;\\&#039; prevents leading new line\nTwo or more string literals are concated automatically\n&gt;&gt;&gt; &#039;good&#039; &#039; morning&#039;;\n&#039;good morning&#039;\nStrings can be indexed\nproject[0];   # returns &#039;y&#039;\nproject[-1];  # returns &#039;a&#039;\n\nIndices can be negative to start from the right.\nStrings are immutable here also\n\nSlicing\n&#039;&#039;&#039; Sytax :\n\tstring[startIndex:endIndex];\n&#039;&#039;&#039;\n \nproject[0:1]; # returns &#039;y&#039;\nproject[0:4]; # returns &#039;yask&#039;\n \nproject[:4]; # returns &#039;yask&#039;. defaults to 0\nproject[1:]; # returns &#039;aska&#039;. defaults to string length\nString length\nlen(project); # returns 5\nString formatting\nMethod 1\nname = &#039;Nitin&#039;\nsurname = &#039;Sharma&#039;\nprint(&quot;Hello %s&quot; % name)\nprint(&quot;Hello %s %s&quot; % (name, surname))\nThis method is used in old versions of pythons.\nMethod 2\nprint(f&quot;Hello {nitin}&quot;)\nMethod 3\nprint(&quot;Hello {}&quot;.format(name))\nList\nList is a collection of elements of same type or different types. It’s more likely arrays in javascript.\npreferences = [&#039;bengalore&#039;, &#039;pune&#039;, &#039;gurgaon&#039;];\npreferences[0]; # returns &#039;bengalore&#039;\n \nlen(preferences); #returns list length i.e. 3\n \n# elements can be changed using indices\npreferences[0] = &#039;pune&#039;;\npreferences[1] = &#039;bengalore&#039;;\n \n# Assignment to slices is also possible\n \npreferences[0:1] = &#039;Pune&#039;;\npreferences; # returns [&#039;Pune&#039;, &#039;bengalore&#039;, &#039;gurgaon&#039;]\n \npreferences[:] = []; # clears the list\nList in a list\ninterests = [&#039;typing&#039;, &#039;reading&#039;];\nperson = [&#039;nitin&#039;, 21, interests];\n \ninterests.append(&#039;traveling&#039;);\nperson[2];  # it also gets &#039;traveling&#039;, i.e. list reference is stored\n\nNote : for equal to operator, the list items are checked. It’s not javascript where references are compared\n\ninterests = [&#039;typing&#039;, &#039;reading&#039;];\n_interests = [&#039;typing&#039;, &#039;reading&#039;];\n \ninterests == _interests; # True\n \n_interests.append(&#039;travel&#039;);\ninterests == _interests; # False\nList comprehension\nList comprehension provides concise way to create lists\nnums = [1, 2, 3, 4]\nevens = [x for x in nums if x % 2 == 0]\n \nprint(evens) # [2, 4]\nLook, how concise it is.\nIt starts with an expression followed by for statement. That expression is compulsory.\nWith if-else\ndef _filter(items: list) -&gt; list:\n    return [item if type(item) is int else 0 for item in items]\nSo, for if-else, the position of the components has to change\nTuples\nTuples are similar to lists but these are immutable.\nnames = &#039;nitin&#039;, &#039;hemant&#039;\nprint(names) #(&#039;nitin&#039;, &#039;hemant&#039;)\n \nnames = () # for empty tuple\n \nnames = &#039;nitin&#039;, # for single value in tuple. Trailing , is needed\n \n# unpacking the tuple\nname1, = names # name1 contains &#039;nitin&#039;\nNumbers of variables on the left hand side should be equal to size of tuple.\nnames = &#039;nitin&#039;, &#039;hemant&#039;\nmy_name, = names # won&#039;t work\n \nname1, name2 = names; # will work\nThis is called sequence unpacking.\n\nNote that multiple assignment is really just a combination of tuple packing and sequence unpacking\n\nSet\nIt’s an unordered collection of items.\nnames = {&#039;nitin&#039;, &#039;hemant&#039;}\n# Or\nnames = set([&#039;nitin&#039;, &#039;hemant&#039;])\n \nnames = set([&#039;nitin&#039;, &#039;hemant&#039;, &#039;nitin&#039;])\nprint(names) # {&#039;nitin&#039;, &#039;hemant&#039;}\nSet operations\nIt also supports set operations\nset1 = {1, 2, 3, 4}\nset2 = {1, 4, 6, 9}\n \nset1 - set2 # {2, 3}, in set1, not in set2\nset1 &amp; set2 # {1, 4} in both\n \nFunction\ndef greet():\n\tprint(&#039;Good morning!&#039;)\nKeyword Arguments\ndef greet(age, name):\n\tprint(&#039;Good morning&#039;, name)\n \ngreet(12, name=&#039;nitin&#039;);\n\nKeyword argument should come after positional argument. If positional argument comes after keyword argument, interepreter throws error.\n\ngreet(name=&#039;nitin&#039;, 12) # won&#039;t work\ngreet(12, name=&#039;nitin&#039;)\n*args and **keywordArgs\ndef function(arg, *args, **keywordArgs):\n\tprint(arg)\n\tprint(args)\n\tprint(keywordArgs)\n\narg is a positional argument,\nargs is a tuple having other dynamic arguments,\nkeywordArgs is a dict having keyword and value\n\nfunction(&#039;hello&#039;, &#039;world&#039;,&#039;bye world&#039;, name=&#039;nitin&#039;, surname=&#039;sharma&#039;);\n# output\nhello\n(&#039;world&#039;, &#039;bye world&#039;)\n{&#039;name&#039;: &#039;nitin&#039;, &#039;surname&#039;: &#039;sharma&#039;}\n\n*args should be before **keywordArgs .\n\nSpecial parameters\ndef f(pos1, pos2, /, pos_or_kwd, *, kwd1, kwd2):\n      -----------    ----------     ----------\n        |             |                  |\n        |        Positional or keyword   |\n        |                                - Keyword only\n         -- Positional only\nNames of positional only parameter can be used in keyword only parameters.\ndef foo(name, /, **kwds):\n\treturn &#039;name&#039; in kwds\n \nfoo(1, name=&#039;nitin&#039;) # return True\nfoo(1, **{name: &#039;niitn&#039;}) # return True\nUnpacking Argument list\n# unpacks list\nargs = [1, 2]\nlist(range(*args))\n \n# unpacks dictionary\ngreet(**{&#039;name&#039;:&#039;nitin&#039;, &#039;age&#039;: 21});\nLambda Expressions\nis_even = lambda number: number % 2 == 0\n \ndef filter(array: list, predicate):\n  resArray = []\n  for item in array:\n    predicate(item) and resArray.append(item)\n  \n  return resArray\n \nprint(filter([1, 2, 3, 4], is_even))\nModule\nfunction.py\ndef greet():\n\tprint(&#039;Good morning&#039;)\nimport function\n \nfunction.greet() # Good morning\nfunction module is in global space where it is imported.\nIt’s like require in node. The module file is evaluated and all the functions and variables can be imported.\ndef fun():\n\timport function\n\tfunction.greet()\n \nfun() # import function in it&#039;s space and calls function.greet\n \nfunction.greet() # won&#039;t work as it&#039;s not in global space of the file\n# different types of import\n \nfrom function import greet\n \nfrom function import *\nAll of these imports get the imported function or variables from the module into the global scope of the file.\nimport function as fun\n \n# or \nfrom function import greet as printGreet\n \nfun.greet()\nprintGreet()\nPackages\nA package is a directory having modules or python files.\napp\n├── app.py\n└── src\n    ├── components\n    │   ├── __init__.py\n    │   └── header.py\n    └── layout.py\n\nimport app.app\n \napp.App()\nThis is one way to import module from package.\nfrom app.app import App\n \nApp()\nThis is another way to import module. It loads the module and puts the function in the namespace.\nImporting * from package\nfrom app import *\nThis doesn’t work until we put __init__.py in the package.\napp\n├── __init__.py\n├── app.py\n└── src\n    ├── components\n    │   ├── __init__.py\n    │   └── header.py\n    └── layout.py\n\napp/__init__.py\n__all__ = [&quot;app&quot;]\n__all__ is a list having all the modules to make avaiable for  import * So, app  module gets imported the following works.\nfrom app import *\n \napp.App()\n&quot;&quot;&quot;\nThis is app layout\nThis is a header\nThis is App\n&quot;&quot;&quot;\nAbsolute imports and relative imports\nIn file layout.py\n\nAbsolute import\n\nfrom app.src.components.header import Header;\n \ndef Layout():\n  print(&#039;This is app layout&#039;)\n  Header()\n\nRelative import\n\nfrom .components.header import Header;\n \ndef Layout():\n  print(&#039;This is app layout&#039;)\n  Header()\nPython Namespaces and Scopes\nNamespace\nA namespace is a collection of unique names. Name in a namespace can be same as in another namespace.\nAttributes of a dictionary are also names in dictionary namespace.\n\nNote : The important thing to know about namespaces is that there is absolutely no relation between names in different namespaces\n\nTypes of Namespaces\n\nBuilt-in : python built-in\nGlobal : in a file or module\nLocal: in function or method\n\nClasses\nclass SomeClass:\n\tdef __init__(self, arg1, arg2):\n\t\tself.arg1 = arg1;\n\t\tself.arg2 = arg2;\n \n\tdef do_something(self):\n\t\tpass\n\t\t//do something\n \ninstance = SomeClass(1, 2)\ninstance.arg1\ninstance.arg2\ninstance.do_something()\nself has to pass as first argument to class function.\nClass function and Instance method\nThere is a difference between class function and instance method. An instance method is created using class function.\nWhen instance.do_something() is called, the attribute do_something is searched in the class that appears to be a function. instance is bounded with that class function and called with argument list.\nPrivate variables\nThere is way to define private methods and variables in python. It’s all convention here. Variable starting with _ are considered as private.\nclass S:\n\tdef __init__(self):\n\t\tself._private_attribute = 2\nCallable class instances\nA class instances can be called if class implements __call__ method.\nclass Counter:\n  def __init__(self, start=0):\n    self.count = start\n  \n  def __call__(self):\n    self.count += 1\n    return self.count\n \ncounter = Counter()\n \ncounter() # returns 1\ncounter() # returns 2\nIterator\nAn iterator has __next__() object method that gives the next value. for does the same thing, it gets iterator of the list and calls this __next__() method.\nTo create iterator, the class should have __next__() and __iter__() functions. for loop calls this __iter__() method to get iterator.\nclass Reverse:\n  def __init__(self, items: list):\n    self.items = items\n    self.index = len(items)\n  \n  def __iter__(self):\n    return self\n  \n  def __next__(self):\n    if self.index == 0:\n      raise StopIteration\n    \n    self.index -= 1\n    return self.items[self.index]\nFile IO\nf = open(&#039;file&#039;, &#039;r&#039;, encoding=&#039;utf-8&#039;)\nf.read()\nJSON in Python\nimport json\n \nnums = [1, 2, 3, 4]\n \njson.dumps(nums) # returns json of nums\n \njson.dump(nums, fileObject) # writes the nums json to file\nLoading Json files\njson.load(fileObject)\nErrors and Exceptions\nBasic Syntax\ntry:\n\t# some code that might raise an exception\nexcept Exception:\n\t# code that handles exception\nelse:\n\t# code that executed when try block gets executed successfully\nelse should follow all except clauses.\nexcept can take multiple type of Exceptions together\nexcept (ExceptionType1, ExceptionType2...):\nThere can be multiple except blocks.\nRaising Exceptions\nExceptions are raised using raise statement.\ntry:\n\traise Exception(&#039;demo&#039;)\nexcept Exception as ex:\n\tprint(ex)\nException Chaining\nAn exception can cause another exception to raised.\ntry:\n\topen(&#039;some-bad-file&#039;, &#039;r&#039;)\nexcept OSError:\n\traise RuntimeError(&#039;unable to process&#039;)\nThis flows like, due to OSError another exception RuntimeError occured.\nTo make an exception direct consquence of another, from clause is used\ntry:\n\topen(&#039;some-bad-file&#039;, &#039;r&#039;)\nexcept OSError as file_error:\n\traise RuntimeError(&#039;unable to process&#039;) from file_error\nNow, file_error is the direct cause of RuntimeError .\nThis chaining can be disabled by using from None\ntry:\n\topen(&#039;some-bad-file&#039;, &#039;r&#039;)\nexcept OSError:\n\traise RuntimeError(&#039;unable to process&#039;) from None\nSo, only RuntimeError will be raised.\nfinally block\ntry:\n\t# some code that might raise an exception\nexcept:\n\t# exception handling code\nfinally:\n\t# code that will run always\nSome points to remember:\n\nReturn value will be taken from the finally even though try returns some value.\nIf try raises some exception, except block handles that. If it doesn’t handle, the finally block will get exectued and the exception will be re-raised.\nIf finally has  break, continue or return, exception won’t be re-raised.\n\nContext Managers\nContext managers allow allocate and release of resources. A common use case of context managers is to lock and unlock resources.\nA context manager basically has two functions\n\n__enter__\n__exit__\n\nwith statement calls this __enter__ method and assigns to as parameter.\nOn any exception, with calls __exit__ method and passes, type, value, and traceback.\nclass FileManager:\n  def __init__(self, filename):\n    self.filename = filename\n  \n  # context manager implementation\n  def __enter__(self):\n    self.fileobject = open(self.filename, encoding=&#039;utf-8&#039;)\n    return self.fileobject\n  \n  def __exit__(self, type, value, traceback):\n    print(&#039;Error:&#039;, type, value, traceback)\n    self.fileobject.close()\n \nwith FileManager(&#039;some-file&#039;) as file:\n\tfile.read()\n \nwith FileManager(&#039;some-file&#039;) as file:\n\tfile.some_undefined_method() # with will call __exit__ method\n \nIf __exit__ method returns True then the exception is handled by __exit__ otherwise with raises exception\nFor more information, read this.\nDecorators\nDecorators are function that wraps a function.\ndef decorator(function):\n  def wrapper():\n    print(&quot;doing something before calling actual function&quot;)\n    function()\n    print(&quot;doing something after calling actual function&quot;)\n  \n  return wrapper\n \ndef greet():\n\tprint(&quot;Good morning peeps!&quot;)\n \ngreet = decorator(greet)\n \ngreet()\nThe same thing can be done using decorator\n@decorator\ndef greet():\n\tprint(&quot;Good morning peeps!&quot;)\n \ngreet()\nDecorating function accepting argument\ndef decorator_with_args(function):\n  def wrapper(*args, **kwargs):\n    print(&quot;before calling actual function&quot;)\n    function(*args, **kwargs)\n    print(&quot;after calling actual function&quot;)\n  return wrapper\nIt is not always necessary that decorate should have wrapper function inside it. The main focus should be on that the decorator takes a function as an argument and returns a function.\n\nNote : With decorator, we are losing information about the actual function. We can’t inspect that function being decorated. For example,\n\n@decorator\ndef greet():\n\tprint(&quot;hi&quot;)\n \ngreet.name # would give wrapper\ngreat.__doc__ # would give doc of wrapper\nTo solve this issue, we use python utility functools package.\nimport functools\n \ndef decorator(function):\n\t@functools.wraps(function)\n  def wrapper():\n    print(&quot;doing something before calling actual function&quot;)\n    function()\n    print(&quot;doing something after calling actual function&quot;)\n  \n  return wrapper\n \ndef greet():\n\tprint(&quot;Good morning peeps!&quot;)\n \ngreet.__name__ # would give greet\nfunctools.wraps is a decorator that takes the function being decorated. What it does is, copies the __doc__ and __name__ information of decorated function given to the wrapper function.\nDecorators with arguments\nA decorator is a function that takes function as a argument. To pass argument to the decorator, we need to make a function that returns decorator.\nimport functools\n \ndef repeat(times):\n  def decorator_repeat(fn):\n    @functools.wraps(fn)\n    def repeat_wrapper(*args, **kwargs):\n      for i in range(times):\n        fn(*args, **kwargs)\n    \n    return repeat_wrapper\n  return decorator_repeat\n \n@repeat(2)\ndef greet():\n\tprint(&quot;Good morning&quot;)\nClass decorators\nClass can be used to make stateful decorators. If we know how decorator works, it would be easy to understand how can be a class used to make decorators.\n@decorator\ndef fn():\n\tpass\nSo, decorator is actually a function which takes fn as argument, wraps this function in a wrapper and returns that wrapper and assigns to same variable fn.\nScenerio with class\n@SomeClassDecorator\ndef fn():\n\tpass\nSo, with this, an instance of the class is created. That instance is assigned to the variable fn. Ofcourse that instance should be callable. And we know how to make class callable.\nclass Countable:\n  def __init__(self, fun):\n    self.fun = fun\n    self.count = 0\n    functools.update_wrapper(self, fun)\n  \n  def __call__(self, *args, **kwargs):\n    self.count += 1\n    self.fun(*args, **kwargs)\n    print(f&quot;Called {self.fun.__name__} {self.count} times&quot;)\n@Countable\ndef greet():\n\tprint(&quot;hi&quot;)\n \n# is equivalent to\ndef greet():\n\tprint(&quot;hi&quot;)\n \ngreet = Countable(greet)\n \nfunctools.update_wrapper is used to update the introspetion details of the instance.\nWhy do we need Decorators\nRead more\nTo look\n\n  Decorators\n  Documentation Strings. Conventions to document.\n  sorted() or .sort() method\n  Inheritance\n  “Compiled” Python files\n  Make interfaces.\n\n  Try to pass classes to function.\n  Try passing drived class to function accepting base class and access drive class methods\n\n\n  Decoratos\n\n  Caching values using decorators\n  Adding Information About Units\n  Validating JSON\n\n\n  dags and airflow (jobs and pipelines)\n\nPoints Noted\n\n\nPython code is synchronous in nature. If there is any blocking code in the infinite loop, the loop will wait for that blocking code to get executed.\nFor example\nwhile True:\n        data = connection.recv(1024)\n        print(&quot;From client: &quot;, data)\n        if not data:\n          print(&quot;Client closed the connection&quot;)\n          break\n        \n        connection.sendall(data)\n\n\nReferences\n\nStandard types\nPython 3 contents\nVariables\nNaming Convenstions\nWalrus operator\nGlob\nPathlib\nSocket Real Python\nPEP\n\n\nmulti-threading\nAsync IO\nReferences\nPandas\nopencv-python\nAsyc Server\npoetry"},"notes/CPython/References":{"title":"References","links":[],"tags":[],"content":"References\ngithub.com/Pierian-Data/Complete-Python-3-Bootcamp"},"notes/CPython/asyncio":{"title":"asyncio","links":[],"tags":[],"content":"It is a library in python to carry out asynchronous tasks. Python is asynchronous by nature.\nCoroutines\nA couroutine is a function that can suspended and resumed.\nFor example,\nasync def fetch(url: str):\n\tprint(&#039;sending request at &#039; + url)\n\tawait asyncio.sleep(2)\n\tprint(&#039;got response from  + url&#039;)\n\treturn url + &#039;?authToken=sdfsd&#039;\nfetch is a couroutine. If call coroutine like,\nfetch(&#039;tw.com&#039;)\n&lt;coroutine object fetch at 0x7f8a6dafba70&gt;\nIt is not called like an ordinary function. We need to run this coroutine using asyncio runner.\nasyncio.run(fetch(&#039;tw.com&#039;))\n&#039;sending request at tw.com&#039;\n&#039;got response from  + url&#039;\n&#039;tw.com#039;\nCoroutine function and coroutine object\nA coroutine function is a function where def is followed by async keyword.\nA coroutine object is an object which is generated when a coroutine function is invoked.\nTask\nA task is a wrapper around coroutine. A task schedules the coroutine concurrently.\nasync def main():\n\ttask = asyncio.create_task(fetch(&#039;tw.io&#039;))\n\tresult = await task\n\treturn result\n.create_task() method takes the coroutine and creates a task. It schedules the task immediately and task can only be completed once it is awaited.\n\nAwaiting a task runs all the tasks which can be finished within the finishing time of that task.  Look following example for that.\n\nasync def main():\n \n    tasks = []\n \n    urls = [{&#039;url&#039;: &#039;tw.com&#039;, &#039;time&#039;: 2}, {&#039;url&#039;: &#039;goo.com&#039;, &#039;time&#039;: 2}, { &#039;url&#039;: &#039;bitphile.com&#039;, &#039;time&#039;: 4}, {&#039;url&#039;: &#039;twi.com&#039;, &#039;time&#039;: 2}]\n \n    print(time.asctime())\n    \n    for url in urls:\n        print(url)\n        tasks.append(\n            asyncio.create_task(fetch(url[&#039;url&#039;], \n        url[&#039;time&#039;])))\n \n        print(&#039;--------&#039;)\n \n    task = tasks[0]\n \n    print(task)\n \n    await task\n \n    print(&#039;--------&#039;)\n \n    print(time.asctime())\nSo, awaiting first task will complete all the tasks having time of 2. Task of time 4 won’t get complete.\nIf we await tasks[3] then all the tasks will be completed getting that task be completed in the last.\nadd_done_callback\nThis is the method on task that gets invoked when the task is completed.\nasync def main():\n    task = asyncio.create_task(fetch(&#039;tw.io&#039;, 2))\n    task.add_done_callback(lambda arg: print(arg))\n    result = await task\n    return result\narg passed to lambda function is the task itself.\nAwaitables\nAnything that can be followed by await keyword.\nFor example,\n\ncoroutines\ntasks\nfutures\n"},"notes/CPython/dis":{"title":"dis","links":[],"tags":["flash-card","python"],"content":"dis\nDisassembles methods, functions, classes and other objects in python.\n \nimport dis\n \ndef inc(x):\n    x += 1\n    return x\n \ndis.dis(inc)\n \n#  2           0 LOAD_FAST                0 (x)\n#              2 LOAD_CONST               1 (1)\n#             4 INPLACE_ADD\n#             6 STORE_FAST               0 (x)\n#\n#  3           8 LOAD_FAST                0 (x)\n#             10 RETURN_VALUE"},"notes/CPython/event-loop":{"title":"event-loop","links":["tags/language","tags/programming-language","tags/python","tags/concurrent","tags/parallelism","tags/asyncio","tags/loop","tags/event-loop"],"tags":["language","programming-language","python","concurrent","parallelism","asyncio","loop","event-loop"],"content":"tags:languageprogramming-languagepythonconcurrentparallelismasyncioloopevent-loop\nEvent Loop\nWhat is an Event loop?\nAn event loop is a loop which has a list of tasks and it attempts to progress them in a sequence in each iteration of the loop. Along with the execute of the tasks, it also executes callbacks and IO handlings.\nAn event loop is the driver code that manages the cooperative multitasking.\nThis is the core part of asyncio library to create and run asynchronuous code in python.\nEvent loop implementations\nEvent loop is an abstract class which has to be implemented explicility according the requirements. There are several event loop implentations are avaiable.\n\nasyncio.DefaultEventLoopPolicy\nasyncio.ProactorEventLoop\nuvloop a high performance event loop implementation based on libuv library which is used in node.js for its event loop.\nasyncio.SelectorEventLoop\n\nLet’s use uvloop implementation.\npip install uvloop\n \nimport asyncio\nimport uvloop\n \nuvloop.install()\n \nloop = asyncio.new_event_loop()\n \n#&lt;uvloop.Loop running=False closed=False debug=False&gt;\nEvent loop APIs\ncall_soon\nThis is the method of event loop to schedule a callback.\nFor instance,\ndef greet():\n    print(&quot;greetings&quot;)\n \nloop = asyncio.new_event_loop()\n \nloop.call_soon(greet)\n \nloop.run_forever()\nIt should print greetings.\nThere may times when we required to schedule a coroutine. Howeer, call_soon only takes function, not a coroutine. We can do by using asyncio.ensure_future which a method that creates a schedules a future wrapping the coroutine.\n \nasync def greet():\n    await asyncio.sleep(2)\n    print(&quot;greetings&quot;)\n \nloop.call_soon(asyncio.ensure_future, greet())\n \nloop.run_forever()\nSo, asyncio.ensure_future is scheduled as callbacks that takes greet() coroutine as an argument.\nReferences\n\n# Asyncio (superseded by async)\nAsync IO Event Loop\n\n"},"notes/CPython/executor":{"title":"executor","links":[],"tags":[],"content":"Executor is a class concurrent.futures.Executor which is used of offload function calls asynchronously. It immediately returns a future object which can be used to get the result.\n \ndef blocking_io(label: int, bytes: int):\n    time.sleep(4)\n    print(f&#039;--------&gt; FOR {label} &lt;---------&#039;)\n    \n    with open(&#039;/usr/share/dict/words&#039;, &#039;r&#039;) as f:\n        print(f.read(bytes))\n        \n    print(&#039;-----------------------------&#039;)\n \nexecutor = concurrent.futures.ThreadPoolExecutor()\n \nhandler = executor.submit(blocking_io, 1, 100)\n \nhandler.add_done_callback(\n                    lambda future: print(future))\nThere are different Executor available in concurrent.futures such as ThreadPoolExecutor to execute tasks in Threads pool.\nWe also have ProcessPoolExecutor for cpu bound tasks."},"notes/CPython/generators":{"title":"generators","links":["tags/language","tags/computer","tags/programming-language","tags/program"],"tags":["language","computer","programming-language","program"],"content":"tags:languagecomputerprogramming-languageprogram\nGenerators\nSpecial kind of functions which returns a lazy iterator.\nFor example, a file reader that files lazily.\n&quot;file_reader.py&quot;\n \n \ndef file_reader(filename):\n    print(&#039;above with&#039;)\n    with open(filename, &#039;r&#039;) as file:\n        print(&#039;below with&#039;)\n        for line in file:\n            yield line\n            print(&quot;after yield&quot;)\n \nGenerator expressions\ncounter = (num in range(10))\nyield\nIt suspends the execution of the function and returns the yield value to the caller. On next generator method call, it continues executing after the yield.\nIt saves the state of the function.\nGenerator methods\n\n.send() : yield is not a statement. It is an expression.\n\ndef counter():\n\ta = 0\n\twhile True:\n\t\tb = (yield a)\n\t\tprint(b)\n\t\ta += 1\nif we do,\nc = counter()\nc.send(12)\n \noutputs:\n0\n12\n1\n12 is assigned to variable b.\n\n.throw() used to throw exception in generator.\n.close() to StopIteration\n\nSee Also\n\ncProfile\n"},"notes/CPython/gil":{"title":"gil","links":[],"tags":["python"],"content":"Global Interpreter Lock (GIL)\nIt is a implementation in CPython which puts a lock on the interpreter in multithreading. Only a single thread can access the interpreter at a time.\nIn IO bound multithreading, threads wait for IO and meanwhile another thread can access the interpreter.\nIn the case of CPU bound multithreading, one thread doesn’t leave the interpreter and executes behaves as single threaded as other threads have to wait for the interpreter to get released.\nPros\n\nMakes single threading faster as there is only lock, that is GIL lock, to maintain.\nGood for IO bound multithreading programs.\n\nCons\n\nNot good for CPU bound and combinations of both IO and CPU bound multithreading programs.\n\nReferences\n\nrealpython.com/python-gil/#what-problem-did-the-gil-solve-for-python\n"},"notes/CPython/interned-objects":{"title":"interned-objects","links":[],"tags":["python","programming"],"content":"Interned Objects\nThese are the cached PyObjects  which python does for optimizations and performance.\n\nAll integers -5 to 255 are cached.\nStrings that contain ASCII letters, digits, or underscores only.\n\nFor example,\na = 1\nb = 1\n \na is b # True\n \nsys.getrefcount(a)\nsys.getrefcount(b)\n \n# both gives same result\nBoth a and b points to the same Python Object (PyObject).\nReferences\n\nrealpython.com/pointers-in-python/#a-note-on-intern-objects-in-python\n"},"notes/CPython/itertools":{"title":"itertools","links":[],"tags":[],"content":"itertools\nProvides functions to deal with iterators. Most commonly used functions are,\n\ncount to create infinite sequence of numbers.\ncycle to create a cycling sequence with given collection.\n\nReferences\n\ndocs.python.org/3/howto/functional.html#the-itertools-module\n"},"notes/CPython/logging/basicConfig":{"title":"basicConfig","links":[],"tags":["python","logs"],"content":"basicConfig\nIt is a method on logging module which tests the basic configurations for the root logger.\nIt can be only called once.\nlogging.basicConfig(level=logging.INFO)\nlogging.basicConfig(level=logging.DEBUG)\nSecond config won’t work.\nAlso, if any of the logging logging method such as .info, .warn etc is called, they implicitly calls this basicConfig with no arguments. So, basic configs can’t be set after calling any of the logging method."},"notes/CPython/logging/logrecord-attributes":{"title":"logrecord-attributes","links":[],"tags":["logs","python"],"content":"LogRecord Attributes\nWhile constructing the format for the log, we can provide LogRecord attributes such asctime, message etc.\nlogging.basicConfig(\n                    format=&quot;%(levelname)s - %(message)s - %(filename)&quot;\n                    )\nReferences\n\ndocs.python.org/3/library/logging.html#logrecord-attributes\n"},"notes/CPython/memory-management":{"title":"memory-management","links":["notes/CPython/pyobject"],"tags":["python","memory-management"],"content":"\nPython code gets converted into bytecode. Then bytecode is interpreted by the interpreter such as Cpython, jython etc.\nWhat is that entity that converts python code to bytecode?\n\nPython Memory Management\nMemory management differs in the implementation of python interpreter (CPython, Jython, etc).\nThis note includes memory management in CPython.\nEverything in Python is an object which has pyobject as internal structure.\nPython Application Memory\nPython application gets memory from OS. It has following roughly structure.\n\nNon-object Memory for python internals objects.\nObject Memory for user objects.\n\n\nMemory structure\nMemory allocated to Object memory is divided into:\n\nArena\nPools\nBlocks\n\nArena are big size memory allocated which has several pools storing same size blocks.\n\nEach pool maintain a double linked list to another pools.\nPool states\n\nusedpools\nfullpools\nfreepools (empty)\n\nusedpools contains references to pools which have some occupied blocks. fullpools has references to fully occupied pools. freepools have references to totally empty pools.\nPool block states\n\nfree\nuntouched\nallocated\n\nReferences\n\nrealpython.com/python-memory-management/#cpythons-memory-management\n"},"notes/CPython/multi-threading":{"title":"multi-threading","links":[],"tags":[],"content":"MultiThreading in Python\nThreadPoolExecutor\nlogging.info(&quot;Submitting thread_function in pool&quot;)\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    executor.map(thread_function, [4])\nlogging.info(&quot;Submitted all the thread_function s &quot;)\nIt holds the control until all the tasks have been completed as we have used context manager with. It does implicit join.\nlogging.info(&quot;Submitting thread_function in pool&quot;)\nexecutor = ThreadPoolExecutor(max_workers=3)\nexecutor.map(thread_function, [4])\nlogging.info(&quot;Submitted all the thread_function s &quot;)\nIt runs the threads and control goes back to main thread. So, first log will come, then thread_function log and immediately last log."},"notes/CPython/mutex-lock":{"title":"mutex-lock","links":[],"tags":["python","operating-system","deadlocks"],"content":"Lock\nTo make threading programming thread safe, we can use locking mechanism python provides. Python provides mutex locking system with threading.Lock.\nlock = threading.Lock()\n \nlock.acquier()\nchange_value()\nupdate_value()\nlock.release()\nIt can be done using with context manager.\nwith lock:\n    change_value()\n    update_value()\nExample\nclass Atom():\n    def __init__(self, value) -&gt; None:\n        self.value = value\n        self._lock = threading.Lock()\n \n    def inc(self, name) -&gt; int:\n        logging.info(&quot;Starting by thread %d&quot;, name)\n        logging.info(&quot;Acquire Lock by thread %d&quot;, name)\n        # with self._lock:\n        self._lock.acquire()\n        logging.info(&quot;Acquired Lock by thread %d&quot;, name)\n        copy = self.value\n        copy += 1\n        time.sleep(0.1)\n        logging.info(&quot;Update by thread %d&quot;, name)\n        self.value = copy\n        self._lock.release()\n        logging.info(&quot;Release Lock by thread %d&quot;, name)\n \n \nif __name__ == &quot;__main__&quot;:\n    format = &quot;%(asctime)s: %(message)s&quot;\n    logging.basicConfig(format=format, level=logging.INFO,\n                        datefmt=&quot;%H:%M:%S&quot;)\n \n    atom = Atom(1)\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        executor.map(atom.inc, list(range(1, 3)))\n \n    logging.info(&quot;MAIN : atom value is %d&quot;, atom.value)\nOutput\n13:48:02: Starting by thread 1\n13:48:02: Acquire Lock by thread 1\n13:48:02: Acquired Lock by thread 1\n13:48:02: Starting by thread 2\n13:48:02: Acquire Lock by thread 2\n13:48:02: Update by thread 1\n13:48:02: Release Lock by thread 1\n13:48:02: Acquired Lock by thread 2\n13:48:02: Update by thread 2\n13:48:02: Release Lock by thread 2\n13:48:02: MAIN : atom value is 3\n"},"notes/CPython/names-not-variables":{"title":"names-not-variables","links":[],"tags":["python","programming"],"content":"Python Names\nPython doesn’t have variables, instead they have names. There is no concept of variables in Python.\n\n\n                  \n                  Note \n                  \n                \n\nThe notion we have of a variable is something which stores value and it can be changed through out the program life cycle. There can be different implementation of in different programming languages. For instance in C, a variable owns a memory location and stores a value in it. If a new value is reassigned, the memory location remains the same but the value is overwritten on the same location.\n\n\nUnderstanding Names\nConsidering the following statement,\na = 1\nFollowing things happen when this statement executes.\n\nA PyObject is created. This PyObject can’t be accessed by user program. It is internal to CPython implementation.\ntypecode is set as integer.\nvalue is set as 1.\nref count is increased by 1.\nname is a is bounded to this PyObject.\n\n\n\n\n                  \n                  Tip\n                  \n                \n\nsys.getrefcount method returns reference count to a PyObject.\n\n\nReferences\n\nrealpython.com/pointers-in-python/\n\n"},"notes/CPython/opencv-python":{"title":"opencv-python","links":[],"tags":[],"content":"opencv-python\nReading images\nimg = cv2.imread(&quot;path/to/image/file.jpg&quot;)\n# img is a numpy array\n \nimg = cv2.imread(&quot;path/to/image/file.jpg&quot;, 0)\n# 0 =&gt; grayscale, 1 =&gt; bgr, -1 =&gt; preserve alpha values\nCreating images\ncv2.imwrite(&quot;path/to/file.jpg&quot;, old_img)\nResizing images\ncv2.resize(image, (width, height))\nDetecting face\nTo detect face, we need some models that will be used by the cv2 to process the image and detect face.\nFor example, haarcascade_frontalface_default.xml is a model\nface_cascade = cv2.cascadeClassifier(&quot;haarcascade_frontalface_default.xml&quot;)\nresults = face_cascade.detectMultiScale(img, scaleFactor=1.05, minNeighbors=5)\nresults is an 2d array of detected positions.\n[\n\t[x, y, width, height]\n]\nDrawing rectangle\nThis positions can be used to draw rectangle around faces\nfor x, y, w, h in results:\n\timg = cv2.rectangle(img, (x, y), (x + w, y + h), 3)\nShowing image\ncv2.imshow(&quot;Image&quot;, img)\ncv2.waitKey(0)  # wait for key &#039;0&#039; to press\ncv2.destroyAllWindows()  # after pressing &#039;0&#039; the image window will get closed\nwaitKey method wait for user key press on the image window. It takes parameter time in milliseconds to wait for the key. If it’s 0 then wait forever.\nIt returns the ascii value key pressed.\nVideo processing from webcam\nvideo = cv2.VideoCapture(0) # 0 is the camera number\nhas_frame, frame = vide.read()\n \nvideo.realase() # release the camera control\nBlurring Images\nimg = cv.GaussianBlur(img, (21, 21), 0)\nReferences\n\nCascade Classifier\nopencv Haarcascades\n"},"notes/CPython/package":{"title":"package","links":[],"tags":["python","programming"],"content":"Python Package\nA package is a directory containing python modules (or files) containing related set of code.\n__init__.py\n__init__.py is a file in a package which is added to make a directory a package. With old versions of python, it was mandatory to add this file. With newer versions, it is optional. However, for best practices, we should add it.\nFeatures\n\nIt can used to execute some code when the module is imported.\nWhen importing everything using from package import *, __init__.py can decide what all to import. By default nothing is imported when does like it. We need to explicitly add __all__ global variable in __init__.py.\n\n__init__.py\n__all__ = [&quot;format&quot;, &quot;my_math&quot;]\n\n\n                  \n                  Tip\n                  \n                \n\nAlthough, above can be accomplished using from .format import *.\n\n\n__init__.py\nfrom .format import *\nfrom .math import *\n\nfrom lib import *\n \nline(10)\n# &#039;----------&#039;"},"notes/CPython/pickle":{"title":"pickle","links":[],"tags":["python","programming-language"],"content":"Pickle and Unpickle\nIt is used to serialize python objects into binary.\nimport pickle\n \npickle.dump(&quot;{&#039;a&#039;: &#039;b&#039;}&quot;, file) # file is file object\nwith open(&#039;./obj&#039;, &#039;rb&#039;) as file:\n    print(pickle.load(file))"},"notes/CPython/poetry":{"title":"poetry","links":[],"tags":[],"content":"poetry\nInstalling dependencies\npoetry add --group test pytest\n"},"notes/CPython/pyobject":{"title":"pyobject","links":["notes/CPython/gil"],"tags":["python","programming"],"content":"PyObject\nA PyObject is a data structure used by CPython to store data objects in the memory. Everything in python is an object.\nPyObject structure\nCPython code has PyObject which is a grand object. It stores following information:\n\nReference Count: Total number of references for the actual data object.\nPointer to the actual data object.\n\nstruct {\n    ob_refcnt,\n    ob_type\n}\nReference count is used in freeing memory used by unused data objects. If reference counts becomes 0, the memory is freed.\nIn multithreading applications, multiple threads may try to access the same object which can cause inconsistencies in data and eventually program crash. To stop this, python has gil which locks the interpreter for a thread and other threads wait for the lock to get released.\nReference\n\nrealpython.com/python-memory-management\n"},"notes/CPython/python-bindings/marshalling":{"title":"marshalling","links":[],"tags":["python"],"content":"Marshalling\nIt is a processing of converting a data residing in memory to a form that can be transmitted or stored. Mostly used with passing the data from one system to another system for remote calls. In this case, function call in another language.\nMarshalling vs Serialisation\n\nMarshalling is serialisation plus information about data type and codebase.\n\n\n\n                  \n                  Note: \n                  \n                \n\nCodebase: It stores the information about the implementation of the serialised object from where the receiver can know where to find the code to implement the object.\n\n\nWhy Marshalling in Python (Need to move into another file)\nTo create python bindings for c/c++, marshalling is used to move data between python and c/c++ because both languages use different mechanism to store data.\nReferences\n\nrealpython.com/python-bindings-overview/#marshalling-data-types\nMarshalling vs Serialisation at Stackoverflow\n"},"notes/CPython/python-bindings/using-ctype":{"title":"using-ctype","links":[],"tags":["python"],"content":"Python binding using ctype\nAssuming there is already a shared c library, we can load this in python using following statement.\n \nshared_lib = &quot;path to shared library&quot;\nc_lib = ctype.CDDL(shared_lib)\nc_lib has all the function present in shared library as attributes.\nLet’s see there is a function mul which takes an integer and a float and returns the product of it as float.\nBy default, ctype marshalling process passes all the data as integer so we need to explicitly tell ctype to use float.\nc_lib.mul(2, ctype.c_float(2.3)) ## FAILS\nThe above statement fails. The reason being the return type is float and marshalling is not happening correctly from ctype end. Again we need to specify it.\nc_lib.mul.restype = ctype.c_float\nNow, it will work."},"notes/CPython/python-loops":{"title":"python-loops","links":[],"tags":["python"],"content":"Why python loops are slow?\n\nPython’s dynamically types nature.\n"},"notes/CPython/re":{"title":"re","links":[],"tags":["python","programming"],"content":"re module\ngroup\nReturns group by the number.\nm = re.match(r&quot;(\\w+).(\\d+)&quot;, &quot;www.222&quot;) \n \nm.group(1) # &#039;www&#039;\nm.group(2) # &#039;222&#039;\ngroups\nReturns a tuple of groups found.\nm = re.match(r&quot;(\\w+).(\\d+)&quot;, &quot;www.222&quot;) # gives matches\nm.groups() # (&#039;www&#039;, &#039;222&#039;)\ngroupdict\nReturns a dictionary of named groups.\nm = re.match(r&quot;(?P&lt;words&gt;\\w+).(?P&lt;numbers&gt;\\d+)&quot;, &quot;www.222&quot;) \nm.groupdict()\n# {&#039;word&#039;: &#039;www&#039;, &#039;number&#039;: &#039;222&#039;}"},"notes/aws/aws-cli-setup-sso":{"title":"aws-cli-setup-sso","links":[],"tags":[],"content":"Setting AWS CLI With SSO\nCreating SSO User\nAWS provides IAM Identity Center to create SSO User. Following are the steps to create SSO User.\n\nEnable IAM Identity Center if now enabled in the region.\n\n\n\n                  \n                  Only one IAM Identity Center is allowed in any region. \n                  \n                \n\n\nCreate User by following the steps.\nCreate a group and add the user into the group.\n\n\n\n                  \n                  it is always advisable to attach permissions to the groups instead of individuals users. \n                  \n                \n\n\nCreate a permission set. For now, keep PowerUserAccess for all access except user management.\nAdd a group/user to an account. Multi-account permissions &gt; AWS Accounts.\n\nClick button Assign users or groups.\nAdd user/group\nAttach Permission set just created.\n\n\n\nInstalling AWS CLI\nThis section describes about installing AWS CLI using command line for current user.\nEither you can use user folder or can create new folder for the installation. Assuming a new folder for installation aws-installer.\n\nSetup choices.xml file which tells where to get the aws files.\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;\n&lt;plist version=&quot;1.0&quot;&gt;\n  &lt;array&gt;\n    &lt;dict&gt;\n      &lt;key&gt;choiceAttribute&lt;/key&gt;\n      &lt;string&gt;customLocation&lt;/string&gt;\n      &lt;key&gt;attributeSetting&lt;/key&gt;\n      &lt;string&gt;/Users/username/aws-installer&lt;/string&gt;\n      &lt;key&gt;choiceIdentifier&lt;/key&gt;\n      &lt;string&gt;default&lt;/string&gt;\n    &lt;/dict&gt;\n  &lt;/array&gt;\n&lt;/plist&gt;\n\nDownload the .pkg file.\n\ncurl &quot;awscli.amazonaws.com/AWSCLIV2.pkg&quot; -o &quot;AWSCLIV2.pkg&quot;\n\n\nInstall the downloaded .pkg.\n\ninstaller -pkg AWSCLIV2.pkg \\\n            -target CurrentUserHomeDirectory \\\n            -applyChoiceChangesXML choices.xml\n\n\nLinking the binaries. By default installation doesn’t modify the PATH to set aws binary location. We have to do it manually.\n\nsudo ln -s /Users/username/aws-installer/aws-cli/aws /usr/local/bin/aws\nsudo ln -s /Users/username/aws-installer/aws-cli/aws_completer /usr/local/bin/aws_completer\n\n\nVerify the installation.\n\n~/aws-installer\n⟩ aws --version\naws-cli/2.15.20 Python/3.11.6 Darwin/23.3.0 exe/x86_64 prompt/off\n\nAWS CLI SSO Setup\nThis setup results in the creation of profile that will be used to access aws resources.\n\nRun the command aws configure sso.\nIt asks for few details such as name, region name, start url that can be fetched from IAM Identity Center.\n"},"notes/aws/email-notification-lambda":{"title":"email-notification-lambda","links":[],"tags":["data-engineering","amazon","lambda","aws"],"content":"Sending Email from Lambda\nA lambda function can be triggered whenever something changes in S3 bucket. This note describes how to setup the lambda to send email on S3 Bucket update.\nCreate Lambda\nCreate lambda with following code.\n&#039;use strict&#039;;\n \nconst AWS = require(&#039;aws-sdk&#039;);\n \nconst sns = new AWS.SNS()\n \nexports.handler = (event, context, callback) =&gt; {\n    console.log(process.env);\n    // This reads the environment variable &#039;sns_topic_arn&#039;\n    var topic_arn = process.env.sns_topic_arn\n    var publishParams = {\n        TopicArn: topic_arn,\n        Message: JSON.stringify(event, null, 2)\n    };\n    sns.publish(publishParams, (err, data) =&gt; {\n        if (err) console.log(err)\n        else callback(null, &quot;Completed&quot;);\n    })\n};\n\n\n                  \n                  Note: Please use Node version 14.x \n                  \n                \n\nSet Trigger Source\nWhile creating lambda function, it provides the trigger source. Select S3 Bucket there.\nAmazon provides SNS (Simple Notification Service) resource for sending email notification. Let’s setup SNS.\nSetting up SNS\nCreate a SNS topic with the name onS3BucketChange (name can be different).\nCreate a subscription where you need to add email address where to send the notification to.\nOnce the email address is added, a mail will be send to that email for subscription confirmation.\nThis should be enough for SNS. Let’s go back to Lambda. As we are using SNS in lambda, we have to provide permission to lambda to use SNS feature.\n\n\n                  \n                  When you create lambda function, you get a role for that lambda created. \n                  \n                \n\nAdd SNS policy for lambda\nGo to roles window and select role for lambda. Add one more policy for lambda role AmazonSNSFullAccess.\nThat’s it. Lambda can access SNS feature.\nConfigure Environment variable\nWe have used sns_topic_arn environment variable in the code. Go to Environment variables tab and add environment variable by putting ARN for SNS topic that we get from Topic dashboard.\nWe are good to go for testing."},"notes/dask/chunking":{"title":"chunking","links":[],"tags":["python","libraries"],"content":"Chunks\n\n\n How chunking across row or column makes processing efficient? (@2024-06-20)\n\n\n What is difference between chunks and partitions? (@2024-05-24)\n\n"},"notes/dask/compute-function":{"title":"compute-function","links":["notes/dask/get-method"],"tags":["libraries"],"content":"compute function\nIt executes the dask collection (dask graph) by the default or given scheduler or get-method function.\ndask.compute(da_1, da_2, scheduler=&quot;threads&quot;)\nAs it takes multiple dask collections, it reuses the nodes in both graphs. For instance,\nx = da.arange(10)\ny = (x + 1).sum()\nz = (x + 1).mean()\n \ndask.compute(y, z)\nSo, node (x + 1) will be reused."},"notes/dask/dask-delayed":{"title":"dask-delayed","links":[],"tags":["python","libraries"],"content":"Dask Delayed\nIt is a feature of dask where it makes the function call lazy.\nimport dask\n \n@dask.delayed\ndef f():\n    return 1\nf becomes lazy. Calling it won’t give back the result. We need to explicitly call compute .\ndelayed = f()\ndelayed.compute()\nDask actually creates a task graph of function calls which can be chained together."},"notes/dask/dask-graph":{"title":"dask-graph","links":["notes/dask/get-method"],"tags":["libraries","distributed-computing"],"content":"Dask Graph\nA dask graph is a DAG of tasks as nodes and edges as passing intermediatery data.\nA dask graph is represented by using python basic building blocks such as tuple, list, dict etc.\ngraph = {\n         &#039;x&#039;: 2,\n         &#039;y&#039;: (add, &#039;x&#039;, 2)\n}\nThese dask graphs are submitted to scheduler to for the execution. Different scheduler may execute these task graphs in differently based on the optimization techniques they have.\nA dask graph can be evaluated/executed using get method."},"notes/dask/dask-so-far":{"title":"dask-so-far","links":[],"tags":["python","libraries"],"content":"Dask So Far\n\nMakes dealing with big data in distributed and parallel manner.\nUses underlying numpy arrays for dask arrays.\n\n\n\n                  \n                  Note: \n                  \n                \n\nFor dataframes and other data structures, some different underlying data structure can be there.\n\n\n\nAll the computations are lazy. Applying computation methods such as mean, max etc won’t get executed immediately until explicitly computation is called. (using compute)\n\nd_array.max().compute()\n\nCreates a task graph for the computation operations like spark.\n\nd_array.visualize()\n"},"notes/dask/get-method":{"title":"get-method","links":[],"tags":["distributed-computing"],"content":"get\nA get is used to execute a task graph. This is provided by different schedulers.\n\ndask.threaded.get\ndask.multiprocessing.get\ndask.distributed.Client.get\n\ndsk = {\n       &quot;x&quot;: 1,\n       &quot;y&quot;: (operator.add, &#039;x&#039;, 1)\n}\ndask.threaded.get(dsk, &#039;y&#039;)\ndask.threaded.get(dsk, &#039;x&#039;)\nReferences\n\ndocs.dask.org/en/stable/scheduler-overview.html\n"},"notes/dask/local-cluster":{"title":"local-cluster","links":[],"tags":["libraries","python","distributed-computing"],"content":"LocalCluster\nDask is designed to do no difference in running it on single machine or a distributed cluster.\nLocalCluster is designed to imitate a distributed cluster on a single machine.\nfrom dask.distributed import Client, LocalCluster\n \ncluster = LocalCluster(n_workers=8, threads_per_worker=8)\n \nclient = Client(cluster)"},"notes/dask/map-blocks":{"title":"map-blocks","links":[],"tags":["libraries"],"content":"map_blocks\nThis function/method maps through the blocks in a dask array and apply a customized function on it.\nmap_blocks function from straight dask.array can take multiple dask arrays.\n \nda.map_blocks(lambda x: x * 2, data)\n \n# or\n \ndata.map_blocks(lambda x: x * 2)\nlambda gets numpy.ndarray as chunks.\n\n\n                  \n                  Note\n                  \n                \n\nChunks passed to the lambda are not fully evaluated. For instance, accessing any other element rather than first one will give error. chunk[2] will give error whilst chunk[0] won’t.\n\n\nExamples\n\nMap on two dask arrays of different block numbers\n\nd5 = da.from_array(np.random.random(10), chunks=2)\nd6 = da.arange(25, chunks=(5))\n \nres = da.map_blocks(lambda x, y: np.array([x.max(), y.max()]), d5, d6)\n \nres.compute()\nlambda returns chunk with max values from two corresponding chunks. So, res.compute() returns max values from corresponding chunks of two dask arrays."},"notes/dask/scheduler":{"title":"scheduler","links":["notes/dask/local-cluster"],"tags":["python","libraries"],"content":"Scheduler\nA scheduler is something that schedules the tasks graph on parallel hardware. There are two types of scheduler,\n\nSingle machine\nDistributed scheduler\n\nSingle machine\nSingle machine scheduler uses machines threads and processes.\ndask.config.set(scheduler=&quot;processes&quot;) # it uses multiprocessing scheduler inside single machine\n \ndask.config.set(scheduler=&quot;threads&quot;)\nAbove configs are set globally. However, they can be used with context manager.\nwith dask.config.set(scheduler=&quot;threads&quot;):\n    d.compute()\nWhile calling compute, we can provide scheduler.\nd.compute(scheduler=&quot;processes&quot;)\nDistributed Scheduler\nDistributed scheduler uses workers to schedule the job on. It can be setup on local single machine or a multi node cluster.\n\n\n                  \n                  Todo\n                  \n                \n\n\n\n single machine\n\n threads\n processing\n single thread\n\n\n\n single machine\n\n threads\n processing\n single thread\n\n\n\n local cluster on single machine\n\n threads\n processing\n single thread\n\n\n\n kubernetes cluster\n\n"},"notes/elk-stack":{"title":"elk-stack","links":[],"tags":["logs","data"],"content":"ELK Stack\nObservations\n\nLogs management and analytics\nELK stack was first introduced by Elastic and was open source. However, the changed it to licensed from open source\nAWS launched OpenSearch as replacement for ELK which was open source.\nLogstash collects the logs data, applies transformations passes onto elasticsearch which indexes and stores the data. Kibana is then used to visualize the data.\n\nElasticsearch\nText search and analytics engine based on apache Lucene open source search engine.\nLogstash\nLogs aggregator and data collector from various sources, execute different transformations and enhancements and ship the data to various supported destinations.\nKibana\nVisualization tool works on top of elasticsearch."},"notes/environment-modules/modules":{"title":"modules","links":[],"tags":["libraries"],"content":"It setups the libraries and paths. Custom configurations that setups the paths, can be written by the user be used with module load command.\nIt already comes on Linux system and can be initialized using,\nsource /usr/local/Modules/init/&lt;shell&gt;\n\nshell can be any supported shell."},"notes/gitlab/ci":{"title":"ci","links":[],"tags":[],"content":"Gitlab CI\n\nA gitlab ci contains stages and job. Jobs are grouped in stages and run in parallel.\n\nstages:\n  - build\n  - deploy\n \nbuild-job:\n  stage: build\n  image: node\n  script:\n    - npm install\n    - npm run build\n  artifacts:\n    paths:\n      - &quot;build/&quot;\n \npages:\n  stage: deploy\n  script:\n    - mv build/ public/\n  artifacts:\n    paths:\n      - &quot;public/&quot;\n \n\nstages directive defines the sequence of jobs. There are two jobs with their stage mentioned in their definition.\n\nReferences\n\ndocs.gitlab.com/ee/ci/quick_start/tutorial.html\n"},"notes/googleapis/delete-rows":{"title":"delete-rows","links":[],"tags":["spreadsheet","gsheets"],"content":"Rows can be deleted using batchUpdate API.\n \nresource = {&quot;requests&quot;:\n                [\n                    {&quot;deleteDimension&quot;: {\n                        &quot;range&quot;:{\n                            &quot;sheetId&quot;:0,\n                            &quot;dimension&quot;:&quot;ROWS&quot;,\n                            &quot;startIndex&quot;:20,\n                            &quot;endIndex&quot;:21}\n                        }\n                    }\n                ]\n            }\n \nbatchReqBody = {spreadsheetId: spreadSheet, resource: resource}\nclient.spreadsheets.batchUpdate(batchReqBody)\nstartIndex specify the range start. This row is not deleted. For instance, the range is startIndex = 16 and endIndex = 20, it will delete rows 17, 18, 19, and 20."},"notes/jax/jax":{"title":"jax","links":[],"tags":["libraries","dic"],"content":"It uses MLIR."},"notes/linux/accessing-usb-drives":{"title":"accessing-usb-drives","links":[],"tags":["linux","terminal","cli","storage"],"content":"List the drives using command,\n\nlsblk - list block devices which lists all the block devices available in the system.\nblkid - list and print block devices attributes\nfdisk - deals file disk partitions and manages them.\nudiskctl\n\nMount and Unmount\n\nMounting is used to mount the external storage device to the local file system. It makes external storage devices accessible within the system.\n\nmount /dev/sda /storage/ultafit\n\n\n\nSyntax - mount device directory\n\n\nUnmounting remove the external storage mount from the directory and the external storage no longer be accessed inside the file system.\n\n\numount /storage/ultafit\n"},"notes/linux/automount-drives":{"title":"automount-drives","links":[],"tags":["storage","linux"],"content":"Auto mount USB drives\nThis can be done by fstab (filesystem table) system file. It can be found at /etc/fstab. It has all the drive information to mount on boot.\nWe can append our usb drive information to this file.\nPARTUUID=494aac82-01 /home/nitin-remote/storage/hp auto defaults,nofail,uid=nitin-remote,gid=nitin-remote,x-systemd.automount 0 2\n\n\n\nWe can provide usb drive path using /dev/sda etc but to play safe and ignoring where we insert the usb into which port, they have a UUID that we can use to identify them.\n\n\nauto flag is tell auto mount.\n\n\nReferences\n\nwww.baeldung.com/linux/automount-usb-device\nwww.redhat.com/sysadmin/etc-fstab\n\n"},"notes/linux/resources":{"title":"resources","links":[],"tags":["linux","resources"],"content":"\nwww.actualtechmedia.com/wp-content/uploads/2017/12/CUMULUS-NETWORKS-Linux101.pdf\n"},"notes/numpy/broadcasting":{"title":"broadcasting","links":[],"tags":["python","libraries"],"content":"Broadcasting\n\nUsed in arithmetic operations in numpy arrays.\nProvides a means of vectorizing array operations so that the looping happens inside C instead of python.\nApplying arithmetic operations on arrays with different shapes and dimensions, the smaller one has to be stretched to make compatible with larger one.\n\nBroadcasting rules\n\nCompatibility checks starts from right to left.\nTwo dimensions are compatible only when\n\nboth are same\neither one of them is 1\n\n\n\ndims1 = (1,)\ndims2 = (1,)\nBOTH ARE COMPATIBLE\n------------\n\ndims1 = (1,2)\ndims2 = (1,1)\nBOTH ARE COMPATIBLE as there is 1 in last dimension\n------------\n\ndims1 = (1,2)\ndims2 = (1,3)\nNOT COMPATIBLE as dimensions differ and one of them is not 1\n------------\n\ndims1 = (4, 1)\ndims2 = (3,)\nBOTH ARE COMPATIBLE\nas\n       4 x 1\n       1 x 3\nresult 4 x 3\n\n\nitems = np.arange(10).reshape((2, 5))\nitems.shape # (2, 5)\n \nmul = np.array([[2, 3], [2, 3]])\nmul.shape # (2, 2)\n \nitems * mul\n# ValueError: operands could not be broadcast together with shapes (2,5) (2,2)"},"notes/numpy/indexing":{"title":"indexing","links":[],"tags":["libraries","python"],"content":"Indexing on ndarrays\n\nThree types of indexing\n\nbasic\nadvanced\nfield\n\n\n\n"},"notes/numpy/internal-organization":{"title":"internal-organization","links":["notes/numpy/view"],"tags":["python","libraries"],"content":"Internal Organization\nNumpy arrays are organized using a set of two parts.\n\nMetadata\nRaw array data (or data buffer)\n\nMetadata is the information about the data buffer. It has information about shape, size, data type etc. A numpy array object is the metadata which points to the data buffer.\nAny operations on the data buffer results in making new metadata object which points to the same data buffer. This is usually called view of the original array (data buffer). This way, same data buffer can be used for different representation.\nHowever, some numpy operation may return new copy of the data buffer.\nReferences\n\nnumpy.org/doc/stable/dev/internals.html\n\n"},"notes/numpy/interp":{"title":"interp","links":[],"tags":["python","libraries"],"content":"np.interp\nThis method interpolate or determines the value of position for a given function.\nFunction here means two arrays data points of equal length. np.interp is then determines the corresponding value of given position with respect to the function (two arrays).\nx = np.array([1, 2, 3, 4])\ny = np.array([1, 2, 3, 4])\n \nnp.interp(0, x, y)\n# 1.0\n# obiuously \n \n# calculating 100 points between x min and max\nx_interp = np.linspace(x.min(), x.max(), 100)\n \ny_interp = np.interp(x_interp, x, y)\n \n# y_interp will be values corresponding to x_interp value with respect to function x, y"},"notes/numpy/newaxis":{"title":"newaxis","links":[],"tags":["python","libraries"],"content":"np.newaxis\nAdds new dimension to the array.\na = np.array([1, 2])\n \na[np.newaxis].shape\n# (1, 2)\nThe same can be done using None. None and np.newaxis are same."},"notes/numpy/pad":{"title":"pad","links":[],"tags":["python","libraries"],"content":"np.pad\nPads the numpy array. By defaults it pads with constant value as shown below.\na = np.arange(5)\n \n# array([0, 1, 2, 3, 4])\nnp.pad(a, (1, 1))\n \n# array([0, 0, 1, 2, 3, 4, 0])\nLooking at the tuple (1, 1) says 1 left and 1 right.\nFor 2d array,\na = np.arange(4).reshape(2, 2)\n \n#array([[0, 1],\n#       [2, 3]])\n \nnp.pad(a, ((1, 1), (1, 1)))\n# array([[0, 0, 0, 0],\n#      [0, 0, 1, 0],\n#      [0, 2, 3, 0],\n#      [0, 0, 0, 0]])\nAgain looking at the tuple for each dimension\n\nfor dimension 0, it is one top and one bottom\nfor dimension 1, it one left and one right.\n"},"notes/numpy/see-also":{"title":"see-also","links":[],"tags":["later","misc"],"content":"See also\n\n C-order indexing\n"},"notes/numpy/transpose":{"title":"transpose","links":["notes/xarray/xarray-so-far"],"tags":["libraries","python"],"content":"Transpose\nSwitch the axis in array. Or switch the dimension for xarray.\nxd = xr.DataArray(np.arange(4).reshape((2,2,1)), dims=(&quot;time&quot;, &quot;baseline&quot;, &quot;pol&quot;))\n \nprint(xd)\nprint(xd.T)\n&lt;xarray.DataArray (time: 2, baseline: 2, pol: 1)&gt; Size: 32B\narray([[[0],\n        [1]],\n\n       [[2],\n        [3]]])\nDimensions without coordinates: time, baseline, pol\n&lt;xarray.DataArray (pol: 1, baseline: 2, time: 2)&gt; Size: 32B\narray([[[0, 2],\n        [1, 3]]])\nDimensions without coordinates: pol, baseline, time\n\nOnly the dimensions are switched. So, accessing the element will be different after the transpose."},"notes/numpy/vectorize":{"title":"vectorize","links":[],"tags":["python","libraries"],"content":"Vectorize\nIt is used to create a vectorize function on the numpy array.\nBasic syntax\nnp.vectorize(user_def_fn)\nIt returns a vectorize function which access numpy array. Numpy then maps the array elements over the function.\nnums = np.arange(10)\nsquare = np.vectorize(lambda x: x * x)\nsquare(nums)\n# array([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])\nsignature\nBy default function gets scaler value from the array and returns the scaler value as output. However, we can change this behavior using optional parameter signature.\nA signature specifies what nd array the function would accept and returns as output.\nnums = np.arange(4).reshape((2, 2))\n# array([[0, 1],\n#       [2, 3]])\n \nsquare_array = np.vectorize(lambda sub_array: sub_array ** 2, signature=&quot;(n)-&gt;(n)&quot;)\nsquare_array(nums)\nSignature (n) -&gt; (n) specifies that the function would take 1d array as input and returns 1d array as output. Signature specifies the shape of input and output array.\n(n, k) -&gt; (n) means the function takes 2d array as input and returns 1d array as output. If the function returns 2d array as output, the signature should be (n, k) -&gt; (x, y).\n\n\n                  \n                  Note: \n                  \n                \n\nThe variables used in input and output signature has not to be same by names.\n\n\nSignature (n),(n)-&gt;(),() specifies that the vectorized function takes two 1d arrays and return two scaler values"},"notes/numpy/view":{"title":"view","links":[],"tags":["libraries","python","programming"],"content":"View\nWhen a new array is created after some operation on existing numpy array, a view is created which points to the same raw array data (data buffer).\nSome operation returns view and some returns array pointing to whole new data buffer.\nIt increase the performance of numpy as no new data is getting created.\nHowever, it has some implications as shown below.\na = np.arange(5)\n#array([0, 1, 2, 3, 4])\n \nb = a[:2]\n#array([0, 1])\n \na[0] = 133\n \n#array([133,   1,   2,   3,   4])\n \nb\n# array([133,   1])\nSo, changing the view modifies the original data buffer.\nReferences\n\nnumpy.org/doc/stable/glossary.html#term-view\n"},"notes/singularity/container-fs":{"title":"container-fs","links":[],"tags":["inferences","docker"],"content":"Container File System\n\n$SINGULARITY_ROOTFS environment variables has the path to the container file system.\n"},"notes/singularity/definition-file-sections":{"title":"definition-file-sections","links":[],"tags":["infrastructure","docker"],"content":"Sections\n\nmultiple sections with the same name can be included and will be appended to one another during the build process.\n\n%setup\n\nExecutes the commands on the host system after base operating system is installed in the container.\nThis section is usually not used as it is dangerous.\n\n%files\n\nThis section is used to copy files from host system to container. Also, from previous stage specified.\n%files section is executed before %post so that files are available while build process in %post.\nsource can be a path from host system or path in previous stage if specified. destination is the path in the container.\n\nif destination is not specified, source path is used for destination.\n\n\n\n# copying file from host to container\n%files\n/from-host /to-container\n\n# copying file from previous stage to current container\n%files prev_stage\n/from /to-container\n"},"notes/singularity/defintion-file-header":{"title":"defintion-file-header","links":[],"tags":["infrastructure","docker"],"content":"Header\nContains information about the base operating system to be used to build container. It is written on the top of def file.\nBootstrap key needs to have a bootstrap agent that will be used to create base operating system.\n“"},"notes/xarray-cuda/xarray-cuda":{"title":"xarray-cuda","links":[],"tags":["libraries","gpu","python"],"content":"Observations\n\nCuda Array works same as numpy array.\nHowever, some of the functionality where xarray more specific to numpy array, operations on cuda arrays give error.\n\nFor example, xarray.apply_ufunc with vectorize param give error when operated on cuda arrays as internally xarray.apply_ufunc -&gt; dask.array.gufunc.apply_gufunc uses np.vectorize\nAlso, passing cuda vectorize function to xarray.apply_ufunc doesn’t work. Looks like they have not implemented it yet.\npassing signature is not yet implemented for cuda vectorize\n\n\n"},"notes/xarray/apply_ufunc-dask-vectorize":{"title":"apply_ufunc-dask-vectorize","links":["notes/xarray/apply_ufunc-dask"],"tags":["libraries","python"],"content":"Vectorize Dask Xarray\nVectorization for dask xarray works only when we give dask=parallelized parameter.\nFrom the note we saw that dask.array.apply_gufunc provides chunk blocks to custom function. When vectorize is provided, numpy vectorization happens on the custom function.\nSo the flow of execution is as follows,\nxarray.apply_ufunc\n        |\n        V\ndask.array.apply_gufunc\n        |\n        V\nnp.vectorize\n        |\n        V\ncustom function\n    ```\n\nFor example,\n\n```python\ndata\n# &lt;xarray.DataArray &#039;air&#039; (time: 2, lat: 5, lon: 5)&gt; Size: 400B dask.array&lt;rechunk-merge, shape=(2, 5, 5), dtype=float64, chunksize=(1, 5, 5), chunktype=numpy.ndarray&gt;\n\n\ndef add_wrapper(d, value):\n     print(&#039;------------------- START ---------------------&#039;)\n     print(type(d))\n     print(d.shape)\n     print(d)\n     print(&#039;-------------------- END ------------------------&#039;)\n     return d + value\n\n\nxr.apply_ufunc(add_wrapper, \n               data, \n               1, \n               input_core_dims=[[&quot;lon&quot;], []], \n               output_core_dims=[[&quot;lon&quot;]], \n               dask=&quot;parallelized&quot;, \n               vectorize=True).compute()\n\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (1,)\n# [1.]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [242.1  242.7  243.1  243.39 243.6 ]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [243.6 244.1 244.2 244.1 243.7]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [253.2  252.89 252.1  250.8  249.3 ]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [269.7 269.4 268.6 267.4 266. ]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [272.5 271.5 270.4 269.4 268.5]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [241.2 242.5 243.5 244.  244.1]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [243.8  244.5  244.7  244.2  243.39]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [250.   249.8  248.89 247.5  246.  ]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [266.5 267.1 267.1 266.7 265.9]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [274.5  274.29 274.1  274.   273.79]\n# -------------------- END ------------------------\n# &lt;xarray.DataArray &#039;air&#039; (time: 2, lat: 5, lon: 5)&gt; Size: 400B\n# array([[[242.2 , 243.5 , 244.5 , 245.  , 245.1 ],\n#         [244.8 , 245.5 , 245.7 , 245.2 , 244.39],\n#         [251.  , 250.8 , 249.89, 248.5 , 247.  ],\n#         [267.5 , 268.1 , 268.1 , 267.7 , 266.9 ],\n#         [275.5 , 275.29, 275.1 , 275.  , 274.79]],\n\n#        [[243.1 , 243.7 , 244.1 , 244.39, 244.6 ],\n#         [244.6 , 245.1 , 245.2 , 245.1 , 244.7 ],\n#         [254.2 , 253.89, 253.1 , 251.8 , 250.3 ],\n#         [270.7 , 270.4 , 269.6 , 268.4 , 267.  ],\n#         [273.5 , 272.5 , 271.4 , 270.4 , 269.5 ]]])\n\nVectorized custom functions is called with 1d array of dim lon 10 times because there are two chunks (1, 5, 5) and (1, 5, 5) and we have broadcasted on dims time and lat. So, function is called with 1d array of 5 elements 10 (1, 5) + (1, 5) = 10\nReferences\n\ntutorial.xarray.dev/advanced/apply_ufunc/dask_apply_ufunc.html#automatic-vectorizing\n"},"notes/xarray/apply_ufunc-dask":{"title":"apply_ufunc-dask","links":[],"tags":["python","libraries"],"content":"apply_ufunc with dask arrays\napply_ufunc handles dask xarray in same manner as it does with numpy xarray. However, it requires an dask argument` to work with dask xarrays.\nxr.apply_ufunc(xda, dask=&#039;allowed&#039;)\ndask parameter\n\ndask=forbidden gives error when dask xarray is given.\ndask=allowed is when given function is able to handle dask array as input.\ndask=parallelized is when function is not able to handle dask array and want xarray.apply_ufunc to make function work with dask arrays.\n\nWorking with non-dask functions\nWhen function doesn’t support dask xarrays, we can use dask=parallelized argument for xr.apply_ufunc.\nHow it works?\n\nxarray.apply_ufunc uses dask.array.apply_gufunc internally.\nunderlying dask array is passed to apply_gufunc.\napply_gufunc then passes each chunk block numpy array to custom function.\ndask stitches back all the output chunk array from custom function.\nxarray then populates meta information to the output array.\n\n\n\n                  \n                  Warning\n                  \n                \n\nIf data is chunked on a core dimension the custom function is working on, executing such operations give error.\n\n\nFor example,\nds.air\n# &lt;xarray.DataArray &#039;air&#039; (time: 2920, lat: 25, lon: 53)&gt; Size: 31MB dask.array&lt;open_dataset-air, shape=(2920, 25, 53), dtype=float64, chunksize=(100, 25, 53), chunktype=numpy.ndarray&gt;\n \n \nxr.apply_ufunc(mean, ds.air, input_core_dims=[[&quot;time&quot;]], kwargs={&quot;axis&quot;: -1}, dask=&quot;parallelized&quot;)\n \n# dimension time on 0th function argument to apply_ufunc with dask=&#039;parallelized&#039; consists of multiple chunks, but is also a core dimension. To fix, either rechunk into a single array chunk along this dimension, i.e., ``.chunk(dict(time=-1))``, or pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` but beware that this may significantly increase memory usage.\nData is chunked across time dimension and providing time as core dimension raises the error. The data has to be re-chunked if we don’t want this error. This can be done by providing argument for dask.apply_gufunc.\n  def mean(d, **kwargs):\n     print(&#039;------------------- START ---------------------&#039;)\n     print(type(d))\n     print(d.shape)\n     print(&#039;-------------------- END ------------------------&#039;)\n     return np.mean(d, **kwargs)\nxr.apply_ufunc(mean,\n               ds.air,\n               input_core_dims=[[&quot;time&quot;]], \n               kwargs={&quot;axis&quot;: -1}, \n               dask_gufunc_kwargs={&quot;allow_rechunk&quot;:True},\n               dask=&quot;parallelized&quot;)\nHowever, this is consumes more memory.\nTo determine the effect of the function on the input, dask first runs the function with dummy data as shown below.\nxr.apply_ufunc(mean, \n               ds.air, \n               input_core_dims=[[&quot;lon&quot;]], \n               kwargs={&quot;axis&quot;: -1}, dask=&quot;parallelized&quot;)\n \n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (1, 1, 1)\n# -------------------- END ------------------------\n#&lt;xarray.DataArray &#039;air&#039; (time: 2920, lat: 25)&gt; Size: 584kB dask.array&lt;transpose, shape=(2920, 25), dtype=float64, chunksize=(100, 25), chunktype=numpy.ndarray&gt;\nReferences\n\ntutorial.xarray.dev/advanced/apply_ufunc/dask_apply_ufunc.html#understanding-what-s-happening\n"},"notes/xarray/apply_ufunc":{"title":"apply_ufunc","links":[],"tags":["libraries","python"],"content":"apply_ufunc\nXarray method that applies custom function on underlying array in xarray. The underlying array can be a numpy or a dask array (need verification).\nHow it works?\n\nWhen xarray DataArray is given to apply_ufunc, it gets the underlying array, and passes to the custom function. Custom function returns the result array which again wrapped by apply_ufunc and returned to us.\nWhen xarray DataSet is given, it loops over the data variables(DataArray), passes each of the data array to the custom function, gets the result array and wraps in Data Array and then in Data set and returns.\nAll the input DataSet/DataArray metadata gets populated on the result by apply_ufunc.\nBy default, attributes are not populated on the result but can retained using keep_attrs=True.\n\n\n\n                  \n                  Note \n                  \n                \n\nvectorize=True doesn’t work when input array is a dask array.\n\n\nSyntax\nIt is the minimal syntax\nxr.apply_ufunct(custom_function, dataset/dataarray, pos_args, kwargs={}, keep_attrs=True)\ninput_core_dims\n\nThis parameter takes the dimensions to which the function would be operated on.\nspecified dimensions are moved to last. for example, input_core_dims=[[&quot;time&quot;]] will be moved to last dimension.\n\nNotebooks\n\n/Users/nitinsharma/TW-Projects/ska/exploration/xarray/apply_ufunc.ipynb\n\n\nReferences\n\ntutorial.xarray.dev/advanced/apply_ufunc/simple_numpy_apply_ufunc.html\n\nincreasing the order by 1 vectorization"},"notes/xarray/core-dimensions":{"title":"core-dimensions","links":["notes/xarray/apply_ufunc"],"tags":["data","python","libraries"],"content":"Core Dimensions\nFundamental dimensions over which an operation is defined.\nWhy core dimensions?\n\napply_ufunc is used to create generalized operations on xarray data.\nCore dimensions specifies which are the main dimensions the function will be operating with.\nCore dimensions are moved to last index (transposed).\n\nFor example,\nda.shape\n# (2, 3, 4)\n \ndef func(d):\n    print(d.shape)\n    return d\n \nxr.apply_ufunc(func, da, input_core_dims=[[&quot;first_dim&quot;]])\n \n# (3, 4, 2)\n\n\n                  \n                  Todo\n                  \n                \n\nUnderstand the use of core dimensions in case of vectorise=True\n\n\nReferences\n\ntutorial.xarray.dev/advanced/apply_ufunc/core-dimensions.html\n"},"notes/xarray/loop-dimensions":{"title":"loop-dimensions","links":["notes/xarray/core-dimensions","notes/xarray/apply_ufunc"],"tags":["python","data","libraries"],"content":"Loop Dimensions\nFunctions that performs operations on data with core-dimensions and taking axis as argument, can be easily used with apply_ufunc.\nHowever, functions which don’t take axis argument and need to work on core-dimensions need loop dimensions. Loop dimension is something where the loop happens and function gets the sub-array as input.\nFor example, a 2d data array with dimensions x and y and square needs to work on only y, we need to loop over x so that function can get only 1d array on dimension y.\ndef square(item):\n    return item ** 2\n    \nxr.apply_ufunc(square,\n               xda,\n               input_core_dims=[[&quot;y&quot;]], \n               output_core_dims=[[&quot;y&quot;]], \n               vectorize=True)\nsquare gets 1d array of dimension y. input_core_dims provides the core dimensions function will take and dimensions which are not provided will be taken as loop dimensions (which is x) here.\nReferences\n\ntutorial.xarray.dev/advanced/apply_ufunc/automatic-vectorizing-numpy.html#core-dimensions-and-looping\n\n"},"notes/xarray/map_blocks":{"title":"map_blocks","links":[],"tags":["python","libraries","parallelism"],"content":"xarray.map_blocks\nIt applies custom function on each chunk block of the DataArray or Dataset.\ndef add_wrapper(d):\n    print(&#039;------------------- START ---------------------&#039;)\n    print(type(d))\n    print(d.shape)\n    print(d)\n    print(&#039;-------------------- END ------------------------&#039;)\n    return d + 1\n \nxda = ds.air[:2, :5, :5].chunk({&quot;time&quot;: 1})\n \n xr.map_blocks(add_wrapper, ds.air).compute()\nOutput\n------------------- START ---------------------\n&lt;class &#039;xarray.core.dataarray.DataArray&#039;&gt;\n(0, 0, 0)\n&lt;xarray.DataArray &#039;air&#039; (time: 0, lat: 0, lon: 0)&gt; Size: 0B\narray([], shape=(0, 0, 0), dtype=float64)\n-------------------- END ------------------------\n------------------- START ---------------------\n&lt;class &#039;xarray.core.dataarray.DataArray&#039;&gt;\n(1, 5, 5)\n&lt;xarray.DataArray &#039;air&#039; (time: 1, lat: 5, lon: 5)&gt; Size: 200B\narray([[[241.2 , 242.5 , 243.5 , 244.  , 244.1 ],\n        [243.8 , 244.5 , 244.7 , 244.2 , 243.39],\n        [250.  , 249.8 , 248.89, 247.5 , 246.  ],\n        [266.5 , 267.1 , 267.1 , 266.7 , 265.9 ],\n        [274.5 , 274.29, 274.1 , 274.  , 273.79]]])\n-------------------- END ------------------------\n------------------- START ---------------------\n&lt;class &#039;xarray.core.dataarray.DataArray&#039;&gt;\n(1, 5, 5)\n&lt;xarray.DataArray &#039;air&#039; (time: 1, lat: 5, lon: 5)&gt; Size: 200B\narray([[[242.1 , 242.7 , 243.1 , 243.39, 243.6 ],\n        [243.6 , 244.1 , 244.2 , 244.1 , 243.7 ],\n        [253.2 , 252.89, 252.1 , 250.8 , 249.3 ],\n        [269.7 , 269.4 , 268.6 , 267.4 , 266.  ],\n        [272.5 , 271.5 , 270.4 , 269.4 , 268.5 ]]])\n-------------------- END ------------------------\nOut[40]:\n&lt;xarray.DataArray &#039;air&#039; (time: 2, lat: 5, lon: 5)&gt; Size: 400B\narray([[[242.2 , 243.5 , 244.5 , 245.  , 245.1 ],\n        [244.8 , 245.5 , 245.7 , 245.2 , 244.39],\n        [251.  , 250.8 , 249.89, 248.5 , 247.  ],\n        [267.5 , 268.1 , 268.1 , 267.7 , 266.9 ],\n        [275.5 , 275.29, 275.1 , 275.  , 274.79]],\n\n       [[243.1 , 243.7 , 244.1 , 244.39, 244.6 ],\n        [244.6 , 245.1 , 245.2 , 245.1 , 244.7 ],\n        [254.2 , 253.89, 253.1 , 251.8 , 250.3 ],\n        [270.7 , 270.4 , 269.6 , 268.4 , 267.  ],\n        [273.5 , 272.5 , 271.4 , 270.4 , 269.5 ]]])\n\nObservations\n\nFunction add_wrapper receives loaded DataArray.\nmap_blocks calls function first time with random data for output type inferencing. If it is not able to determine the type, we need explicitly specify type using template kwarg.\nSo in the above example, the function is called three times.\n\nOne for inference\nFirst chunk\nSecond chunk\n\n\n\n"},"notes/xarray/xarray-so-far":{"title":"xarray-so-far","links":[],"tags":["libraries","python"],"content":"\nxarray is made on top of numpy by adding more information about the nature of data.\nadding lables to the dimensions. Instead of just raw dimension, xarray added name to them such as time, longitude, etc.\n\n"},"notes/xarray/xarray-with-dask":{"title":"xarray-with-dask","links":[],"tags":["libraries","python","data-engineering"],"content":"Xarray with Dask\nXarray is numpy array with metadata and accessing the array with metadata.\nXarray does the computations in memory so has the limitation on data size.\nDask enables xarray to compute with big data with in disk computation like spark.\nDask makes the computation parallel using chunking method.\nXarray fits well with numpy as data array and dask array. Underling array can be a numpy or a dask.\nd = xr.DataArray(np.arange(10)) # numpy array\n \nb = xr.open_dataset(&quot;file.nc&quot;, chunks={&quot;dim_0&quot;: 1}) # dask array\nopen_dataset method reads netCDF files.\nReferences\n\nstephanhoyer.com/2015/06/11/xray-dask-out-of-core-labeled-arrays/\n"}}