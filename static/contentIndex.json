{"blogs/tech/docker/docker-compose-todo-app":{"title":"Docker Compose Todo App","links":["blogs/tech/docker/docker-compose"],"tags":["docker-compose","compose","microservices","networks"],"content":"Greetings, esteemed readers! It is an absolute pleasure to see you all here. I trust that each one of you is in the pink of health and high spirits. In this post, we are going to containerize our todo application using docker compose. If you want to learn about docker compose, refer my post on Docker Compose. Letâ€™s hit the road!\nWhat is there to Cover?\nWe will be covering the following sections here.\n\nSetting up the project\nCreate Dockerfile for the backend\nCreate Dockerfile for the frontend\nCreate a docker compose\nSpin up the compose\nAn introduction to deploys\nConclusion\n\nLetâ€™s follow through with each one of these sections.\nSetting up the application\nPrerequisites\n\nnode\nnpm\nmysql\nand docker of course\n\nCreate your sandbox directory where you would be writing the code. Clone the following project in your sandbox.\n# ssh\nâŸ© git clone git@github.com:nitinsharmacs/bitphile-todo-app-docker-compose.git\n \n# https\nâŸ© git clone github.com/nitinsharmacs/bitphile-todo-app-docker-compose.git\nListing down the contents of the project, we will see.\nâŸ© tree                                                                   (base)\n.\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ backend\nâ”‚Â Â  â”œâ”€â”€ README.md\nâ”‚Â Â  â”œâ”€â”€ index.js\nâ”‚Â Â  â”œâ”€â”€ package-lock.json\nâ”‚Â Â  â””â”€â”€ package.json\nâ””â”€â”€ frontend\n    â”œâ”€â”€ README.md\n    â”œâ”€â”€ package-lock.json\n    â”œâ”€â”€ package.json\n    â”œâ”€â”€ public\n    â”‚Â Â  â”œâ”€â”€ favicon.ico\n    â”‚Â Â  â”œâ”€â”€ index.html\n    â”‚Â Â  â”œâ”€â”€ logo192.png\n    â”‚Â Â  â”œâ”€â”€ logo512.png\n    â”‚Â Â  â”œâ”€â”€ manifest.json\n    â”‚Â Â  â””â”€â”€ robots.txt\n    â””â”€â”€ src\n        â”œâ”€â”€ App.css\n        â”œâ”€â”€ App.js\n        â”œâ”€â”€ App.test.js\n        â”œâ”€â”€ components\n        â”‚Â Â  â”œâ”€â”€ CreateTodo.jsx\n        â”‚Â Â  â”œâ”€â”€ Header.jsx\n        â”‚Â Â  â””â”€â”€ TodoItems.jsx\n        â”œâ”€â”€ index.css\n        â”œâ”€â”€ index.js\n        â”œâ”€â”€ logo.svg\n        â”œâ”€â”€ reportWebVitals.js\n        â””â”€â”€ setupTests.js\n \n5 directories, 25 files\nInstall dependencies in each of those directories.\nâŸ© cd frontend\nâŸ© npm install\nâŸ© cd ../backend\nâŸ© npm install\nNow, start both the backend and frontend.\nfrontend\nâŸ© npm start\n \nbackend\nâŸ© npm start\n\nFacing issues with MySQL db? Please make sure it is running. Also, provide environment variables, DB_USER and DB_PASSWORD while running the backend.\n\nOpen localhost:3000, and you should see an ugly page without CSS. Letâ€™s not worry about the styling of the page.\n\nYou should be able to add a to-do.\nCreate Dockerfile for the backend\nLetâ€™s write Dockerfile for the backend.\nFROM node:12-alpine\n \nCOPY package.json package.json\n \nRUN npm install\n \nENV NODE_ENV=production\n \nCOPY index.js index.js\n \nCMD [&quot;npm&quot;, &quot;start&quot;]\n \nEXPOSE 3001\nCopy and paste this content inside backend/Dockerfile.\nNow, build the image.\nâŸ© docker build -t bitphile/todo-backend .\nLetâ€™s try running the image weâ€™ve just built.\nâŸ© docker run -d --name todo-backend -p 3001:3001 bitphile/todo-backend\n68dc2b21f5e2b8a6d54c35dc46a56f6d0e117793a2a7c1e80dab42ae91826f93\nSeeing the logs of the container,\nâŸ© docker logs -f 68dc2b21f5e2b8a6d54c35dc46a56f6d0e117793a2a7c1e80dab42ae91826f93\n \n&gt; backend@1.0.0 start /\n&gt; node .\n \n(node:18) UnhandledPromiseRejectionWarning: Error: getaddrinfo EAI_AGAIN mysql-server\n    at Object.createConnection (/node_modules/mysql2/promise.js:242:31)\n    at Object.&lt;anonymous&gt; (/index.js:54:4)\n    at Module._compile (internal/modules/cjs/loader.js:999:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1027:10)\n    at Module.load (internal/modules/cjs/loader.js:863:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:708:14)\n    at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:60:12)\n    at internal/main/run_main_module.js:17:47\n(node:18) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 1)\n(node:18) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\n \nðŸ¤¯ We are bombarded with an error. If we see it, it is crying because of the MySQL connection. Our container is not able to connect to the MySQL server. And this is straightforward as no MySQL container is running in the network. We will look at it later when we compose our application. For now, letâ€™s move ahead.\nCreate Dockerfile for frontend\nCopy the following Dockerfile content inside frontend/Dockerfile\nFROM node:12-alpine AS builder\n \nWORKDIR /app\n \nCOPY package.json package.json\n \nRUN npm install\n \nCOPY . .\n \nRUN npm run build\n \nFROM nginx:alpine\n \nWORKDIR /usr/share/nginx/html\n \nRUN rm -rf ./*\n \nCOPY --from=builder /app/build .\n \nENTRYPOINT [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]\n\nOption daemon off specifies nginx to run in the foreground so that the container will continue running. If we donâ€™t specify this option, the nginx service would run in the background and the container will stop running as there is nothing to hold it up.\n\nLetâ€™s build the image.\nâŸ© docker build -t bitphile/todo-frontend .                               (base)\nSending build context to Docker daemon  478.7kB\nStep 1/11 : FROM node:12-alpine AS builder\n ---&gt; bb6d28039b8c\nStep 2/11 : WORKDIR /app\n ---&gt; Using cache\n ---&gt; 8610e0568d30\nStep 3/11 : COPY package.json package.json\n ---&gt; 2cf4f42b3f97\nStep 4/11 : RUN npm install\n ---&gt; Running in 0606b5b332ec\nperformance.now() and performance.timeOrigin.\n \n&gt; core-js@3.29.1 postinstall /app/node_modules/core-js\n&gt; node -e &quot;try{require(&#039;./postinstall&#039;)}catch(e){}&quot;\n \nThank you for using core-js ( github.com/zloirock/core-js ) for polyfilling JavaScript standard library!\n \nThe project needs your help! Please consider supporting of core-js:\n&gt; opencollective.com/core-js\n&gt; patreon.com/zloirock\n&gt; boosty.to/zloirock\n&gt; bitcoin: bc1qlea7544qtsmj2rayg0lthvza9fau63ux0fstcz\n \nI highly recommend reading this: github.com/zloirock/core-js/blob/master/docs/2023-02-14-so-whats-next.md\n \n \n&gt; core-js-pure@3.29.1 postinstall /app/node_modules/core-js-pure\n&gt; node -e &quot;try{require(&#039;./postinstall&#039;)}catch(e){}&quot;\n \nThank you for using core-js ( github.com/zloirock/core-js ) for polyfilling JavaScript standard library!\n \nThe project needs your help! Please consider supporting of core-js:\n&gt; opencollective.com/core-js\n&gt; patreon.com/zloirock\n&gt; boosty.to/zloirock\n&gt; bitcoin: bc1qlea7544qtsmj2rayg0lthvza9fau63ux0fstcz\n \nI highly recommend reading this: github.com/zloirock/core-js/blob/master/docs/2023-02-14-so-whats-next.md\n \nadded 1489 packages from 684 contributors and audited 1490 packages in 69.985s\n \n233 packages are looking for funding\n  run `npm fund` for details\n \nfound 1 high severity vulnerability\n  run `npm audit fix` to fix them, or `npm audit` for details\nRemoving intermediate container 0606b5b332ec\n ---&gt; e84821d3c273\nStep 5/11 : COPY . .\n ---&gt; 2582c9e1ccf7\nStep 6/11 : RUN npm run build\n ---&gt; Running in 24a2880afcea\n \n&gt; todo-app@0.1.0 build /app\n&gt; react-scripts build\n \nCreating an optimized production build...\nCompiled successfully.\n \nFile sizes after gzip:\n \n  47.56 kB  build/static/js/main.bf9caf07.js\n  1.78 kB   build/static/js/787.2843ca88.chunk.js\n  264 B     build/static/css/main.e6c13ad2.css\n \nThe project was built assuming it is hosted at /.\nYou can control this with the homepage field in your package.json.\n \nThe build folder is ready to be deployed.\nYou may serve it with a static server:\n \n  npm install -g serve\n  serve -s build\n \nFind out more about deployment here:\n \n  cra.link/deployment\n \nRemoving intermediate container 24a2880afcea\n ---&gt; ca6123013213\nStep 7/11 : FROM nginx:alpine\n ---&gt; 2bc7edbc3cf2\nStep 8/11 : WORKDIR /usr/share/nginx/html\n ---&gt; Using cache\n ---&gt; 14150ff7fb4b\nStep 9/11 : RUN rm -rf ./*\n ---&gt; Using cache\n ---&gt; dec51b86101d\nStep 10/11 : COPY --from=builder /app/build .\n ---&gt; bfc76e5e13ec\nStep 11/11 : ENTRYPOINT [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]\n ---&gt; Running in c01fb95e9ca3\nRemoving intermediate container c01fb95e9ca3\n ---&gt; 9ad959056d7a\nSuccessfully built 9ad959056d7a\nSuccessfully tagged bitphile/todo-frontend:latest\n \nLetâ€™s run the image.\nâŸ© docker run -d --name todo-frontend -p 4000:80 bitphile/todo-frontend\nNow, if we open http://localhost:4000, we should the to-do page.\nCreate Docker Compose file\nIt is time to take the whole picture of our application. To run our application with all those services, we have to create a docker-compose.yml file. Letâ€™s do create one.\nversion: &#039;3.7&#039;\nname: todo-app\n \nservices:\n  todo-frontend:\n    container_name: todo-frontend\n    build: ./frontend\n    ports:\n      - 4000:80\n    networks:\n      - todo-app\n    depends_on:\n      - todo-backend\n \n  todo-backend:\n    container_name: todo-server\n    build: ./backend\n    ports:\n      - 3001:3001\n    networks:\n      - todo-app\n    environment:\n      - NODE_ENV=production\n      - DB_USER=root\n      - DB_PASSWORD=password\n    depends_on:\n      - mysql\n \n  mysql:\n    container_name: mysql-server\n    image: mysql:5.7\n    volumes:\n      - ./mysql:/var/lib/mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: password\n      MYSQL_DATABASE: todo_app\n    networks:\n      - todo-app\n \nnetworks:\n  todo-app:\n \nLetâ€™s do compose up,\nâŸ© docker compose up -d                                                   (base)\n[+] Building 83.3s (22/22) FINISHED\n =&gt; [todo-app_todo-frontend internal] load build definition from Dockerfi  0.0s\n =&gt; =&gt; transferring dockerfile: 305B                                       0.0s\n =&gt; [todo-app_todo-backend internal] load build definition from Dockerfil  0.0s\n =&gt; =&gt; transferring dockerfile: 191B                                       0.0s\n =&gt; [todo-app_todo-frontend internal] load .dockerignore                   0.0s\n =&gt; =&gt; transferring context: 61B                                           0.0s\n =&gt; [todo-app_todo-backend internal] load .dockerignore                    0.0s\n =&gt; =&gt; transferring context: 60B                                           0.0s\n =&gt; [todo-app_todo-frontend internal] load metadata for docker.io/library  0.0s\n =&gt; [todo-app_todo-backend internal] load metadata for docker.io/library/  0.0s\n =&gt; CACHED [todo-app_todo-frontend builder 1/6] FROM docker.io/library/no  0.0s\n =&gt; [todo-app_todo-frontend stage-1 1/4] FROM docker.io/library/nginx:alp  0.0s\n =&gt; [todo-app_todo-frontend internal] load build context                   0.0s\n =&gt; =&gt; transferring context: 458.37kB                                      0.0s\n =&gt; CACHED [todo-app_todo-frontend stage-1 2/4] WORKDIR /usr/share/nginx/  0.0s\n =&gt; [todo-app_todo-frontend stage-1 3/4] RUN rm -rf ./*                    0.5s\n =&gt; CACHED [todo-app_todo-frontend builder 2/6] WORKDIR /app               0.0s\n =&gt; [todo-app_todo-frontend builder 3/6] COPY package.json package.json    0.0s\n =&gt; [todo-app_todo-backend internal] load build context                    0.0s\n =&gt; =&gt; transferring context: 1.60kB                                        0.0s\n =&gt; [todo-app_todo-backend 2/4] COPY package.json package.json             0.0s\n =&gt; [todo-app_todo-frontend builder 4/6] RUN npm install                  69.0s\n =&gt; [todo-app_todo-backend 3/4] RUN npm install                            5.9s\n =&gt; [todo-app_todo-backend 4/4] COPY index.js index.js                     0.0s\n =&gt; [todo-app_todo-frontend] exporting to image                            0.2s\n =&gt; =&gt; exporting layers                                                    0.0s\n =&gt; =&gt; writing image sha256:c30480a00b3ca5464f566693178dd5ae117d60a3a62e5  0.0s\n =&gt; =&gt; naming to docker.io/library/todo-app_todo-backend                   0.0s\n =&gt; =&gt; writing image sha256:28501b3d67ba977a812cfc9e21e91279e52765dc1276a  0.0s\n =&gt; =&gt; naming to docker.io/library/todo-app_todo-frontend                  0.0s\n =&gt; [todo-app_todo-frontend builder 5/6] COPY . .                          0.0s\n =&gt; [todo-app_todo-frontend builder 6/6] RUN npm run build                13.5s\n =&gt; [todo-app_todo-frontend stage-1 4/4] COPY --from=builder /app/build .  0.0s\n[+] Running 4/4\n â ¿ Network todo-app_todo-app  Created                                      0.0s\n â ¿ Container mysql-server     Started                                      0.7s\n â ¿ Container todo-server      Started                                      1.1s\n â ¿ Container todo-frontend    Started                                      1.6s\n \nGreat! Containers seemed to be started. Letâ€™s see,\nâŸ© docker compose ps -a                                                   (base)\nNAME                COMMAND                  SERVICE             STATUS              PORTS\nmysql-server        &quot;docker-entrypoint.sâ€¦&quot;   mysql               running             3306/tcp, 33060/tcp\ntodo-frontend       &quot;nginx -g &#039;daemon ofâ€¦&quot;   todo-frontend       running             0.0.0.0:4000-&gt;80/tcp\ntodo-server         &quot;docker-entrypoint.sâ€¦&quot;   todo-backend        running             0.0.0.0:3001-&gt;3001/tcp, 3001/tcp\n \nNow, open http://localhost:4000 and add todos.\nAn introduction to deploys\nWe can create multiple containers for the same service using deploy in docker compose.\nversion: &#039;3.7&#039;\nname: todo-app\n \nservices:\n  todo-frontend:\n    container_name: todo-frontend\n    build: ./frontend\n    ports:\n      - 4000:80\n    networks:\n      - todo-app\n    depends_on:\n      - todo-backend\n \n  todo-backend:\n    container_name: todo-server\n    build: ./backend\n    ports:\n      - 3000:3000\n    networks:\n      - todo-app\n    environment:\n      - NODE_ENV=production\n      - DB_USER=root\n      - DB_PASSWORD=password\n    depends_on:\n      - mysql\n    deploy:\n\t    mode: replicated\n\t    replicas: 2\n \n  mysql:\n    container_name: mysql-server\n    image: mysql:5.7\n    volumes:\n      - ./mysql:/var/lib/mysql\n    environment:\n      MYSQL_ROOT_PASSWORD: password\n      MYSQL_DATABASE: todo_app\n    networks:\n      - todo-app\n \nnetworks:\n  todo-app:\nSo, this would create two replicas of the service todo-backend. The routing of which container the request should be sent to would be taken care of by the docker itself.\nRestart policy\nIf the container dies unexpectedly, docker compose can restart the container. Docker compose provides a restart policy for the same that we can define in docker compose file.\nversion: &#039;3.7&#039;\nname: todo-app\n \nservices:\n  todo-backend:\n    container_name: todo-server\n    build: ./backend\n    ports:\n      - 3001:3001\n    networks:\n      - todo-app\n    environment:\n      - NODE_ENV=production\n      - DB_USER=root\n      - DB_PASSWORD=password\n    depends_on:\n      - mysql\n    deploy:\n\t    mode: replicated\n\t    replicas: 2\n\t    restart_policy:\n\t\t    condition: any\n\t\t    delay: 5s\n\t\t    max_attempts: 2\n\t\t    window: 80s\ncondition means the condition to restart the service. delay is the delay in subsequent restart tries and window means how long to wait to decide if the restart is done.\nConclusion\nWell, this is it for now. I hope you find this useful. If there is any problem while setting up the project and running that up, please drop a comment. Please share your feedback and what our next junction topic should be, in the comment section.\nUntil then,\n\nReferences\n\nDocker Compose file reference\n"},"blogs/tech/docker/docker-compose":{"title":"Introduction to Docker Compose","links":["blogs/tech/docker/docker-networking"],"tags":["docker","docker-compose","compose","services","management","orchestration"],"content":"Hello people! Good to see you here. Today we would be learning about docker compose, what is it, why we need it, and its examples. Letâ€™s begin.\nAgenda\n\nCurrent Scenario\nWhat is docker-compose?\nHow does docker compose change the current scenario?\nDefining Services\nVolume and Bind mounts\nNetworking\nEnvironment Variables\nConclusion\n\nCurrent Scenario\nA single application comprises containers for different services responsible for their respective tasks. For example, a backend service, a database service to manage data, a frontend for the app, and so on. All of these services need to be in the same network to communicate with each other. I have explained docker networking in my post Docker Networking.\nManaging all these services and different constituents such as networking, volumes, bind mounts, etc becomes an arduous task.\n\nThe following points summarise the problems in the current scenario:\n\nNetworking. We have to explicitly create and manage networks. We have to be sure that a particular container is connected to a particular network.\nVolumes and bind mounts. There is no centralized way to know what all volumes are connected to all services of an application.\nContainer lifecycle. If any container stops, it becomes our responsibility to restart the container.\n\nThese are some of the major problems. Setting up a multi-container application comprises multiple commands. For example,\n# for running database container\ndocker run -d --name database --network bitphile images/mysql\n \n# for running frontend\ndocker run -d --name frontend -p 3001:3001 --network bitphile bitphile/frontend\n \n# for running backend\ndocker run -d --name backend --network bitphile -mount type=volume,source=bitphile,target=/vol bitphile/backend\nThis is just for a small multi-container application comprising three services. Think about if the application is composing several services.\nDocker Compose\nDocker compose is a tool that helps in defining and running multi-container applications. It enables us to define each container as a service, its interactions with other services, and some other configurations.\nHow does it help to tackle the current Scenario?\nLetâ€™s see each of the points mentioned in the problem statement and scrutinize how docker compose helps in there.\nNetworking\nServices defined inside a docker compose for a particular application are by default in a network for that project. When the docker compose is up, all those services are in a project namespace network.\nSo, there is no need to worry if the frontend sends a request to the backend. Moreover, the networking properties still apply here. A container can be referenced with its name for sending a request instead of its IP address.\nWe will this into action shortly in a subsequent section.\nVolumes and bind mounts\nWe can define volumes and bind mounts for services inside the docker compose definition file. We can specify to delete the volumes when doing compose down.\nContainers Life Cycle\nDocker compose acts like an orchestrator which manages the life cycle of containers. Though it is not much effective for small applications it suffices. It provides ways to specify restart the container if it dies.\nIt becomes very easy to create, run and stop services by using docker compose.\nWhole application picture in a single place\nUsing docker compose, the application becomes fathomable. By seeing the docker compose file itself we can identify what is happening. What all services are there, what is the networking structure, what volumes are in use, etc.\nWell, it is all about theory. Letâ€™s see docker compose in action.\nDefining services\nWe are required to create a docker compose file named docker-compose.yml (you can give any other name but have to specify explicitly in the command, we will see shortly).\nA minimal docker compose file looks like following\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n \nExplanation\n\nversion defines the version of Compose file. It is required to identify the schema used for the docker compose file.\nname is the name of the project. If it is omitted, the project directory name is assigned to name.\nservices defines what all containers the compose comprises. Each item is the name of the service.\n\nBuilding image\nThere are two ways we can define service.\n\nSpecifying how docker can build an image by giving build property.\nDirectly giving the image.\n\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    build:\n\t    dockerfile: Dockerfile\n\t\tcontext: .\n \nThis builds the image and spins up the container.\nDirectly using image\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: node-alpine\nThis will lead to pulling off the image and then spinning up the container.\nVolume and Bind mounts\nIt is fairly simple to mount a volume in docker compose. There are following ways a docker volume can be mounted.\nLet docker create volume for you\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    volumes:\n\t    - /app/volume\n \nNow, if we spin up the compose\nâŸ© docker compose up -d\nand listing down the volumes, we get\nâŸ© docker volume ls                                                           \nDRIVER    VOLUME NAME\nlocal     a98a19a6d84da93031cb6a331993d2a49bfde5689594a413caddefe4a84d4aef\nSo, docker itself created a volume for you.\nNamed Volume\nWe can name our volume by explicitly specifying volumes section in compose file.\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    volumes:\n\t    - bitphile:/app/volume\n \n \nvolumes:\n\tbitphile:\n\t\tdriver: local\nvolumes can have more properties. You can check those out on the official docs of docker (I have mentioned them in the references section).\nBind a host path\nWe can bind a host path as follows\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    volumes:\n\t    - ./host/path:/app/host\n \n\nNote: The structure of the volumes item is, [source:]target[:mode] where mode can be ro (read only) and rw (read and write).\n\nNetworking\nDocker compose by default creates a default network where all the defined services reside. However, we can explicitly create a network and define the services.\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    network:\n\t    - bitphile\n \nnetworks:\n\tbitphile:\n\t\tdriver: bridge\nWe donâ€™t need to mention driver properties. Docker uses it as default. Just to mention that we can specify more properties, I have specified this property.\nEnvironment Variables\nYou must be already aware of environment variables and their use cases. We can use environment variables in docker compose in the following ways.\nPassing to services\n\nUsing environment\nUsing env_file\n\nEnvironment variables\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    environment:\n\t    - NODE_ENV=production\nEnvironment file\nversion: &#039;3.7&#039;\nname: bitphile\n \nservices:\n  todo-backend:\n    container_name: todo-backend\n    image: bitphile/todo-backend\n    env_file: \n     - .env-file\nPopulating Environment variables in compose file\n\nPassing environment file through docker compose command\nUsing .env file\n\nPassing through CLI\nDocker provides --env-file flag to specify the environment file to use.\nâŸ© docker compose --env-file .env-file up\nUsing .env\nWe donâ€™t need to specify this file. Docker takes it by default.\nConclusion\nSo, docker compose is a powerful tool helping in creating and managing multi containers applications. It provides several features and options which canâ€™t be covered in this post. If you are interested in deep diving into those features, I would recommend you to go through the official docs. For simple uses, I donâ€™t think you would have to rummage through all those options.\nSo, this is it for now. We have reached our junction and our train has to take a halt. Letâ€™s continue our journey on the next green light with some other interesting topic.\nPlease provide your valuable feedback about what you liked, what has to improve, and what topic should be there for our next junction.\nUntil then,\n\nReferences\n\nDocker compose overview\nDocker CLI reference\nDocker compose file\n"},"blogs/tech/docker/docker-image-layers":{"title":"Docker Image Layers","links":["blogs/tech/docker/getting-started-with-docker","blogs/tech/docker/docker-storage-driver"],"tags":["docker","infra","kubernetes","docker-image","image-layers","image"],"content":"Deep dive into Docker Image Layers\nIn my previous post Getting started with docker, I explained the basics of docker. In this post, we will be deep diving into docker image layers. We will be covering the following topics in this post,\n\nWhat are docker image layers?\nWhy does knowing docker image layers matter?\nWhere do these intermediary images reside and how to see them?\nWhat are the constituents of a docker container size?\n\nAre you ready? Yes! So letâ€™s startâ€¦\nWhat are docker image layers?\nWhen we build a docker image using Dockerfile, you must have seen the logs on the terminal console. There you must have seen some sha IDs that it shows. Those sha IDs are nothing but intermediary image layers IDs.\nâŸ© docker build -f Dockerfile.new -t bitphile/test-image .      (base)\nSending build context to Docker daemon  7.168kB\nStep 1/4 : FROM alpine\n ---&gt; da7260331371\nStep 2/4 : COPY app.sh ./app.sh\n ---&gt; 198fdf85c15b\nStep 3/4 : RUN chmod +x app.sh\n ---&gt; Running in cf5c357a3254\nRemoving intermediate container cf5c357a3254\n ---&gt; 8e758ee45de3\nStep 4/4 : ENTRYPOINT [&quot;./app.sh&quot;]\n ---&gt; Running in 857700b4b452\nRemoving intermediate container 857700b4b452\n ---&gt; 4d85107b2c59\nSuccessfully built 4d85107b2c59\nSuccessfully tagged bitphile/test-image:latest\nA docker image is made up of several docker image layers. Each of the commands in Dockerfile stacks up a new image layer on top of the previous one.\n\nNote: Commands in Dockerfile are those instructions which do some changes in the file system. Any change made in the file system creates a new image layer. So, commands are responsible for the creation of a new image layer.\nFor example, COPY, RUN, etc are commands.\n\nThe following image shows the docker image layers having ID and their corresponding parent layer ID.\n\n\nNote: Each of the layers coming on top of the existing one depends on their parent layer. Changing the sequence may altogether change the final image. Caching of image layers also depends upon this (upcoming sections).\n\nR/W image layers\nAn image layer can be read-only or read and write. All the intermediate image layers are read-only. One canâ€™t create a file in any of those layers. If anyone of you has worked on docker must be thinking, bitPhile is saying wrong as we can create files in running docker container.\nYour concern is right. We can create files in the docker container. The reason behind this is Container Layer which is explained in the next section.\nContainer Layer\nWhen an image is turned into a container, a new thin layer is stacked on top of the layers. This thin layer is called container layer. This image layer has both Read and Write access. All other image layers beneath this can only be read.\n\nSource: docs.docker.com/storage/storagedriver/images/container-layers.jpg\nWhy does knowing Image Layers matter?\nThere are several reasons why one should know about docker image layers. A few of them are described below.\nImage Size\nAn image is a pack of all dependencies, libraries, and src code required for an application. These images are stored on cloud registries and shared across devices. Size becomes a foremost requirement for an image.\nImage layers size consititute into the size of the final image. Identifying which layer is adding more size can help the developer to find areas of concern.\nLayers are Cached\nImage layers are stored on the host machine for future builds. This helps build docker images faster.\nLetâ€™s see an example. Consider two Dockerfiles, Dockerfile.new-1 and Dockerfile.new-2.\nDockerfile.new-1\n# syntax=docker/dockerfile:1\nFROM bitphile/my-base-image\n \nCOPY app.sh ./app.sh\n \nRUN chmod +x app.sh\n \nENTRYPOINT [&quot;./app.sh&quot;]\nDockerfile.new-2\n# syntax=docker/dockerfile:1\n \nFROM bitphile/my-base-image\n \nCOPY app.sh ./app.sh\nBuilding the first image, the consoles show,\nSending build context to Docker daemon  7.168kB\nStep 1/4 : FROM bitphile/my-base-image\n ---&gt; da7260331379\nStep 2/4 : COPY app.sh ./app.sh\n ---&gt; 198fdf85c15b\nStep 3/4 : RUN chmod +x app.sh\n ---&gt; Running in cf5c357a3254\nRemoving intermediate container cf5c357a3254\n ---&gt; 8e758ee45de3\nStep 4/4 : ENTRYPOINT [&quot;./app.sh&quot;]\n ---&gt; Running in 857700b4b452\nRemoving intermediate container 857700b4b452\n ---&gt; 4d85107b2c59\nSuccessfully built 4d85107b2c59\nSuccessfully tagged bitphile/my-new-image:latest\nThe second, it shows,\nSending build context to Docker daemon  8.192kB\nStep 1/2 : FROM bitphile/my-base-image\n ---&gt; da7260331379\nStep 2/2 : COPY app.sh ./app.sh\n ---&gt; Using cache\n ---&gt; 198fdf85c15b\nSuccessfully built 198fdf85c15b\nSuccessfully tagged bitphile/my-new-image-2:latest\nWe can see it uses caches for the second build. This increases the speed of building the image. Interesting, right?\n\nNote: If any instruction changes, caches are not used for the next instructions. New image layers are created for them from the scratch. Why does this happen? Think about it!\n\nSeeing Image Layers\nOnce the final image is built, we can see intermediary image layers by,\ndocker history &lt;image-name&gt;\nSo, by running the above commands,\ndocker history bitphile/my-new-image\nGives the output,\nIMAGE          CREATED          CREATED BY                                      SIZE      COMMENT\n4d85107b2c59   50 minutes ago   /bin/sh -c #(nop)  ENTRYPOINT [&quot;./app.sh&quot;]      0B\n8e758ee45de3   50 minutes ago   /bin/sh -c chmod +x app.sh                      33B\n198fdf85c15b   50 minutes ago   /bin/sh -c #(nop) COPY file:1179990b720068e4â€¦   33B\nda7260331379   2 hours ago      /bin/sh -c apk add --no-cache bash              2.24MB\nb2aa39c304c2   3 weeks ago      /bin/sh -c #(nop)  CMD [&quot;/bin/sh&quot;]              0B\n&lt;missing&gt;      3 weeks ago      /bin/sh -c #(nop) ADD file:40887ab7c06977737â€¦   7.05MB\n \nThis shows the intermediate image ID, Dockerfile instruction, and size.\n\nNote: &lt;missing&gt; indicates that a particular image is built on a different machine or using builtKit docker builder.\n\nImage Layers on Machine\nWe can find these image layers on the host machine at location /var/lib/docker. If you are using a virtual machine to run a docker daemon, ssh into that machine and access this location.\nListing the contents of /var/lib/docker gives,\ndrwx--x--x   4 root root 4.0K Feb 26 09:14 buildkit\ndrwx--x--x   3 root root 4.0K Feb 26 09:14 containerd\ndrwx--x---  20 root root 4.0K Mar  5 05:51 containers\ndrwx------   3 root root 4.0K Dec 16  2021 image\ndrwxr-x---   3 root root 4.0K Feb 26 09:14 network\ndrwx--x--- 173 root root  24K Mar  5 05:51 overlay2\ndrwx------   4 root root 4.0K Feb 26 09:14 plugins\ndrwx------   2 root root 4.0K Mar  5 02:42 runtimes\ndrwx------   2 root root 4.0K Feb 26 09:14 swarm\ndrwx------   2 root root 4.0K Mar  5 06:21 tmp\ndrwx------   2 root root 4.0K Feb 26 09:14 trust\ndrwx-----x   6 root root 4.0K Mar  5 02:42 volumes\nWe can find these images in images/&lt;storage-driver-name&gt;/imagedb/content/sha25. In my case, overlay2 is storage driver. Refer my post Storage Driver.\nListing content of images/overlay2/imagedb/content/sha25, we see,\n-rw------- 1 root root 1.9K Mar  5 05:10 08bd868662db2cd09a4c7b8e74dad6a9c8fe186964ff31c65d335299d1324ff3\n-rw------- 1 root root 1.9K Mar  5 05:51 198fdf85c15b292a5fff554f3afc26fd1824413822ddd75e72f08937889f1770\n-rw------- 1 root root 2.2K Mar  5 05:11 1d17f325c607e937d89b19de66287aad351fc1abb80bc36591d34b7457ff8316\n-rw------- 1 root root 1.7K Dec 16  2021 25f8c7f3da61c2a810effe5fa779cf80ca171afb0adf94c7cb51eb9a8546629d\n-rw------- 1 root root  16K Mar  1 12:49 2bc7edbc3cf2fce630a95d0586c48cd248e5df37df5b1244728a5c8c91becfe0\n-rw------- 1 root root 2.4K Feb 26 09:51 2f6974482449313e863b0b709841b50da40ce7ef32017f0279671e9b2ee47289\n-rw------- 1 root root  11K Mar  1 06:10 31d22a1554dfca7ad9377b67974ae2e0e3282247fc40fbf2ac3c380102ae772f\n-rw------- 1 root root 4.4K Mar  1 05:26 3c3a210148bab6bc3faedba72ad7de5e58a7cdb6bba0847965211f47fe1c4c21\n-rw------- 1 root root 2.1K Feb 26 09:35 da7260331379a532a15cf3e9bff522627b1574a9ce21bb06c92cea74992613fa\nAs you can see da7260331379 is used as a cache to build the image.\nDocker Container Size\nThere are two things, size and virtual size.\n\nsize is the size of container layer. It comprises of all the files that are written container layer.\n\n\nvirtual size is the sum of size and data of read-only intermediate image layers. If two containers have the same read-only intermediate image layers, will share the same data with separate container layer data.\n\nFor example,\nCONTAINER ID   IMAGE                                          COMMAND                  CREATED         STATUS                   PORTS                  NAMES                                                                                                         SIZE\n7937d5f33e6e   bitphile/my-new-image-2                        &quot;./app.sh&quot;               5 seconds ago   Up 5 seconds                                    bitphile-test-container                                                                                       0B (virtual 9.29MB)\nshows container bitphile-test-container has 0B size and 9.29MB virtual size. If you see the size of the whole image itself is 9.29MB which means it is all size of read-only image layer data.\nNow, adding a new file into the running container should increase the size.\ndocker exec 7937d5f33e6e sh -c &quot;echo hello world &gt; file.txt&quot;\nNow if we see the same container,\nCONTAINER ID   IMAGE                                          COMMAND                  CREATED          STATUS                   PORTS                  NAMES                                                                                                         SIZE\n7937d5f33e6e   bitphile/my-new-image-2                        &quot;./app.sh&quot;               39 seconds ago   Up 39 seconds                                   bitphile-test-container                                                                                       11B (virtual 9.29MB)\ngot 11B of size.\nSo, this is it for now. Itâ€™s great that you come reading till here. I will see you in my next post on docker. Till then,\n\nReferences\n\nStorage Drivers \n"},"blogs/tech/docker/docker-networking":{"title":"Docker Networking - How does it work?","links":[],"tags":["docker","networking","ports","bridge","routintg","routing-mesh","overlay"],"content":"Hello folks! Good to see you again on our journey of Docker. In this post, we would be learning about docker networking. So, letâ€™s get started.\nWhat is covered?\n\nAbstract\nProblem Statement\nHow does Docker Network solve the problem?\nSimple example\nTypes of networks based on drivers\nDiving deep into bridge networks\nConclusion\n\nAbstract\nDocker has been a powerful tool for creating and managing independent applications and services. These services are interacting with each other through specified interfaces. However, the isolated environment of docker stymies that interaction. An isolated environment hinders services request-response communications as they have different network namespaces.\nTo solve this problem, docker introduced the concept of networking. Docker Networking.\nProblem without networking\nApplications are running in their containers. These containers have their networking interfaces, namespace, etc. They are not aware of other containers. If there is an application that wants to communicate with another application, sends a request to a specified endpoint of another application. The request never reaches another application as networks are different and isolated.\nFor example, consider a chat application feeberchat (I created this app a long time ago ðŸ˜„). feeberchat is running inside a container A. The database of feeberchat is getting stored in an instance of mongodb running inside another container B.\nAs both A and B have different network namespaces, they canâ€™t communicate.\n\nHow does Docker Networking solve the problem?\nDocker networking allows to create isolated networks for containers to communicate with each other. It creates a common networking namespace where containers reside and communicate.\n\nAn example\nAssume we have a node (express) application running on port 3000, dockerized with endpoints:\n\n/\n/health\n\nRunning that application in no network using\nâŸ© docker run -d --name express-app --network none bitphile/express-app       (base)\nLetâ€™s try to access it from another container.\nâŸ© docker run -it --rm --name alpine-shell alpine:latest sh       (base)\nIt opens up the container. Now we have to send a request to container express-app. What is the endpoint? We donâ€™t know as express-app is running without any network. Docker didnâ€™t assign any IP address to express-app container.\nNow, letâ€™s run express-app again differently\nâŸ© docker run -d --name express-app bitphile/express-app                (base)\nInspecting network details of express-app gives\n{\n  &quot;bridge&quot;: {\n    &quot;IPAMConfig&quot;: null,\n    &quot;Links&quot;: null,\n    &quot;Aliases&quot;: null,\n    &quot;NetworkID&quot;: &quot;8f7326c66c80c45501e24eb7c29596b9a1065624a81b443ef34f8e5c59abab05&quot;,\n    &quot;EndpointID&quot;: &quot;987f33191fc7843190ac74227c8d062b6a8e2b9c2101e52e289f2f82e6f8c9af&quot;,\n    &quot;Gateway&quot;: &quot;172.17.0.1&quot;,\n    &quot;IPAddress&quot;: &quot;172.17.0.3&quot;,\n    &quot;IPPrefixLen&quot;: 16,\n    &quot;IPv6Gateway&quot;: &quot;&quot;,\n    &quot;GlobalIPv6Address&quot;: &quot;&quot;,\n    &quot;GlobalIPv6PrefixLen&quot;: 0,\n    &quot;MacAddress&quot;: &quot;02:42:ac:11:00:03&quot;,\n    &quot;DriverOpts&quot;: null\n  }\n}\nIt bombarded us with a bunch of details. Letâ€™s not worry about the rest of the details and take IPAddress into consideration. IPAdderss is the IP address of container express-app in the default bridge network (coming in the following sections. Just assume it is as a networking type).\nLetâ€™s send a request to this ip address from alpine-shell container.\n$ curl 172.17.0.3:3000/health\nThis is healthy $\nGreat! We got some results.\nWhat did happen?\nWhen we run express-app container without specifying --network flag, it creates a virtual network (which is called default bridge network). Docker assigns an ip address from that virtual network to the container. Thatâ€™s why we could see IPAddress in docker inspect.\nIf you try running express-app with --network none flag and inspect the container, you wouldnâ€™t be seeing any network information.\n\nNetwork drivers\nDocker provides various network drivers for different types of requirements. Following is a list of those drivers.\n\nbridge\nhost\noverlay\nmacvlan\nipvlan\n\nEach of these drivers has different capabilities. Select a driver best suits your requirements. For the sake of simplicity and the size of the post, we would be going with bridge network driver.\nBridge Networks\nAligning ourselves with the name, we can think of a bridge as something that connects segments. In networking, it is a Link Layer device that connects network segments.\nIn docker, bridge networks connect containers in a network where they can communicate easily while allowing isolation from the rest of the containers not connected to the same bridge network.\nBridge networks can only be used on the same docker daemon host. For networking among containers from different docker hosts, prefer some other docker networking driver (eg overlay network).\nDefault Bridge Network\nDocker creates this network by default if we donâ€™t specify any network to use.\nâŸ© docker network ls                                                              (base)\nNETWORK ID     NAME         DRIVER    SCOPE\n8f7326c66c80   bridge       bridge    local\n1b69478406e4   host         host      local\n9324e36db9ab   none         null      local\nLook bridge network of the driver bridge. This is the default bridge network created and managed by docker. Though we can also configure it but not doing also suffices the requirements in most cases.\n\nNote: Have a look at none network with null driver. This is what we have used for no networking.\n\nUser-defined Bridge Network\nWe can also create a network of driver bridge. This has a few advantages over default bridge network:\n\nThe most important, user-defined bridge network provides automatic DNS resolution between containers. It means, we can directly use container names to send requests instead of IP addresses.\nUser has total control over the configurations on the network.\nOnly containers with this network type can communicate.\n\nUsing user-defined Bridge Network\nCreating a network\nFollowing is the command used to create a docker network\ndocker network create [options] &lt;name&gt;\nLetâ€™s create a network of the name bitphile.\nâŸ© docker network create bitphile                                                (base)\n2bfb26180adc42445ca0127f8628d45b71f1871b8b3fdea2a7d3da8b31e9dd17\nâŸ© docker network ls                                                             (base)\nNETWORK ID     NAME       DRIVER    SCOPE\n2bfb26180adc   bitphile   bridge    local\n8f7326c66c80   bridge     bridge    local\n1b69478406e4   host       host      local\n9324e36db9ab   none       null      local\nBy default, it creates a network of driver type bridge. We can specify the driver using --driver flag.\nâŸ© docker network create --driver macvlan bitphile                               (base)\n237a35cfa290dd59db70b099cf970efd5efbd29f150ae65c7add51632d704a81\nInspecting the network details gives us\n[\n  {\n    &quot;Name&quot;: &quot;bitphile&quot;,\n    &quot;Id&quot;: &quot;2bfb26180adc42445ca0127f8628d45b71f1871b8b3fdea2a7d3da8b31e9dd17&quot;,\n    &quot;Created&quot;: &quot;2023-03-19T09:42:37.48139717Z&quot;,\n    &quot;Scope&quot;: &quot;local&quot;,\n    &quot;Driver&quot;: &quot;bridge&quot;,\n    &quot;EnableIPv6&quot;: false,\n    &quot;IPAM&quot;: {\n      &quot;Driver&quot;: &quot;default&quot;,\n      &quot;Options&quot;: {},\n      &quot;Config&quot;: [\n        {\n          &quot;Subnet&quot;: &quot;172.20.0.0/16&quot;,\n          &quot;Gateway&quot;: &quot;172.20.0.1&quot;\n        }\n      ]\n    },\n    &quot;Internal&quot;: false,\n    &quot;Attachable&quot;: false,\n    &quot;Ingress&quot;: false,\n    &quot;ConfigFrom&quot;: {\n      &quot;Network&quot;: &quot;&quot;\n    },\n    &quot;ConfigOnly&quot;: false,\n    &quot;Containers&quot;: {},\n    &quot;Options&quot;: {},\n    &quot;Labels&quot;: {}\n  }\n]\nRunning the container with this network\nâŸ© docker run -d --name express-app --network bitphile bitphile/express-app\n70ae390032cbf2e1e38e749895cf17533c70021168a457480a3dd57f035296fe\nIf we inspect the network bitphile again, we would see a list of containers using this network.\n[\n  {\n    &quot;Name&quot;: &quot;bitphile&quot;,\n    &quot;Id&quot;: &quot;56b7da97479a3a27859e2c62b5a9f84b8389b5b3d5b41b40283b56a40e88d1a6&quot;,\n    &quot;Created&quot;: &quot;2023-03-19T09:46:13.74404902Z&quot;,\n    &quot;Scope&quot;: &quot;local&quot;,\n    &quot;Driver&quot;: &quot;bridge&quot;,\n    &quot;EnableIPv6&quot;: false,\n    &quot;IPAM&quot;: {\n      &quot;Driver&quot;: &quot;default&quot;,\n      &quot;Options&quot;: {},\n      &quot;Config&quot;: [\n        {\n          &quot;Subnet&quot;: &quot;172.22.0.0/16&quot;,\n          &quot;Gateway&quot;: &quot;172.22.0.1&quot;\n        }\n      ]\n    },\n    &quot;Internal&quot;: false,\n    &quot;Attachable&quot;: false,\n    &quot;Ingress&quot;: false,\n    &quot;ConfigFrom&quot;: {\n      &quot;Network&quot;: &quot;&quot;\n    },\n    &quot;ConfigOnly&quot;: false,\n    &quot;Containers&quot;: {\n      &quot;70ae390032cbf2e1e38e749895cf17533c70021168a457480a3dd57f035296fe&quot;: {\n        &quot;Name&quot;: &quot;express-app&quot;,\n        &quot;EndpointID&quot;: &quot;0fda9ef2aa92e5966320c330776986c9cea15210354d2994e3060522619a5290&quot;,\n        &quot;MacAddress&quot;: &quot;02:42:ac:16:00:02&quot;,\n        &quot;IPv4Address&quot;: &quot;172.22.0.2/16&quot;,\n        &quot;IPv6Address&quot;: &quot;&quot;\n      }\n    },\n    &quot;Options&quot;: {},\n    &quot;Labels&quot;: {}\n  }\n]\nQuick introduction to subnet and gateway\nThis wonâ€™t be thorough. It would just be for understanding networking in the context of docker.\nSubnet\nIt is used to divide the larger network into the smaller network to make network management easy. For example, subnet 172.22.0.0/16 means the first 16 bites will be used for networks, and the rest of 16 bits will be used for hosts in the sub-network.\n \n172    22    0    0\n -------      ----\n network      host\nIn the context of docker, the subnet makes sure that the network interface has a unique IP address (gateway IP address) and the containers are assigned unique IP addresses. It also helps in routing traffic.\nGateway\nAs the name suggests, it is the entry point (and exit point) of a network segment.\nIn the context of docker, we have gateway 172.22.0.1 which is an entry point for network bitphile.\nSpecifying subnet and gateway\nIt is possible to provide a subnet and gateway while creating a network.\nâŸ© docker network create --subnet=172.20.0.0/16 --gateway=172.20.0.1 bitphile  (base)\n340e9f5c82185940df2a3a2009e6ef5276c1c6b440a9a8dc5376c5c5df448b90\nTry running the same command again and see what happens.\nYou canâ€™t create another network with the subnet already used.\nDelete a network\nDeleting something gives such a relaxation that canâ€™t be expressed. Specially deleting the code (donâ€™t delete a working code ðŸ˜‚).\nWe can delete the network using\nâŸ© docker network rm bitphile                                                    (base)\nbitphile\nConclusion\nFinally, we are at the home stretch of this post. Hope you found this useful and you got something important out of this. Please reach out to me if you have any feedback. I have given my mail below. See you at the next junction of our journey on docker.\nUntil then,\n\nReferences\n\nNetworking overview\nContainer networking\nDocker bridge networking\n\nðŸ“¨Mail to me"},"blogs/tech/docker/docker-storage-driver":{"title":"docker-storage-driver","links":[],"tags":[],"content":""},"blogs/tech/docker/docker-volume":{"title":"Docker Volume","links":["blogs/tech/docker/docker-image-layers"],"tags":["docker","infra","kubernetes","docker-volume","volume","storage","bind-mounts","tmpfs"],"content":"Understanding Docker Volume Thoroughly\nAgenda\n\nAbstract\nContainer layer and its data on docker host machine\nWhat are the flaws of storing data in the container layer?\nWhat is the solution?\nUsing docker volume\nConclusion\n\nAbstract\nA Docker container is an independent process comprising the application and its dependencies. Container has its processes, file system, and networks independent of the host machine.\nCreating a resource in the running container stores that resource in the Read-Write layer or Container layer. This is a temporary storage mechanism. It is accessible until the container is running. Once the container is removed, all the data goes.\nDocker provides a few mechanisms to tackle this problem and volume is one of them. Letâ€™s dig deeper into docker volumes.\n\nNote: All the examples are being done by using minikube for running docker daemon. Any mention of docker host machine refers to minikube node (You can go inside minikube node using, minikube ssh).\n\nContainer Layer\nI have explained about container layer in my previous post on Docker at Docker Image Layers. In this section, we would be seeing where actually the container layer is stored on the docker host machine. Excited? Seems like yes ðŸ˜„. Letâ€™s start.\nWhere does the container layer reside?\nContainer layer is a read-write layer stacked on top of read-only image layers. We canâ€™t modify read-only filesystem of read-only image layers. We can modify the contents in the file system of the container layer.\nLetâ€™s spin up an alpine container with sh command.\ndocker run --rm -it alpine sh\nCreating a file as\n/ # echo &quot;Docker is awesome - Nitin&quot; &gt; testimonials.txt\n/ # ls\nbin               home              mnt               root              srv               tmp\ndev               lib               opt               run               sys               usr\netc               media             proc              sbin              testimonials.txt  var\n\nNow, letâ€™s see where this data is stored on the docker host machine.\nInspecting the above container of ID 385f23abffa3 gives the following\n$ docker inspect --format &#039;{{ .GraphDriver.Data }}&#039; 385f23abffa3\nmap[LowerDir:/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032-init/diff:/var/lib/docker/overlay2/71129bf0c1fb93d386d1abd9390e68eb08b64e65cb31186a0e710931359adb72/diff MergedDir:/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/merged UpperDir:/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/diff WorkDir:/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/work]\n$\nWoo! It gives a map with a bunch of paths. Let me example you each of these paths. The following descriptions are based on overlay2 storage driver.\n\nLowerDir is the read-only file system for lower image layers. Any changes made to the file system are reflected in the new file system of the read-write container layer. LowerDir acts as a base file system.\nUpperDir is the read-write file system for the container layer where we can create and delete files and directories. Container layer changes are stored in this location.\nMergedDir is the merge of LowerDir and UpperDir. It gives a unified view of the file system for the container.\nWorkDir is something overlay2 driver uses for its internal operations such as copy-on-write process.\n\nAs you can see LowerDir has two paths separated by :.\nFirst one is the path of the lower image layer read-only file system.\n/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032-init/diff\n\nSecond one is the path of the lower image layer read-only file system after changes made by previous running containers.\n/var/lib/docker/overlay2/71129bf0c1fb93d386d1abd9390e68eb08b64e65cb31186a0e710931359adb72/diff\n\nThese two directories serve as a base read-only file system for the container layer read-write file system.\nAs we create a new file testimonials.txt in the container, letâ€™s peek into UpperDir to see this new file. Remember, this is on the docker host machine.\n$ sudo ls -la /var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/diff\ntotal 16\ndrwxr-xr-x 3 root root 4096 Mar 12 06:45 .\ndrwx--x--- 5 root root 4096 Mar 12 06:44 ..\ndrwx------ 2 root root 4096 Mar 12 06:44 root\n-rw-r--r-- 1 root root   26 Mar 12 06:45 testimonials.txt\n$\nYay! We have testimonials.txt on our docker host machine. If we cat we see,\n$ pwd\n/var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/diff\n$ cat testimonials.txt\nDocker is awesome - Nitin\nGreat. If you have noticed when we do ls inside the container, it gives more directories like,\n$:/ ls\nbin               home              mnt               root              srv               tmp\ndev               lib               opt               run               sys               usr\netc               media             proc              sbin              testimonials.txt  var\nHowever, we didnâ€™t see these directories on UpperDir. Guess why?\nThe reason is, UpperDir only has the changes made to the container file system. This is where MergedDir comes into existence which provides a unified view of the file system. MergedDir is the merge of LowerDir and UpperDir to give a unified file system.\nLetâ€™s see what is inside MergedDir.\n$ sudo ls /var/lib/docker/overlay2/7cb62392ec2ace2c1ea07bbdee8c4976105f4f081531a29b7c5a0320aa5ae032/merged\nbin  etc   lib    mnt  proc  run   srv  testimonials.txt  usr\ndev  home  media  opt  root  sbin  sys  tmp               var\n$\nAs we can see it is the same as the file system of the container. You can go ahead and try creating files in this location and see if it turns up in the container.\nThe Flaws of storing data in the container layer\n\n\nFirst thing first, storing data in the container layer is not permanent. Once you delete the container, all the data stored goes away.\n\n\nstorage-driver is getting used to dealing with the file system on the container. Using container file system for an IO-intensive application is always not a good idea. It causes performance issues and increases latency when accessing files stored on a container file system.\n\n\nContainer size gets increase when we start to store files on the container file system. I would give the example of a use case. Sometimes we get the need to create an image from the container itself. And if we have a huge amount of data inside the container, the resulting image would be big.\n\n\nSharing of data across containers is hard as it is highly coupled with the container itself.\n\n\nThese are the problems we are considering here. There may be different other problems.\nThen what is the solution, huh?\nSolution - Using Docker volume\nVolume is not the only solution. It is one of the solutions docker provides. Some other solutions are bind mound, tmpfs etc.\nWhat is Docker Volume?\nThis is the way of storing and managing persistent data outside of container file system. It is stored on a host file system managed by Docker. These volumes can be shared across different containers.\nCool, Huh?\nLetâ€™s create a volume\nVolumes are not tied to containers. So, we can create and manage volumes without even touching containers.\ndocker volume command is used to manage volumes.\n$ docker volume\n \nUsage:  docker volume COMMAND\n \nManage volumes\n \nCommands:\n  create      Create a volume\n  inspect     Display detailed information on one or more volumes\n  ls          List volumes\n  prune       Remove all unused local volumes\n  rm          Remove one or more volumes\n \nRun &#039;docker volume COMMAND --help&#039; for more information on a command.\n$\nLetâ€™s create a volume of the name bitphile, ðŸ˜.\n$ docker volume create bitphile\nbitphile\n$ docker volume ls\nDRIVER    VOLUME NAME\nlocal     bitphile\n$\nInspecting this volume gives us,\n$ docker volume inspect bitphile\n[\n    {\n        &quot;CreatedAt&quot;: &quot;2023-03-12T07:38:38Z&quot;,\n        &quot;Driver&quot;: &quot;local&quot;,\n        &quot;Labels&quot;: {},\n        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/bitphile/_data&quot;,\n        &quot;Name&quot;: &quot;bitphile&quot;,\n        &quot;Options&quot;: {},\n        &quot;Scope&quot;: &quot;local&quot;\n    }\n]\n$\nMountpoint is where this volume is mounted. That is the location on the host machine where volume data will be stored.\nLetâ€™s peek inside that location and see whatâ€™s there.\n$ sudo ls -la /var/lib/docker/volumes/bitphile/_data\ntotal 8\ndrwxr-xr-x 2 root root 4096 Mar 12 07:38 .\ndrwx-----x 3 root root 4096 Mar 12 07:38 ..\n$\nNOTHING? Well, we just created it ðŸ˜‚!\nMount Volume to container\nThere are two methods of mounting volume to the container.\n\n--mount option\n-v option\n\n-v option is like shorthand. --mount is verbose and explicit, which is why we going to use --mount throughout this post.\nLetâ€™s create a container and mount it to this volume we have just created.\n$ docker run --rm -it --mount type=volume,source=bitphile,target=/bitphile-vol alpine sh\n/ #\n--mount option takes parameters are key=value pairs separated by ,.\n\ntype specifies the volume as type. --mount is being used by bind mounts, and tmpfs also. By default, type is volume. But for the sake of being explicit, I have used it.\nsource is the volume name.\ntarget is the path in the container file system that will be mounted with the volume. There is alias of this option as dest, destination etc.\n\nListing contents in the container file system root we see,\n/ # ls\nbin           home          opt           sbin          usr\nbitphile-vol  lib           proc          srv           var\ndev           media         root          sys\netc           mnt           run           tmp\n/ #\n\nWe have bitphile-vol directory mounted to volume bitphile.\nLetâ€™s create a file inside that directory.\n/ # echo Good morning &gt; bitphile-vol/greetings.txt\n/ # cat bitphile-vol/greetings.txt\nGood morning\n/ #\n\nPeeking on the volume mount location on the host machine, we see,\n$ sudo ls -la /var/lib/docker/volumes/bitphile/_data\ntotal 12\ndrwxr-xr-x 2 root root 4096 Mar 12 07:48 .\ndrwx-----x 3 root root 4096 Mar 12 07:38 ..\n-rw-r--r-- 1 root root   13 Mar 12 07:48 greetings.txt\n$\nCool! We have greetings.txt.\nMount the same volume to another container\nLetâ€™s spin up a new container with the same volume.\n$ docker run --name bitphile-second -it --mount source=bitphile,target=/bit-vol alpine sh\n/ $ ls\nbin      etc      media    proc     sbin     tmp\nbit-vol  home     mnt      root     srv      usr\ndev      lib      opt      run      sys      var\n/ $\nLetâ€™s see what bit-vol has,\n/ # ls bit-vol\ngreetings.txt\n/ # cat bit-vol/greetings.txt\nGood morning\n/ #\n\nNice.\nConclusion\nFinally, we are at the end (the happiest moment when the zoom meeting ends ðŸ˜‚).\nSo, docker volumes are used as one of the methods to store data permanently. There are some other methods of doing the same. We will have look at those sometime later.\nUntil then,\n\nReferences\n\nDocker Volumes\n"},"blogs/tech/docker/getting-started-with-docker":{"title":"Getting Started With Docker","links":["blogs/tech/docker/docker-image-layers"],"tags":["docker","infra","kubernetes","container","containerisation","containerization"],"content":"Introduction\nDeveloping software and applications is getting more complex day by day. This introduces the complexity of running and deploying the applications in different machines or environments. It also includes making sure it works everywhere no matter what the architecture of the host machine is.\nApplications running in the same environment can conflict with other applications. This can cause malfunctions and make application development less effective.\nTo solve this problem we need isolation and a virtual machine is one way to achieve this. We can have different virtual machines to run the applications that will be independent of other applications environments.\nHowever, using a virtual machine has its own pros and cons. Some of these are the followings:\nVirtual Machine Pros\n\nStronger isolation and security as it provides hardware-level isolation which makes them more secure and isolated.\n\nVirtual Machine Cons\n\nRequires more resources. Virtual machine requires guest system operation to run which consumes more resources.\nStartup time is more in virtual machines as it has to boot the whole operating system.\nPortability is less as virtual machines are tied to specific hardware and require additional configurations to run.\n\nSo, what alternative do we have to achieve this isolated environment? The answer is containerization.\nContainerization\nJust to get started with this concept, I would like to give an example. We have seen containers that are used on ships to transport things. These containers have isolated environments and temperatures specific to the items they are containing.\nSimilarly, containers can contain applications and supporting libraries required to run the application. This helps in running applications in any environment no matter what host system OS and architecture are.\nDocker\nDocker is a containerization tool that is used to create and run containers. There are several other alternatives to docker like Podman, openVZ, etc.\nArchitecture\nDocker uses client-server architecture where the docker client talks with the docker daemon which can run on either a host machine, a virtual machine, or a machine on the cloud. Docker clients can connect to multiple daemons.\n\nSource: docs.docker.com/engine/images/architecture.svg\nDocker client\nDocker client is an interface that lets users create and run containers. Client talks with docker daemon using Docker API requests. It is not more than an application that can be web-based or cli that sends requests to the server and gets responses back.\nThe server or daemon performs the requested job.\nDocker daemon\nA Docker daemon is the server that accepts requests from the docker client and performs the respective job. It can run on a host machine or virtual machine. Docker clients can connect to multiple docker daemons and change the daemon using docker client-provided commands.\nFor example,\ndocker context ls\nIt shows available contexts or daemons to which a client can connect to.\nNAME                DESCRIPTION                               DOCKER ENDPOINT                                     KUBERNETES ENDPOINT   ORCHESTRATOR\ndefault             Current DOCKER_HOST based configuration   tcp://localhost:2376                                                      swarm\ndesktop-linux                                                 unix:///Users/test/.docker/run/docker.sock\nrancher-desktop *   Rancher Desktop moby context              unix:///Users/test/.rd/docker.sock\n\nTo change context, use\ndocker context use rancher-desktop\n\nNote: Rancher desktop is an alternative to Docker desktop you can use. Docker desktop requires a license. Rancher desktop creates a lightweight virtual machine on the host system which contains docker components. Docker daemon runs inside that virtual machine.\n\nDocker Container and Image\nContainer\nAs I mentioned before, a container is a self-contained unit having all the necessary things required for the item container is containing. In this context, a container is a self-contained runnable unit that comprises the application and supporting libraries that are required to run that application.\nImage\nAn image is a blueprint of a container that defines a container. An image contains all the instructions and resources required to create a container.\nIf you have worked on OOPs, you must have heard about classes and objects. You can think of images as classes and objects as containers.\nYou can create your image by using other images as a base. I will show you shortly how to do it.\nRegistry\nThis is the place where you can push your docker images. Itâ€™s like GitHub where you push source code onto GitHub and here you would be pushing docker images. You can also pull docker images stored on the docker registry.\nDocker provides a docker hub register where you can get almost all docker images pushed by other people. Visit hub.docker.com for more details.\nDockerfile\nDockerfile is used to build docker images. It contains all the instructions required to build an image. Following is an example of a Dockerfile.\n\nFROM fedora:latest\nRUN dnf install figlet -y\nENTRYPOINT [&quot;figlet&quot;]\nCMD [&quot;bitPhile&quot;]\nExplanation\n\nFROM specifies to use fedora:latest as the base image. The base image is the parent image on which your image is based. A new image will be built on top of this base image.\nRUN instruction is used to run the given command while building the image.\nENTRYPOINT is the starting script that will run when the container is started.\nCMD specifies the default command to run when the container is started.\n\n\nThere is a difference between ENTRYPOINT and CMD. When ENTRYPOINT is provided, whatever argument CMD instruction has will be append as arguments to ENTRYPOINT. For instance, if we will see the above example, the final command will be figlet hello world.\n\nBuilding Docker Image\nSave the above file and run the following docker command.\ndocker build -t docker-fedora-test .\nbuild is the subcommand that tells to build a docker image. -t is a tag where docker-fedora-test is passed as a tag name. . is the context.\n\nWhat is context? Context the part of the host file system that is required to build a docker image. For example, there may be some file or data which is required in image building. So, we pass those files using context.\nWe can only pass one context.\n\nA docker image is built through several layers of images. Each of the instructions in Dockerfile runs in a separate image layer. To learn more about docker image layers, refer to my blog docker image layers.\nRunning Docker Image\nTo run the image weâ€™ve just built, run the following docker command.\ndocker run docker-fedora-test\nThis gives output as,\n _     _ _   ____  _     _ _\n| |__ (_) |_|  _ \\| |__ (_) | ___\n| &#039;_ \\| | __| |_) | &#039;_ \\| | |/ _ \\\n| |_) | | |_|  __/| | | | | |  __/\n|_.__/|_|\\__|_|   |_| |_|_|_|\\___|\nAwesome, right?\nWe can also pass arguments while we are running the container.\ndocker run docker-fedora-test hello world\nhello world gets append as an argument to fedora entrypoint command.\nFolks, this is it for now. There are lots of things that come in docker and it is hard to put everything in one place both for you and me. So, see you in the next post on docker. Stay tuned.\nPlease provide your feedback as it helps me improve the posts and get more information to you.\nReferences\n\ndocs.docker.com/engine/reference/builder/ for Dockerfile\ndocs.docker.com/get-started/overview/ for overview and architecture\n"},"blogs/tech/programming/scala/chat-app-in-scala-armeria":{"title":"Chat App in Scala Armeria","links":[],"tags":["rest-api","microservices","armeria","websocket","socket"],"content":"Contents\n\nAbstract\nWhat is Armeria and its features?\nProject details\nProject setup requirements\nSetting up Armeria and service handler\nSetting up the web socket handler\nShow example in the browser console\nSocket service for frontend\nCode snippet of the chat screen\nWorking example Screenshots etc\nConclusion\nAcknowledgements\nReferences\n\nAbstract\nAs a part of E4R (Engineering for Research) retreat Hackathon, we explored Armeria and created a small PoC single group chat application using HTTP request polling and websockets. We are using the Armeria framework to develop one of the services in our project. So, it became the inspiration for the exploration.\nThis blog constitutes building a small chat application using REST API framework Armeria, Scala, and JavaScript. It wonâ€™t have end-to-end code but only contains code snippets. You can find code on here.\nWhat is Armeria?\nArmeria is a REST API microservice framework that becomes a backbone of powerful, fast, and asynchronous web services. It is completely asynchronous and built on top of reactive streams and Java 8 CompletableFuture. It is built in Java and compatible with any language that runs on JVM such as Scala, Kotlin, Clojure, etc.\nIt lets you develop services with technologies like gRPC, Thrift, HTTP, etc. running on the same application and port.\nI think this introduction to Armeria would be enough to get a basic understanding. If you are more interested in Armeriaâ€™s features, please have a look at its official website. Now letâ€™s see Armeria in action with the help of a small chat application.\nAbout PoC\nWe are going to build a single group chat application comprising a backend service in Scala using the Armeria framework and a frontend service using React/Javascript.\nPrerequisites\nTo get started with the project, we have to have some required prerequisites.\n\nJava 17 or more installed\nScala installed and sbt\nNode v17 or more\nAnd an IDE of your choice.\n\nSetting up the project\nAssuming all the prerequisites done, create a new project using sbt.\nmkdir chat-app\ncd chat-app\nsbt\nIt setups the project. Now we need to create build.sbt and write the following inside it.\nThisBuild / version := &quot;0.1.0-SNAPSHOT&quot;\n \nThisBuild / scalaVersion := &quot;3.3.0&quot;\n \nlazy val root = (project in file(&quot;.&quot;))\n  .settings(\n    name := &quot;chat-app-retreat&quot;,\n    libraryDependencies ++= Seq(\n      &quot;com.linecorp.armeria&quot;  &quot;upickle&quot; % &quot;3.0.0&quot;\n    )\n  )\nCreate src directory with the following basic structure.\nsrc\nâ”œâ”€â”€ main\nâ”‚Â Â  â”œâ”€â”€ resources\nâ”‚Â Â  â””â”€â”€ scala\nâ””â”€â”€ test\n    â””â”€â”€ scala\n\nNow reload the project using sbt reload.\nCreate a service handler in Main.scala\nCreating a simple server in Armeria is very simple as below code snippet shows. For now, we only have a single handler on path &quot;/&quot; that responds with OK.\nobject Main:  \n  def main(args: Array[String]): Unit =\n    val port: Int      = 8000\n    val server: Server = newServer(port)\n    val future         = server.start()\n    future.join()\n    logger.info(s&quot;Server is running on port $port&quot;)\n \n  private def newServer(port: Int) =\n    val serverBuilder: ServerBuilder = Server.builder()\n \n    serverBuilder\n      .http(port)\n      .service(\n        &quot;/&quot;,\n        (ctx, req) =&gt; HttpResponse.of(HttpStatus.OK)\n      )\n      .requestTimeout(Duration.ofSeconds(1000))\n      .build()\nA handler or a service is attached to the server using service method. We can also have an annotated controller similar to Spring Boot that can be attached using annotatedService method.\nRunning this and sending request to our server, we get 200 OK. Cool, our server works.\nâŸ© curl http://localhost:8000/\n200 OKâŽ\nCreating message endpoints\nWe require to create two endpoints for creating and fetching all the messages.\n\nGET /e4r/message\nPOST /e4r/create-message\n\nSupposing we have a ChatController which handlers messages services. Adding it to server would like as shown below.\nprivate def newServer(port: Int) =\n    val serverBuilder: ServerBuilder = Server.builder()\n \n    serverBuilder\n      .http(port)\n      .service(\n        &quot;/&quot;,\n        (ctx, req) =&gt; HttpResponse.of(HttpStatus.OK)\n      )\n\t  .annotatedService(\n        &quot;/e4r&quot;,\n        ChatController(ChatService(new ChatRepository))\n      )\n      .requestTimeout(Duration.ofSeconds(1000))\n      .build()\nAdding two handlers in ChatController\nclass ChatController(chatService: ChatService):\n  @Get(&quot;/message&quot;)\n  def getMessages: HttpResponse =\n    val result = chatService.getMessages\n    HttpResponse.ofJson(HttpStatus.OK, messages)\n \n  @Post(&quot;/create-message&quot;)\n  @RequestConverter(classOf[JacksonRequestConverterFunction])\n  def createMessage(message: MessageDTO): HttpResponse =\n    chatService\n      .createMessage(message.toMessage)\n      HttpResponse.of(HttpStatus.CREATED)\n \nSending request on /e4r/message returns all the messages in group. /e4r/create-message creates a new message.\nâŸ© curl -X POST --header &quot;content-type: application/json&quot; -d &#039;{&quot;sender&quot;:&quot;@atul&quot;, &quot;text&quot;: &quot;Hey there!&quot;, &quot;createdAt&quot;: &quot;2023-08-03T09:30:37.258Z&quot;}&#039; http://localhost:8000/e4r/create-message\n201 Created\n \nâŸ© curl -q http://localhost:8000/e4r/message | jq\n[\n  {\n    &quot;id&quot;: &quot;d69f836c-eac7-48dc-9138-9d2d40b38f4c&quot;,\n    &quot;sender&quot;: &quot;@atul&quot;,\n    &quot;text&quot;: &quot;Hey there!&quot;,\n    &quot;createdAt&quot;: 1692114770.973873\n  },\n  {\n    &quot;id&quot;: &quot;50ca044a-0f2b-4904-902f-297946c30413&quot;,\n    &quot;sender&quot;: &quot;@atul&quot;,\n    &quot;text&quot;: &quot;Hey there!&quot;,\n    &quot;createdAt&quot;: 1691055037.258\n  }\n]\nImplementing Websocket handler\nTo make the chat experiment real-time, we are required to use web sockets. Web socket is a protocol that creates a full duplex connection among devices. Want to deep dive into web sockets? You can have a read about it over this.\nTalking about Armeria, it has got web socket support in recent versions and thatâ€™s why there is no rich documentation on it. However, we managed to get some implementation of web sockets by going through the code and tests.\nLetâ€™s attach a socket service to our server.\n \nval socketService = new SocketService()\nsocketService.on(&quot;new-message&quot;, (data) =&gt; println(data))\nsocketService.on { (data) =&gt;\n    println(&quot;from broadcast&quot;)\n    println(data)\n}\n    \nprivate def newServer(port: Int) =\n    val serverBuilder: ServerBuilder = Server.builder()\n \n    serverBuilder\n      .http(port)\n      .service(\n        &quot;/&quot;,\n        (ctx, req) =&gt; HttpResponse.of(HttpStatus.OK)\n      )\n      .service(\n        &quot;/chat/:clientId&quot;,\n        WebSocketService.builder(socketService).allowedOrigins(&quot;*&quot;).build()\n      )\n\t  .annotatedService(\n        &quot;/e4r&quot;,\n        ChatController(ChatService(new ChatRepository))\n      )\n      .requestTimeout(Duration.ofSeconds(1000))\n      .build()\nWebSocketService builder takes socketService and builds a websocket handler. Method allowOrigins letâ€™s it accept connection from any origin.\nFollowing code snippet shows SocketService.\nclass SocketService extends WebSocketServiceHandler:\n \n  override def handle(ctx: ServiceRequestContext, in: WebSocket): WebSocket =\n\tval writer: WebSocketWriter = WebSocket.streaming()\n    in.subscribe(new Subscriber[WebSocketFrame]:\n      def onSubscribe(s: Subscription): Unit =\n        s.request(Long.MaxValue)\n \n      def onNext(webSocketFrame: WebSocketFrame): Unit =\n        try\n          val frame = webSocketFrame\n          try\n            frame.`type` match\n              case WebSocketFrameType.TEXT =&gt;\n\t\t\t\t// call handlers\n\t\t\t\tgetAllHanders.foreach((cb) =&gt; cb(frame.text()))\n              case WebSocketFrameType.CLOSE =&gt;\n                val closeFrame = frame.asInstanceOf[CloseWebSocketFrame]\n                writer.close(closeFrame.status, closeFrame.reasonPhrase)\n \n              case _ =&gt;\n              // do nothing\n          catch\n            case jsonException: ujson.ParseException =&gt;\n              println(jsonException)\n            case t: Throwable =&gt;\n              writer.close(t)\n          finally println(&quot;finally&quot;)\n        catch\n          case t: Throwable =&gt;\n            println(t)\n \n      def onError(t: Throwable): Unit =\n        writer.close(t)\n \n      def onComplete(): Unit =\n        println(&quot;CLOSED&quot;)\n        writer.close\n    )\n    writer\n \n  def emit(namespace: String, data: String): Unit =\n    this.socketWriters.values.foreach((writer) =&gt;\n      writer.write(this.createPayload(namespace, data, &quot;namespaced&quot;))\n    )\n \nSocketService extends WebSocketServiceHandler. The important method to implement is handle which triggers every time any client connects with the server.\nArmeria implements Websocket using reactive streams. So, a subscriber is attached to the web socket reactive stream in on every new connection. The writer stream is used to write data to the connected client socket.\nwriter.write(&quot;Hello world&quot;)\nemit is the method that takes a namespace and data. Any handler on the client side listening on this namespace will get that data.\nSo, whenever a new message is created, all the connected clients have to be notified about the message. This can be accomplished using,\n  class ChatController(chatService: ChatService, socket: SocketService):\n  // ...\n  @Post(&quot;/create-message&quot;)\n  @RequestConverter(classOf[JacksonRequestConverterFunction])\n  def createMessage(message: MessageDTO): HttpResponse =\n\tsocket.emit(&quot;new-message&quot;, message.toJson)\n    chatService\n      .createMessage(message.toMessage)\n      HttpResponse.of(HttpStatus.CREATED)\n \nnew-message is namespace and message.toJson is data.\nLetâ€™s try creating a message and see if we get new message on client socket.\n\nSo, the client is connected to the server. Creating a new message triggers a message event on client socket. The below picture shows the message it got from the server.\n\nOur simple chat application (without any UI ðŸ˜‚) is working.\nImplementing Socket on UI\nSimilar to backend, we have written a wrapper on top of WebSocket. This simplistic wrapper manages namespaces and invokes handlers listening on respective namespaces.\ntype fn = { (data: string): void };\n \nclass ChatSocket {\n  handlers: { [key: string]: fn[] };\n  //...\n  connect() {\n    this.socket = new WebSocket(this.url + `/${this.username}`);\n    this.socket.addEventListener(&#039;open&#039;, () =&gt; {\n      console.log(&#039;socket connected&#039;);\n    });\n \n    this.socket.addEventListener(&#039;message&#039;, (event: MessageEvent) =&gt; {\n      const { namespace, data } = JSON.parse(event.data);\n      this.handlers[namespace].forEach((cb) =&gt; cb(data));\n    });\n \n    this.socket.addEventListener(&#039;close&#039;, (event) =&gt;\n      console.log(&#039;Closed&#039;, event)\n    );\n \n    return this;\n  }\n  //...\n}\nTo register a handler, wrapper implements an on method.\nclass ChatSocket {\n  //...\n  on(namespace: string, cb: fn): void {\n    const handlers = this.handlers[namespace];\n \n    if (handlers) {\n      handlers.push(cb);\n      return;\n    }\n \n    this.handlers[namespace] = [cb];\n  }\nUsing Socket wrapper in Chat component\nAs of now, we are only listening on new messages. We are not sending messages using socket. Messages are created using HTTP REST request only.\nFollowing shows usages of socket wrapper in a chat component.\nconst Chats = ({ chatService, username }: ChatAppProps) =&gt; {\n  //...\n \n  useEffect(() =&gt; {\n    //...\n    const chatSocket = new ChatSocket(\n      &#039;ws://localhost:8000/chat&#039;,\n      username\n    ).connect();\n \n    chatSocket.on(&#039;new-message&#039;, (message: Message) =&gt;\n      setMessages((prevMessages: Message[]) =&gt; [...prevMessages, message])\n    );\n  }, []);\n  //...\n};\nuseEffect hook initialises the socket and socket starts listening on new-message namespace. Whenever a new message comes, setMessages updates the messages state and the component updates.\nConclusion\nArmeria is a newly developed framework which is currently in development. It provides several powerful features such as allowing to run different services within a single application and on the same port. It also provides asynchronous execution out of the box. So, it can handle multiple requests without us implementing concurrency and get our hands dirty in threads and concurrency. If you are looking for a lightweight, fast, asynchronous, and ready-to-use REST API framework, it is a good choice to go with.\nHowever, being a new framework, the documentation is undeveloped. We tried to find the implementation of web sockets by going inside the tests theyâ€™ve created.\nAcknowledgements\nWe would like to thank Satya and the Retreat team for organising such an interesting Retreat. Thanks to all the team members (Shubham Chauhan and Atul) who have chosen this trivial problem statement to explore.\nReferences\n\nArmeria Official Docs. URL: armeria.dev/docs/\nArmeria Github Repo. URL: github.com/line/armeria\nTrustin Lee â€” Armeria: A microservice framework well-suited everywhere. Youtube. URL: www.youtube.com/watch%2CJoker%D0%B8JUGru\nArmeria: A microservice framework well-suited everywhere. URL: speakerdeck.com/trustin/armeria-a-microservice-framework-well-suited-everywhere\n"},"index":{"title":"index","links":[],"tags":[],"content":"bitPhile\nA knowledge repository where I add my daily notes on tech I read and blogs I create, and other content."},"notes/book-reviews/design-of-everyday-things":{"title":"design-of-everyday-things","links":[],"tags":["books","review"],"content":"Book - The design of everyday things\nIntroduction\n\nâ€œThe design of everyday thingsâ€ is a book by Don Norman, published in 1988, critiques the design of everyday things and objects. It focuses on the importance of how good design can make people lives easy.\nIt also stresses upon how poor design can create frustrations and confusions and that good design should be intuitive and easy to use.\nAuthor shares several design principles and respective examples in depth that brought from the real world.\n\nAuthor\n\nNow a bit about the author.\nDon Norman is a prominent researcher, writer, and a consultant in the field of human-centered design and user experience.\nHis book â€œThe design of everyday thingsâ€ has become classic in the field of design and read by designers, engineers , students etc.\nHe has written other books like â€œEmotional Design: Why we love or hate everyday thingsâ€ and â€œLiving with complexityâ€.\nBook has : pages\n\nTheme\n\nThe theme of the book is mainly on design principles, user focused design and effects of good and poor design in surroundings.\n\nPrinciples discussed\n\nUnderstanding the needs of the user\naffordances: products should clearly indicate how they can be used and what all is possible\n\ngive some impromptu examples\n\n\nsimplicity\nconsistency: the should be consistency across different products, interfaces and systems. This helps user to understand and navigate easily.\nfeedback\nmapping : relationships between controls and the effects should be clear and intuitive\nConstraints: Products should constrain the actions of users in appropriate ways, making it harder for them to make mistakes\nError recovery: Products should provide clear and straightforward methods for users to recover from mistakes and continue using the product effectively.\n\nExamples\n\nDoors. Push and pull doors\nDoors. Fire exit doors\nLight switches\nComputer interfaces\n\nLot of system related things on the front page\nhave to remember complex commands\nnot giving clear feedback\ngive example of government websites\n\n\nYour example of plane\n\nIt is about British European Airways flight 548\nThere were controls for droops and flaps.\n\ndroops are aerodynamic surfaces that help increase the lift during take off\n\n\nThose were same looking controls and anyone can get confused between them.\nThird pilot by mistake changes the flaps liver instead of droops\nIn the end plane crashes.\nExample of your water bottle accessibility\n\n\n"},"notes/logics/browser-tabs/fol":{"title":"fol","links":[],"tags":["logics","links","fol"],"content":"\nSemantics of predicate logics@Youtube\nFOL truth assignments @Youtube\nSemantics of FOL blog\nSemantics of fol lecture pdf\nFOL syntax and semantics\n\n"},"notes/logics/discussions/kkr-meetings":{"title":"kkr-meetings","links":[],"tags":[],"content":"\nexpert systems\n\n"},"notes/logics/domain-reasoning":{"title":"domain-reasoning","links":["tags/fol"],"tags":["fol"],"content":"fol\nClosed world\n\nIt says what you all know is all there in the world\nIt implies If there is something which is not known, it is false.\n\nOpen world\n\nIt says we have only partial knowledge of the world.\nIt implies that conclusions made now will have to be withdrawn when more facts come to light.\n"},"notes/logics/examples/entailment-using-language-of-logic":{"title":"entailment-using-language-of-logic","links":[],"tags":["reasoning","inferences","semantics","example"],"content":"Below is an example which shows Entailment using language of logic and connectives.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABA -&gt; BTTTTFFFTTFFT\nGiven A -&gt; B is true. Then new table looks like,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABA -&gt; BTTTFTTFFT\nGiven A is true. Then new table is,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABA -&gt; BTTT\nSo, B is necessarily be true.\nWe can say that, A and A -&gt; B entail B.\n\\{(A -&gt; B), A\\} âŠ¨ B\nReference\n\nThe language of Logic\n"},"notes/logics/examples/proving-modus-ponens-resolution":{"title":"proving-modus-ponens-resolution","links":["notes/logics/propositional-logic/clausal-form"],"tags":["reasoning","example","inferences"],"content":"Modus Ponens,\n(p \\land (p \\rightarrow q)) \\rightarrow q\nLetâ€™s tryna prove this using contradiction by checking satisfiability using resolution method.\n\\lnot ((p \\land (p \\rightarrow q)) \\rightarrow q)\nIt can be solved more as shown below.\n(p \\land (p \\rightarrow q)) \\land \\lnot q, using \\lnot (A \\rightarrow B) \\equiv A \\land \\lnot B\nSimplifying it further,\np \\land (\\lnot p \\lor q) \\land \\lnot q\nConverting it into clausal form,\n\n\\displaylines{\n\\{p\\} - (1) \\\\\n\\{\\lnot p, q\\} - (2) \\\\\n\\{\\lnot q\\} - (3)\n}\nUsing (1) and (2),\n\\displaylines{\n\\{q\\} . . . (1), (2) - (3)\\\\\n\\{\\lnot q\\} - (4)\n}\nFurther down,\n\\displaylines{\n\\{\\} ... (3), (4)\n}\nWe get empty set which depicts negation of modus ponens is unsatisfiable. Hence proved that Modus Ponens is a tautology."},"notes/logics/exercises/convert-formula":{"title":"convert-formula","links":[],"tags":["exercises","fol","reasoning","inferences"],"content":"Given\n(A === B)\nExpress it using AND and NOT"},"notes/logics/exercises/find-tautology-of-formula":{"title":"find-tautology-of-formula","links":[],"tags":["reasoning","inferences","exercises"],"content":"Given formula\n(B âˆ§ (A âŠƒ B)) âŠƒ A\nFind if it is tautology using truth table method.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABA âŠƒ BB âˆ§ (A âŠƒ B)(B âˆ§ (A âŠƒ B)) âŠƒ ATTTTTTFFFTFTTTFFFTFT\nAs third rows give a false value, Hence proved that the given formula is not a tautology."},"notes/logics/first-order-logic/first-order-logic":{"title":"first-order-logic","links":[],"tags":["reasoning","inferences","knowledge-representation"],"content":""},"notes/logics/first-order-logic/interpretation":{"title":"interpretation","links":[],"tags":["reasoning","inferences","knowledge-representation"],"content":"Interpretation of the language with the domain assigns semantics to the symbols used in language.\nIn first order logic, the interpretation requires a domain and interpretation changes when domain changes.\nA constant symbol C is interpreted in domain D to an element or an object the domain.\nC \\in D\nA function symbol f of an arity n maps n elements in domain D to an element in D.\nf(E1_D, E2_D,..., En_D) \\in D\nA predicate or relation symbol R of an arity n is interpreted as a tuple of elements in domain D for which the R is true.\n\\langle E1_D, E2_D,..., En_D \\rangle \\subset D\n\n\n                  \n                  Todo\n                  \n                \n\n\n Verify the interpretation of predicate or relation symbol (@2024-06-20)\n\n\n"},"notes/logics/first-order-logic/quantifiers":{"title":"quantifiers","links":[],"tags":["logics","reasoning","inferences"],"content":"Quantifiers\nFOL provides two quantifiers, \\forall and \\exists which stands for for\\ all and exists respectively.\n\\forall\n\\forall x\\ (\\alpha)\nThis implies that for all x, \\alpha formula holds (true or false).\nIn lisp, it can be represented as, (forall\\ (x) (\\alpha))\n\\exists\n\\exists x\\ (\\alpha)\nThis implies that there exists a x for which \\alpha formula holds (true or false).\nIn lisp, it can be represented as, (exists\\ (x) (\\alpha))\nExamples\nAll Men are Mortal.\n\\forall x\\ (Man(x) \\rightarrow Mortal(x))\nIn lisp,\n(forall (x) (if (man x) (mortal x)))\nSome Men are tall.\n\\exists x\\ (Man(x) \\land Tall(x))\n(exists (x) (and (man x) (tall x)))\n\n\n                  \n                  Info\n                  \n                \n\nNotice the connective with \\forall and \\exists.\n\n\n\n\n                  \n                  Todo\n                  \n                \n\n\n Find why different connective for universal and existential quantifier (@2024-06-20)\n\n\n\nWhy different connectives for universal and existential quantifier?\nWe use \\rightarrow with \\forall and \\land with \\exists. Why?\nNot enough clear about why? As of now just with the thumb rule. I need to figure out why or Iâ€™m not satisfied or didnâ€™t understand it.\nSee these conversations\n\nWhy canâ€™t we use implication for the existential quantifier?\nâ€œWhy canâ€™t we use implication for the existential quantifier?â€ (From MSE)\n\n"},"notes/logics/first-order-logic/rules-of-inference":{"title":"rules-of-inference","links":[],"tags":["inferences","reasoning","knowledge-representation"],"content":"Rules Of Inference\nPropositional rules are still applicable in FOL. In addition there are two more rules in FOL.\nUniversal Instantiation\n\\forall x \\ P(x) \\over P(a), a \\in C\nIt says that if for all x on P(x) is true then P(a) also holds true. That is, if for all values for x, if the formula turns out to be true, then formula for a in P(a) will also holds true.\nFor instance,\n\\displaylines {\n    \\forall x \\ (Man(x) \\rightarrow Mortal(x)) \\over Man(Socrats) \\rightarrow Mortal(Socrats)\n}\nGeneralisation\nP(a) \\over\n\\exists x \\ P(x)\nIt says that if, P(a) is true, then some of x for P(x) also holds true.\nFor instance,\n\\displaylines {\n    Student(Hemant) \\land Bully(Hemant) \\over\n    \\exists x \\ Student(x) \\land Bully(x)\n}"},"notes/logics/first-order-logic/rules-of-substitution":{"title":"rules-of-substitution","links":[],"tags":["reasoning","inferences","knowledge-representation"],"content":"Similar to PL, FOL also have rules of substitution.\n\\displaylines {\n    \\lnot (\\forall x \\ P(x)) \\equiv \\exists x \\ \\lnot(P(x)) \\ \\ldots \\ DeMorgan&#039;s \\ Law \\\\\n    \\lnot (\\exists x \\ P(x)) \\equiv \\forall x \\ \\lnot (P(x)) \\ \\ldots \\ DeMorgan&#039;s \\ Law \\\\\n    \\forall x \\forall y \\equiv \\forall y \\forall x \\\\\n    \\exists x \\exists y \\equiv \\exists y \\exists x \\\\\n    \n}\n\n\n                  \n                  Info\n                  \n                \n\n\\exists x \\forall y \\equiv \\forall y \\exists x is not valid.\n\n"},"notes/logics/first-order-logic/syntax":{"title":"syntax","links":[],"tags":["reasoning","syntax","inferences","knowledge-representation"],"content":"Syntax\nFirst order syntax comprises followings\n\nConstants symbols. Such as Bob, Alice, Mathematics etc.\nVariables symbols.\nFunctions symbols. Functions interpreted as Symbols taking input objects and returning output objects. They have arity (number of arguments). For example, fatherOf(Bob), etc. Function symbols follow snake case.\nPredicate Symbols. Similar to functions but can be thought as specifying the property of given input. It defines relationships between/among objects. For example, Rich(Bob) defines Bob\\ is\\ Rich. Owns(Bob, Car) etc. Predicate symbols follow CamelCase.\nQuantifiers. Universal Quantifier  \\forall and existential quantifier \\exists.\nLogical connectives and negation.\n\nUsing these six components, FOL has\n\nTerms\n\nConstants such as mathemetics, arithmetic, drawing etc.\nVariables such as x.\nFunctions of terms. Functions taking terms.  P(T_1, T_2, ...,T_n) where T_i are terms and n is the arity of the function.\n\n\nFormulas\n\nAtomic formulas. P(T_1, T_2, ...,T_n) where T_i are terms and n is the arity of the formula.\nQuantifiers, \\forall and \\exists.\n\n\n\nReferences\n\nFirst Order Logic Syntax and Semantics\n"},"notes/logics/formal-language":{"title":"formal-language","links":[],"tags":["knowledge-representation","reasoning","artificial-intelligence"],"content":""},"notes/logics/hierarchies-of-structured-knowledge":{"title":"hierarchies-of-structured-knowledge","links":["tags/fol","tags/references","tags/reasoning"],"tags":["fol","references","reasoning"],"content":"folreferencesreasoning\nTypes of Hierarchies\n\nAbstraction\n\nis-a\n\n\nAggregation\n\nis-part-of\n\n\n"},"notes/logics/inference-engine/backward-chaining":{"title":"backward-chaining","links":[],"tags":["inferences","reasoning","methods"],"content":"Taking a goal and proofing it by applying inference rules on KB."},"notes/logics/introduction":{"title":"introduction","links":["tags/fol","tags/inferences","tags/reasoning","tags/artificial-intelligence","tags/ai"],"tags":["fol","inferences","reasoning","artificial-intelligence","ai"],"content":"folinferencesreasoningartificial-intelligenceai\nThree types of inferences\n\nDeduction\n\nTaking all the facts and the giving the statement that necessarily be true\nMaking logical inferences\n\n\nAbduction\n\nTaking the final result going back to find the causes which may not be true.\nthese are mostly guesses.\n\n\n\nInduction\n\nGeneralisation of facts.\n\n\n\nEpistemic Logic\nIt deals with the logic of multi-agent systems. It allows us to represent statement of what agents know about other agents."},"notes/logics/left-sessions":{"title":"left-sessions","links":[],"tags":["sessions"],"content":"\n\n\n"},"notes/logics/logic-and-representation":{"title":"logic-and-representation","links":["tags/fol"],"tags":["fol"],"content":"fol\n\nPropositional logic\nFirst order logic\nDescription logic\nDefault reasoning\nEvent calculus\nEpistemic reasoning\n\n"},"notes/logics/logic/formal-logic":{"title":"formal-logic","links":["notes/logics/formal-language"],"tags":["artificial-intelligence","reasoning","knowledge-representation"],"content":"Formal logic uses formal system to represent logic where concrete expressions are replaced with symbols. It is only concerned with the abstract structure of the argument instead of the concrete content.\nFor example,\nAll Sanitisers have Alcohol.\nRoss is a Sanitiser.\nRoss has Alcohol.\nAll S have A.\nR is a S.\nR has A.\nFormal logic uses formal-language to express and analyse arguments.\nReferences\n\nFormal Logic Wikipedia\n"},"notes/logics/logic/logic":{"title":"logic","links":["notes/logics/logic/formal-logic"],"tags":["artificial-intelligence","reasoning","knowledge-representation"],"content":"A logic is used to asses the correctness of an argument. An argument is a set of premises together with a conclusion. For example,\nTeachers are good people.\nRamesh is a teacher.\nThen Ramesh is good person.\n\nThere are two types of logic.\n\nFormal logic\nInformal logic\n"},"notes/logics/new-note":{"title":"new-note","links":[],"tags":["reasoning","inferences","knowledge-representation"],"content":""},"notes/logics/potential-ideas":{"title":"potential-ideas","links":[],"tags":["reasoning","inferences","knowledge-management","ideas"],"content":"\nCombining inferences/reasoning with deep learning methods?\n"},"notes/logics/propositional-logic/ask":{"title":"ask","links":["notes/logics/terminology/entailment","notes/logics/propositional-logic/contradiction","notes/logics/propositional-logic/contingency"],"tags":["inferences","reasoning"],"content":"Asking is the queries the knowledge base.\nAgain three possible responses.\n\nYes. Entailment.\nNo. Contradiction.\nI donâ€™t know. Contingency\n"},"notes/logics/propositional-logic/clausal-form":{"title":"clausal-form","links":[],"tags":["inferences","reasoning"],"content":"Clausal form consists of clause sets.\nClause\nA clause is a set of clause expressions.\nFor example, \\{p\\}, \\{q\\}, ...\nHere, \\{p\\} is a clause.\nClause expression\nA clause expression can be a literal or disjunction of literals.\n\\displaylines {\n    p \\\\\n    q \\\\\n    p \\lor q\n}\nRepresenting these expressions as clauses,\n\\displaylines {\n    \\{p\\} \\\\\n    \\{q\\} \\\\\n    \\{p, q\\} \\\\\n}\nReferences\n\nPropositional Resolution\n"},"notes/logics/propositional-logic/conditional-proofs":{"title":"conditional-proofs","links":["notes/logics/propositional-logic/conditional-wff","notes/logics/terminology/antecedent-consequent"],"tags":["reasoning","inferences","knowledge-management"],"content":"A conditional proof is used to derive a conditional wff. A conditional proof involves that if the consequent of conditional wff can be derived from the given premises and derivations by assuming Antecedent to be true.\nFor example,\n\\begin{array}{l} P \\rightarrow (Q \\lor R)\\\\ P \\rightarrow \\neg S\\\\ S \\leftrightarrow Q\\\\ \\hline P \\rightarrow R \\end{array}\nIf we can derive R by assuming P to be true and using rules of inferences and replacement rules on those given premises, then we can conclude that P \\rightarrow R.\nReferences\n\nConditional Proofs\n"},"notes/logics/propositional-logic/conditional-wff":{"title":"conditional-wff","links":[],"tags":["reasoning","inferences","knowledge-management"],"content":"A conditional wff is a formula whose main operator is \\rightarrow.\nFor example,\n A \\rightarrow B"},"notes/logics/propositional-logic/contingency":{"title":"contingency","links":[],"tags":["inferences","reasoning"],"content":"Contingency when a new information is added to the knowledge base which was not previously known.\nIn logical terms,\n\\emptyset \\subseteq M(f) \\subsetneq M(KB)"},"notes/logics/propositional-logic/contradiction":{"title":"contradiction","links":[],"tags":["inferences","reasoning"],"content":"KB contradicts f iff M(KB) \\cap M(f) = \\emptyset.\nFor example, Rain \\land Wet contradicts \\lnot Wet."},"notes/logics/propositional-logic/deduction-theorem":{"title":"deduction-theorem","links":[],"tags":["reasoning","inferences"],"content":"This theorem says that,\n\n\\Gamma, A \\vdash B \\ \\ \\ \\  \\ iff \\ \\ \\ \\ \\ \\Gamma \\vdash A \\rightarrow B\nThis theorem is also called as conditional proof. (Still not verified)."},"notes/logics/propositional-logic/definite-clauses":{"title":"definite-clauses","links":[],"tags":["reasoning","inferences"],"content":"Definite Clause have this form,\n(P_1 \\land P_2 \\land P_3, ...p_k) \\rightarrow q"},"notes/logics/propositional-logic/direct-deductions":{"title":"direct-deductions","links":[],"tags":["reasoning","inferences","knowledge-management"],"content":"Direct deduction is a form of reasoning where reaching to the conclusion comprises applying the inference rules on given premises and deriving new rules and in turn applying inference rules on them until the conclusion is derived.\n\n\n                  \n                  Not Sure \n                  \n                \n\nIf natural deduction is a type of direct deduction.\n\n"},"notes/logics/propositional-logic/forward-reasoning":{"title":"forward-reasoning","links":[],"tags":["reasoning","inferences"],"content":"Forward Reasoning is the repetitive process of finding new facts using rules of inference, adding them to KB and continue this process until we reach the final goal.\nFor example,\nReferences\n\nForward reasoning at nptel youtube video\n"},"notes/logics/propositional-logic/frege-axioms":{"title":"frege-axioms","links":["notes/logics/propositional-logic/propositional-logic","notes/logics/terminology/tautology"],"tags":["inferences","reasoning","logic"],"content":"Gottlob Frege was the first person who axiomatise Propositional Calculus by giving Tautologies.\nReferences\n\nFregeâ€™s Propositional Calculus Wikipedia\n"},"notes/logics/propositional-logic/horn-clauses":{"title":"horn-clauses","links":[],"tags":["inferences","reasoning"],"content":""},"notes/logics/propositional-logic/interpretation-function":{"title":"interpretation-function","links":[],"tags":["reasoning","inferences"],"content":"An interpretation function is a something which takes a formula (f), a model (w) and evaluates the formula with the given models which results in either of the two values true or false.\nI(f, w)"},"notes/logics/propositional-logic/limitations":{"title":"limitations","links":[],"tags":["reasoning","inferences"],"content":"Limitations\n\nIt only consists of propositional and formulas. It doesnâ€™t have relationships among objects.\nNot able to express complex sentences and relations.\n\n1. \\ Alice\\ and\\ Bob\\ know\\ drawing\nHow can it be represented in PL?\nSomething like this?\n\\displaylines{\n    AliceKnowsDrawing \\land BobKnowsDrawing\n}\n2. \\ All\\ students\\ know\\ drawing\nHow can it be represented in propositional logic?\n\\displaylines {\n    AIsStudent \\rightarrow AKnowsDrawing \\\\\n    BIsStudent \\rightarrow BKnowsDrawing \\\\\n    CIsStudent \\rightarrow CKnowsDrawing \\\\\n    ...\n    \n}\n\nNo notion for categories and individuals.\nIt is missing objects and relations or predicates. For example, AliceKnowsDrawing can be decomposed into Alice, \\ knows, \\ drawing where Alice and drawing are two objects connected with a predicate knows.\n\nReferences\n\nLimitations of propositional logic\nFirst-Order Logic: Syntax and Semantics web.engr.oregonstate.edu/~afern/GOFAI/notes/first-order-logic-syntax-semantics.pdf\n\n"},"notes/logics/propositional-logic/model-checking":{"title":"model-checking","links":["notes/logics/examples/entailment-using-language-of-logic"],"tags":["inferences","reasoning"],"content":"A formula has a valuation. Valuation means it may have value true or false. This is also called as interpretation of the formula.\nIf the interpretation results in true value, then we can say that the interpretation is the model of the formula (Not sure about this statement. Some resources say this and some tell some other).\nFor example, letâ€™s take this formula and try to find its model.\n\\lnot (A \\rightarrow B)\nWe have to find valuation that results in true value for the formula.\n(A \\rightarrow B) has to be false which means, A has to be true and B has to be false. So, the whole formula turns into false. Refer the truth table for implies.\n\\lnot (A \\rightarrow B) will becomes true. We found the model for this formula.\n\nIf \\lnot (A \\rightarrow B) has no model, then we can say that (A \\rightarrow B) is tautology.\n\nReferences\n\nPropositional logic review\n"},"notes/logics/propositional-logic/model":{"title":"model","links":[],"tags":["reasoning","inferences"],"content":"A model is a putting true and false values for variables in a formula (f). It is denoted using w.\nf = Rain \\lor Wet\nPossible ws are \n1. \\hfill &amp; w \\hfill &amp; \\{Rain: true, Wet: True\\} \\hfill \\cr \n2. \\hfill &amp; w \\hfill &amp; \\{Rain: false, Wet: True\\} \\hfill \\cr \n3. \\hfill &amp; w \\hfill &amp; \\{Rain: True, Wet: False\\} \\hfill \\cr \n4. \\hfill &amp; w \\hfill &amp; \\{Rain: False, Wet: False\\} \\hfill \\cr }$$"},"notes/logics/propositional-logic/models":{"title":"models","links":["notes/logics/propositional-logic/model"],"tags":["reasoning","inferences"],"content":"Models is a set of all the models for which the interpretation function gives true for formulas. It is represented as,\nf = Rain \\lor Wet\nM(f) = set\\ of\\ \\bf w\\ \\it which\\ gives\\ \\bf true \nSo, M(f) will be models 1 to 3 from ws as shown in Model.\nM(KB)\nM(KB) is an intersection of all the formulas in the KB.\nM(KB) = \\bigcap \\limits_{f\\in KB} M(f) \nFor example,\nKB = \\{Rain, Rain \\rightarrow Wet\\}\nThen M(KB) will be,\nM(Rain) \\cap M(Rain \\rightarrow Wet)\nM(Rain) = \\{Rain: True\\}\nM(Rain \\rightarrow Wet) = {(Rain: true, Wet: true), (Rain: false, Wet: true), (Rain: false, Wet: false)}\n\nSo, the intersection will be.\n\n$M(KB) = \\{Rain: true, Wet: true\\}$"},"notes/logics/propositional-logic/proof-by-contradiction":{"title":"proof-by-contradiction","links":[],"tags":["reasoning","inferences"],"content":"It is an indirect way of proofing where we assume that the statement is false and try to proof that. If we are not able to proof that the statement is false, then the statement is essentially true."},"notes/logics/propositional-logic/propositional-logic-syntax":{"title":"propositional-logic-syntax","links":["notes/logics/propositional-logic/frege-axioms"],"tags":["reasoning","inferences","syntax"],"content":"Propositional Logic Syntax consists of following parts.\n\nPropositional statements represented as Upper case letters. For instance, P, Q, R, ..., P_1, P_2, ...\n\nAll the statements are represented using these letters.\n\n\nLogical connectives\n\n\\land, \\lor, \\leftrightarrow, \\rightarrow and \\lnot.\n\n\nInference rules\nAxioms\n\nReferences\n\nSyntax and Formation rules of PL\n"},"notes/logics/propositional-logic/propositional-logic":{"title":"propositional-logic","links":["notes/logics/terminology/connectives"],"tags":["reasoning","inferences","logic"],"content":"Propositional logic is a formal system that deals with manipulation and evaluation of propositions or statements based on their truth or falsity. It doesnâ€™t concern about the meaning of the statements for the logic.\nIt consists a set of propositional variables, logical connectives, inference rules and axioms.\nReferences\n\nPropositional Calculus Wikipedia\nPropositional Logic at Internet Encyclopedia of Philosophy\n"},"notes/logics/propositional-logic/rules-of-inference/Modus-tollens":{"title":"Modus-tollens","links":[],"tags":["inferences","reasoning"],"content":"This says, if you know ~Q and P -&gt; Q then you can say ~P.\nA simple example of modus-tollens is\n\\matrix{ 1. \\hfill &amp; \\lnot Q \\hfill &amp; \\hbox{Premise} \\hfill \\cr 2. \\hfill &amp; P âŠƒ Q \\hfill &amp; \\hbox{Premise} \\hfill \\cr 3. \\hfill &amp; \\lnot P \\hfill &amp; \\hbox{Modus tollens (1, 2)} \\hfill \\cr}\nAn invalid modus-tollens\n\\matrix{ 1. \\hfill &amp; Q \\hfill &amp; \\hbox{Premise} \\hfill \\cr 2. \\hfill &amp; P âŠƒ Q \\hfill &amp; \\hbox{Premise} \\hfill \\cr 3. \\hfill &amp; \\lnot P \\hfill &amp; \\hbox{INVALID Modus tollens (1, 2)} \\hfill \\cr}\nFor example,\nYou water the garden, the garden get wet -&gt; 1\nthe garden is not wet -&gt; 2\nYou not watered the garden -&gt; Modus tollens (1, 2)\n\nLetâ€™s verify the other way round,\n\\matrix{ 1. \\hfill &amp; \\lnot P \\hfill &amp; \\hbox{Premise} \\hfill \\cr 2. \\hfill &amp; P âŠƒ Q \\hfill &amp; \\hbox{Premise} \\hfill \\cr 3. \\hfill &amp; \\lnot Q \\hfill &amp; \\hbox{INVALID Modus tollens (1, 2)} \\hfill \\cr}\nFor example,\nYou water the garden, the garden get wet -&gt; 1\nYou not watered the garden -&gt; 2\nThe garden is not wet -&gt; INVALID Inference as the garden can be wet if it rains.\n\nReferences\n\nRules of inferences and logic proofs\nLogic rules\n"},"notes/logics/propositional-logic/rules-of-inference/abduction":{"title":"abduction","links":[],"tags":["reasoning","inferences"],"content":"Representing Abduction using formal language,\nA âŠƒ B\\ and\\ B\\ \\over A\nThe conclusion is not necessarily true. It is possible that A is true or false.\nFor instance,\nIf you water the garden (A), then the lawn becomes wet (B)\nthe lawn is wet (B)\nTherefore, you watered the garden (A)\n\nIt is not always true. As rain can also wet the garden."},"notes/logics/propositional-logic/rules-of-inference/modus-ponens":{"title":"modus-ponens","links":[],"tags":["inferences","reasoning"],"content":"Modus Ponens Denoted as\nA âŠƒ B\\ and\\ A\\ \\over B\nIt means, If A implies B and A is true then B will also be true. This is Deduction.\nFor example,\nIf you water the garden (A), then the lawn becomes wet (B)\nyou water the garden (A)\nTherefore, lawn become wet (B)\n"},"notes/logics/propositional-logic/rules-of-inference/resolution-method":{"title":"resolution-method","links":[],"tags":["inferences","reasoning"],"content":"Resolution is a theorem prover method which is sound and complete. It works by converting the premises and conclusions to clause expressions or clausal form.\nWhat resolution method says is,\n\\{\\lnot p, q\\}, \\{p, r\\}\n\\over\n\\{q, r\\}\nIt means, if two clauses having a literal and its negate, then we can combine all those clauses by removing those complementary pair.\n\\{p_1, p_2,...,p_n,\\lnot q\\}, \\{r_1, r_2,...,r_n, q\\}\n\\over\n\\{p_1, p_2,...,p_n, r_1, r_2,...,r_n\\}\n\n\n                  \n                  Important\n                  \n                \n\nEmpty clause means that the set of clauses are unsatisfiable. This property will be used one the applications (proving the unsatisfiability).\n\n\nCommon Resolutions\n1. Modus Ponens\n    p \\rightarrow q\n    ,\\ p\n    \\over\n    q\nThis is can be done using resolution method.\n\\{\\lnot p, q\\}, \\{p\\} \\over \\{q\\}\nReference\n\nPropositional Resolution\n"},"notes/logics/propositional-logic/rules-of-inference/substitution":{"title":"substitution","links":[],"tags":["inferences","reasoning"],"content":""},"notes/logics/propositional-logic/satisfiability":{"title":"satisfiability","links":[],"tags":["inferences","reasoning"],"content":"A KB is satisfiable iff M(KB) \\ne \\emptyset\nA compound formula is a satisfiable when there is at least one model which evaluates formula to true."},"notes/logics/propositional-logic/tableau-method":{"title":"tableau-method","links":["notes/logics/terminology/tautology","notes/logics/propositional-logic/proof-by-contradiction","notes/logics/propositional-logic/rules-of-inference/modus-ponens"],"tags":["reasoning","inferences"],"content":"Tableau Method\nTableau method is a method to proof satisfiability of a set of formulas.\nIt is also used to proof a formula to be tautology using proof by contraction.\nBasic rules\n\nWe have to break the complex formula until it gets to single atomic formulas.\nLogical \\lor creates branches (thatâ€™s obvious as it makes two sets out of single set).\nEach of the branch should have every formula expanded at least once.\nIf there is a contraction of any atomic formula in the branch, then that branch is closed.\nIf all the branches are closed then set of formulas is not satisfiable.\nIf there is any one branch opened, set is satisfiable.\n\n\n\n                  \n                  Info\n                  \n                \n\nClosed: When there is any contraction exists for any formula in a branch.\nOpened: Opposite of closed.\n\n\nLetâ€™s see an example for better understanding.\nset = \\{(a \\lor b) \\land c, \\lnot b \\lor \\lnot c, \\lnot a\\}\n\nProof Tautologies\nA tautology can be proofed by taking the negate of that and try to proof satisfiability for that negate using tableau method. If that negate is unsatisfiable, then the tautology is proofed.\nexample, letâ€™s proof modus ponens\n(A \\land (A \\rightarrow B)) \\rightarrow B\nTaking negation of this,\n\\lnot((A \\land (A \\rightarrow B)) \\rightarrow B)\n   ~((A âˆ§ (A -&gt; B)) -&gt; B)\n           |\n     (A âˆ§ (A -&gt; B))\n           |\n          ~B\n           |\n           A\n           |\n        (A -&gt; B)\n        /      \\\n       /        \\\n     ~A         B\n    CLOSED    CLOSED\n\nSo, both branches are closed and there is no model for the negation. So, we can conclude that Modus Ponens is a tautology.\nReferences\n\nPropositional tableaux - DIAG\n"},"notes/logics/propositional-logic/tell":{"title":"tell","links":["notes/logics/terminology/entailment","notes/logics/propositional-logic/contradiction","notes/logics/propositional-logic/contingency"],"tags":["inferences","reasoning"],"content":"A tell operation is feeding knowledge to the KB.\nTell[f]\\rightarrow KB  \\rightarrow ?\nPossible respones:\n\nAlready know that. This is entailment. KB \\vDash f\nDonâ€™t believe that. This is contradiction. KB \\vDash \\lnot f.\nNew knowledge. This is contingency. f \\rightarrow KB\n"},"notes/logics/questions":{"title":"questions","links":[],"tags":["query","questions"],"content":"\nWhat actually a model is?\n\nA model is the true or false value for variables?\n\nTold by this video\n\n\nOr a set of values of variables which that evaluates the formula true?\n\nTold by this video \n\n\n\n\nHow to fix completeness in Propositional logic?\n\nRestricting propositional logic using clauses?\nUsing resolution?\n\n\nIs natural deduction and direct deduction same?\nWhat is deduction theorem?\n\nis it just assuming A to derive B and the concluding this A \\rightarrow B ?\n\n\n\n"},"notes/logics/syllogism":{"title":"syllogism","links":["notes/logics/logic/formal-logic"],"tags":["artificial-intelligence","knowledge-representation","reasoning"],"content":"It is a form of logical reasoning with two premises which considered to be true to arrive at the conclusion. It consists of three sentences.\nAll forts are historic.\nSinhagad is a fort.\nTherefore, Sinhagad is historic.\nIt was used in antiquity (Ancient times). Now it is replaced with formal-logic.\nReferences\n\nSyllogism\n"},"notes/logics/symbols-and-thought/reasoning":{"title":"reasoning","links":[],"tags":["ai","fol","reasoning","flash-card"],"content":"\nThe manipulation of symbol in a meaningful manner.\nReasoning is the activity of drawing inferences.\n\n"},"notes/logics/symbols-and-thought/representation":{"title":"representation","links":[],"tags":["fol","ai","reasoning"],"content":"Semiotics\nA branch of study of symbols.\nBiosemiotics\nA branch of study about the interaction of simple systems to give complex behaviours.\nPhysical symbol system\nA symbol system is a collection of systems. Such as alphabets etc.\nA physical symbol systems follows the rules or laws such as addition, division etc.\nSymbolic AI\nThe ability of manipulating symbols."},"notes/logics/symbols":{"title":"symbols","links":[],"tags":["symbols","connectives"],"content":"\nAND : âˆ§\nOR: âˆ¨\nImplies: âŠƒ\nNAND: â†‘\nNOR: â†“\n\n"},"notes/logics/taxonomies":{"title":"taxonomies","links":["tags/fol","tags/reasoning","tags/knowledge-representation"],"tags":["fol","reasoning","knowledge-representation"],"content":"folreasoningknowledge-representation\nTaxonomy is the classification data into categories and sub-categories.\nFor example, animals have two eyes. So, Cat inherits animal. So, Cat has two eyes."},"notes/logics/terminology/antecedent-consequent":{"title":"antecedent-consequent","links":[],"tags":["reasoning","inferences"],"content":"Given the Proposition below,\nA \\to B\nA is called antecedent and B is called consequent."},"notes/logics/terminology/connectives":{"title":"connectives","links":[],"tags":["artificial-intelligence","knowledge-representation","reasoning"],"content":"Connectives are symbols used to conjunct two or more logical sentences. There are different logical connectives, 1 Unary (~) , 16 binary connectives etc.\nLogical connectives are defined using truth table. For example, below shows the truth table for implies (-&gt;) connective.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\negABA -&gt; B1TTT2TFF3FTT4FFT\nSheffer Stroke or NAND is a connective. It is represented using â†‘\nPeirce Arrow or NOR is a connective. It is represented using â†“."},"notes/logics/terminology/dag":{"title":"dag","links":[],"tags":["graphs"],"content":""},"notes/logics/terminology/entailment":{"title":"entailment","links":[],"tags":["reasoning","knowledge-representation","artificial-intelligence","flash-card"],"content":"Entailment means that a sentence is true belonging to a true Knowledge base. It is said that the sentence entailed from the knowledge base.\nIt is denoted as where KB is knowledge base which is true and Î± is a sentence.\nKB âŠ¨ Î±\n\nSo, above statement says that KB entails Î±.\nWhen a sentence is entailed from a true knowledge base, then the sentence is said to be true.\nIn more logical terms, an formula f is entailed from the KB then the following is true.\nM(KB) \\subseteq M(f)\nEntailed f is not adding any new information to the KB. For example,\nKB = \\{Rain \\land Wet\\}\nf = Rain\nthen,\nM(KB) = \\{(Rain: true, Wet: true)\\}\nM(f) = \\{Rain: true\\}\nSo, M(KB) \\subseteq M(f)\nWe can say that KB \\vDash f."},"notes/logics/terminology/premise":{"title":"premise","links":["notes/logics/terminology/proposition"],"tags":["artificial-intelligence","inferences","reasoning","terminology","flash-card","knowledge-representation"],"content":"A premise is a proposition which is used in an argument to prove the truth of another proposition called conclusion."},"notes/logics/terminology/proof":{"title":"proof","links":[],"tags":["inferences","reasoning","artificial-intelligence","knowledge-representation","logic"],"content":"A logic machine can add new sentences to a knowledge base using the rules of inferences. A logic machine can have the algorithm to do so.\nWhen a sentence Î± is added to the knowledge base by a logic machine using the rules of inferences then we can say that,\nÎ± is provable.\nKB â”œâ”€ Î±\n\nA proof system is defined by two things: premises (or axioms) and rules of inference. To proof a sentence truthiness, we use those premises and rules to synthesis new sentences and in the end the sentence we want to prove.\nThere are two types of proofs:\n\nDirect proofs. It goes from given premises to proof the conclusion.\nIndirect proofs\n\nReferences\n\nWhat are soundness and completeness?\nRules of inferences\n"},"notes/logics/terminology/proposition":{"title":"proposition","links":[],"tags":["artificial-intelligence","knowledge-representation","reasoning"],"content":"Proposition is a statement or assertion that expresses a judgement or opinion. (from Google).\nFor example, â€œGlass has milkâ€ is a proposition which can be true or false.\nReferences\n\nWhat is Proposition? @nptelherd Youtube\n"},"notes/logics/terminology/soundness-and-completeness":{"title":"soundness-and-completeness","links":[],"tags":["inferences","reasoning","semantics"],"content":"Soundness\n\nWhat all can be derived is entailed.\n\n    KB \\vdash f \\subseteq KB \\vDash f\n\n\nThis statement states that what all derived is has superset of what all is entailed.\n\n\nSoundness of an reasoning algorithm means that it proves only true statements which are totally valid. These statements are guaranteed to be true. Deductive reasoning algorithms are supposed to sound in nature.\n\n\nSoundness means that if a sentence is provable, then it must be true with all its interpretations. In other words, it should also be entailed.\n\n\nA proof system isÂ soundÂ if everything that is provable is actually true.\n\n\nCompleteness\n\nWhat all be derived is more that what all can be entailed.\n\n    KB \\vdash f \\supseteq KB \\vDash f\n\n\nThis states that what all can be entailed is a part of what all can be derived.\n\n\nCompleteness means that it proves all the true statements. It ensures that it can find a valid solution for any valid input.\n\n\nA proof system isÂ completeÂ if everything that is true can be proved\n\n\nIt is required for any algorithm to be sound and complete.\nReferences\n\nlogic - what does soundness of a semantic reasoner mean? @stakeoverflow\ntextbooks.cs.ksu.edu/cis301/4-chapter/4_9-soundcomplete/index.html\n"},"notes/logics/terminology/tautology":{"title":"tautology","links":["notes/logics/propositional-logic/rules-of-inference/modus-ponens"],"tags":["inferences","reasoning","logic"],"content":"A statement which is always true. For example, Modus Ponens."},"notes/operating-systems/architectures":{"title":"architectures","links":["tags/os","tags/operating-system","tags/architecture"],"tags":["os","operating-system","architecture"],"content":"tags:osoperating-systemarchitecture\nis_child_of: [operating-systems]]\nOperating system architectures\n\nMonolithic\nLayered\n\nMonolithic\n\nIt is a single executable program comprising all the operating system functionalities.\nIt contains of different procedures linked togther to build a single executable that runs in kernel mode. If we see, it is a big kernel handles everything of an operating system.\nUser programs directly run on top of it. It becomes direct to access system resources as there is no layer.\nFor example, Linux is a monolithic operating system.\nHowever, some of modern monolithic operating systems have modular code which makes it quite simple to design the system. Moreover, these modules can be loaded at run time as not to make the boot process heavy.\nLayered\n\nOperating system responsibilities are divided among different layers solely responsible for their specific set of tasks.\nFor instance, layer for process management, IO, file management etc.\nMicrokernel\n\nIt basic idea about this design is that to put very less in kernel to make system reliable and robust.\nThe operating system is divided in small modules such that the very less is given to the kernel and most of the activities happen in user mode.\nA kernel is reponsible for the mechanism that is, it is reponsible for doing the work (lower level work) and the policies or rules of how to do things will be decided in user mode.\nSo mechanism and policy are decoupled. Mechanism will be taken care by kernel and policies will reside in user mode.\nFor instance, priority is the basic process scheduling algorithm. The priorities can be assigned in the user mode and the kernel can run process by looking their priorities."},"notes/operating-systems/file-system/i-node":{"title":"i-node","links":["tags/files","tags/file-system","tags/directories","archive/operating-systems/file-system/introduction"],"tags":["files","file-system","directories"],"content":"tags:filesfile-systemfilesdirectories\nis_child_of: introduction\nWhat is an i-node?\nAn i-node is a data structure which is used by file system to store metadata about files.\nIt contains information like name, permissions, file type, disk blocks location and so on.\nFile system maintains a table in the hard disk to store all these i-nodes.\nA directory contains files. These are not the actual files for sure. These are the structure having information about the file. It is a node of 16 bytes out of which, 2 bytes for i-node number and 14 bytes for file name.\nWhenever a file or a directory has to be located, it gets the file name and finds the i-node number and then gets the file metadata and the locates the file from the disk."},"notes/operating-systems/file-system/introduction":{"title":"introduction","links":["tags/os","tags/operating-system","tags/file-system","tags/files","tags/directories","tags/mounts","notes/operating-systems/operating-systems"],"tags":["os","operating-system","file-system","files","directories","mounts"],"content":"tags:osoperating-systemfile-systemfilesdirectoriesmounts\nis_child_of: operating-systems"},"notes/operating-systems/history":{"title":"history","links":["tags/os","tags/operating-system","notes/operating-systems/operating-systems"],"tags":["os","operating-system"],"content":"tags:osoperating-system\nis_child_of: operating-systems\nHistory of Operating System\nGeneration 1945-55 of Vaccum Tubes\n\nVaccum tubes were used in computers that were very slow.\n\nGeneration 1955-65 of Transistors and Batch systems\n\nTransisters were used in computers.\nTasks were carried out batch sytems.\nIn batch system, tasks were divided in parts.\n\nTo run a program, program as to write programs on card.\nPrograms from those cards where written on magnetic taps using a reader.\nThose taps where then provided to mainframe computer who actually does computing.\nOutput where written on magnetic taps by mainframe computer.\nOutput taps then provided to reader again to print the output through printer.\n\n\n\nGeneration 1965-80 of ICs and multiprogramming\n\nComputer systems became more compact by the introduction of ICs.\nThis was the time of mutiprogramming where various users can access a single mainframe computer.\nThese computers were used in time sharing mode.\nMULTICS(Multiplexed information and computing service) was a an operating system designed for time sharing.\nMULTICS leads the development of UNIX. An employee in MULTICS named Ken Thompson developed UNIX.\nBy seeing MULTICS as it was open source, a Finnish student Linos Torvalds developed Linux.\n"},"notes/operating-systems/input-output":{"title":"input-output","links":["tags/operating-system","tags/input/output","tags/architecture"],"tags":["operating-system","input/output","architecture"],"content":"tags:operating-systemoutputarchitecture\nCPU talks with IO devices using device controllers. More specificially, CPU send request to device driver and device driver talks with device controller.\nThere are different approaches of communication involves in different computer architectures. Some of them are\n\nDevice driver send request to device controllers and waits for device controller to give some response. Device driver goes to a tight loop of waiting.\nDevice driver gets interrupted from device controller once the operation is done.\nUsing a dedicated hardware DMA (Direct Memory Access). There is no intervention of CPU in the data transfer. CPU talks to DMA when there is an IO operation and once the operation is done, CPU gets notified by DMA.\n"},"notes/operating-systems/memory-management/page-fault":{"title":"page-fault","links":["tags/memory-management","tags/memory"],"tags":["memory-management","memory"],"content":"tags:memory-managementmemory\nIt is the problem in which when there is an instruction to execute but that is not present in the memory, the operating system goes and gets the missing instruction from the disk.\nThis blocks the whole process."},"notes/operating-systems/modes":{"title":"modes","links":[],"tags":[],"content":""},"notes/operating-systems/operating-system-types":{"title":"operating-system-types","links":["tags/operating-system","tags/architecture","tags/os","notes/operating-systems/operating-systems"],"tags":["operating-system","architecture","os"],"content":"tags:operating-systemarchitectureos\nis_child_of: operating-systems\nThere are different types of operating systems used for,\n\nMainframe computers\nServer computers\nMultiprocessor\nPersonal comptuers\nHandheld computers\nEmbedded systems\nReal time systems\nSensory nodes operating systems\nSmart card operating systems\n\nMainframe computer\nThese are super computer handles high computing and IO.\nOperating systems used are\n\nOS/390\nUnix and Linux\n\nServer computers\nThese come next from mainframe computers. These are workstations and large personal comptuer connected with each other.\n\nSolaris\nFreeBSD\nLinux\nWindows server\n\nMultiprocessor\nHaving multiple processors in a single chips. It requires an operating systems which can control those processors and resources.\n\nWindows\nLinux\nUnix\n\nPersonal computers\nWe do use them in daily routine.\n\nWindows\nLinux\nUnix\nMacintosh\n\nHandheld computers\nThese are Personal Digital Assistants (PDAs) devices.\n\nPalm OS\nSymbian OS\n\nSensory nodes\nNodes with sensors to examine surroundings and other designed use cases.\n\nTinyOS\n\nEmbedded systems\nThese are systems which are designed for a specific use case. These usually embedded into the hardware to control its functions. For example washing machine, microwave etc.\n\nQNX\nVxWorks\n\nReal time systems\nTime is a real constraint here. Actions have to be taken at their right time otherwise can cause consequences.\nThis includes hard real time and soft real time systems. Hard real time where time is a serious constraint and wheareas in soft real time, missing the desirable action on a particular time can be accepted.\n\ne-Cos\n\nSmart card operating systems\nWe have ATM cards and metro cards. They also have an operating that gets activated when come in contact with sensory machines."},"notes/operating-systems/operating-systems":{"title":"operating-systems","links":["tags/os"],"tags":["os"],"content":"tag:os\nOperating System\nReferences\n\nTextbook\n"},"notes/operating-systems/process/interprocess-communication/condition-variable":{"title":"condition-variable","links":["tags/multithreading","tags/multiprocessing","tags/thread","tags/busy-waiting","tags/synchronization","notes/operating-systems/process/interprocess-communication/mutex"],"tags":["multithreading","multiprocessing","thread","busy-waiting","synchronization"],"content":"tags:multithreadingmultiprocessingthreadbusy-waitingsynchronization\nCondition Variable\nCondition variable is a synchronisation method where a process/thread is blocked by some condition. The thread is not restarted until the condition becomes true.\nIt commonly used with mutex.\nOperations\nThe main operations associated with condition variable are.\n\nwait\n\nSuspend the process/thread calling this procedure until a signal to wake it up not send called by some other process/thread. It also unlocks the locked mutex.\n\n\nsignal\n\nResumes the suspended process/thread\n\n\nbroadcast\n\nResumes all of the process/threads waiting for the particular condition variable. Schedular then can choose which one to restart.\n\n\n\nFollowing shows a C program using pthread.\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;pthread.h&gt;\n \n#define MAX 10\n \npthread_mutex_t mutex;\npthread_cond_t condc, condp;\nint buffer = 0;\n \nvoid *producer() {\n\tint i;\n \n\tfor (i = 1; i &lt; MAX; i++) {\n\t\tpthread_mutex_lock(&amp;mutex);\n\t\twhile ( buffer != 0) {\n\t\t\tprintf(&quot;Producer waiting\\n&quot;);\n\t\t\tpthread_cond_wait(&amp;condp, &amp;mutex);\n\t\t}\n\t\t\n\t\tprintf(&quot;Producer producing\\n&quot;);\n\t\tbuffer = i;\n\t\tpthread_cond_signal(&amp;condc);\n\t\tpthread_mutex_unlock(&amp;mutex);\n\t}\n \n\tpthread_exit(0);\n}\n \nvoid *consumer() {\n\tint i;\n\tfor (i = 1; i &lt; MAX; i++) {\n\t\tpthread_mutex_lock(&amp;mutex);\n\t\twhile (buffer == 0) { \n\t\t\tprintf(&quot;Consumer waiting\\n&quot;);\n\t\t\tpthread_cond_wait(&amp;condc, &amp;mutex);\n\t\t}\n\t\t\n\t\tprintf(&quot;Consumer consuming buffer\\n&quot;);\n\t\tbuffer = 0;\n\t\tpthread_cond_signal(&amp;condp);\n\t\tpthread_mutex_unlock(&amp;mutex);\n\t}\n \n\tpthread_exit(0);\n}\n \nint main() {\n\tpthread_t con, prod;\n\tpthread_mutex_init(&amp;mutex, 0);\n\tpthread_cond_init(&amp;condc, 0);\n\tpthread_cond_init(&amp;condp, 0);\n\tpthread_create(&amp;con, 0, consumer, 0);\n\tpthread_create(&amp;prod, 0, producer, 0);\n\t\n\tpthread_join(prod, 0);\n\tpthread_join(con, 0);\n\t\n\tpthread_cond_destroy(&amp;condc);\n\tpthread_cond_destroy(&amp;condp);\n\tpthread_mutex_destroy(&amp;mutex);\n\treturn 0;\n}\nProgram in python.\nimport threading\nfrom random import randint \n \nMAX = 5\n \nshared_buffer = []\n \ncond = threading.Condition()\n \ndef producer():\n\t\n\tfor i in range(1, 20):\n\t\twith cond:\n\t\n\t\t\twhile len(shared_buffer) &gt;= MAX:\n\t\t\t\tprint(&quot;===&gt; Producer waiting &lt;===&quot;)\n\t\t\t\tcond.wait()\n\t\n\t\t\titem = randint(0, 100)\n\t\t\tprint(&quot;Producing item &quot;, item)\n \n\t\t\tshared_buffer.append(item)\n\t\t\tcond.notify()\n \n \n \ndef consumer():\n\tfor i in range(1, 20):\n\t\twith cond:\n\t\t\twhile len(shared_buffer) == 0:\n\t\t\t\tprint(&quot;===&gt; Consumer waiting &lt;===&quot;)\n\t\t\t\tcond.wait()\n \n\t\t\titem = shared_buffer.pop(0)\n\t\n\t\t\tprint(&quot;Consuming item &quot;, item)\n\t\n\t\t\tcond.notify()\n \n \n \ndef main():\n \n\tprod_thread = threading.Thread(target=producer)\n\tcon_thread = threading.Thread(target=consumer)\n \n\tprod_thread.start()\n\tcon_thread.start()\n \n\tprod_thread.join()\n\tcon_thread.join()\n \n \nmain()\n \nHere, condition variable is much abstract as it interally controls mutex (with context manager is for that only)."},"notes/operating-systems/process/interprocess-communication/for-later":{"title":"for-later","links":["tags/later"],"tags":["later"],"content":"tags:later\n\nRead-Copy-Update (page 148)\n"},"notes/operating-systems/process/interprocess-communication/interprocess-communication":{"title":"interprocess-communication","links":["tags/process","tags/communication","tags/ipc","tags/threads","notes/operating-systems/process/process"],"tags":["process","communication","ipc","threads"],"content":"tags:processcommunicationipcthreads\nis_child_of: process\nThere are some of the issues related to interprocess communcation. Three of them are.\n\nHow process will communicate?\nHow process wonâ€™t come in each othersâ€™ way?\nHow proper sequencing will met incase of dependencies?\n\nProcess B waiting for output generated by process A.\n\n\n\nAll the problems and solutions are same for threads as well."},"notes/operating-systems/process/interprocess-communication/message-passing":{"title":"message-passing","links":["tags/message-passing","tags/mpi","tags/ipc","tags/communication"],"tags":["message-passing","mpi","ipc","communication"],"content":"tags:message-passingmpiipccommunication\nMessage Passing\nIt is an interprocess communication mechanism in which one process sends messages to another process. It can happen on the local processes or processes running on different system on the remote.\nThere are two system calls to achieve message passing.\nsend(reciever, &amp;message)\n\nreceive(sender, &amp;message) \n\nDifferent approaches of implementation\n\nMailbox: There is a mailbox of each process running. Processes send messages to that mailbox. Mailbox are the buffers of certain size specified when these are created, to store messages.\n\nInstead of addressing processes in the messages, mailbox for the particular process is addressed.\n\n\nRendezvous: In the mechanism, if a send is done before the receive, the sender will be blocked. If a receive is done before the send, the receive gets blocked.\n\n\nNote: Message passing is commonly used in parallel programming. One of the well known interfaces for message passing is MPI (Message Passing Interface).\n"},"notes/operating-systems/process/interprocess-communication/monitors":{"title":"monitors","links":["tags/synchronization","tags/multithreading","tags/multiprocessing","tags/threads","tags/process","notes/operating-systems/process/interprocess-communication/producer-consumer-problem"],"tags":["synchronization","multithreading","multiprocessing","threads","process"],"content":"tags:synchronizationmultithreadingmultiprocessingthreadsprocess\nMonitors\nA monitor is an abstraction provided to implement synchronization while accessing sharing resource. It abstracts the implementation of conditional variables and semaphores or mutexes.\nThis is the feature provided the by language itself. It is the concept and different language implements this concept.\nFor instance, Java uses synchronized keyword to achieve synchronization on the block of code or the method.\nThe producer and consumer problem can be thought using monitors in this way.\nfunction Producer() {\n    while true {\n        item = produceItem()\n        ProducerConsumer.insert(item)\n    }\n}\n\nfunction Consumer() {\n    while true {\n        ProducerConsumer.remove()\n    }\n}\n\nmonitor ProducerConsumer {\n    cond_var full, empty;\n    int buffer = 0;\n    mutex_var mutex;\n    \n    function insert(item) {\n        lock(mutex)\n        if (buffer != 0) wait(full, mutex)\n        buffer = 1\n        signal(empty)\n        unlock(mutex)\n    }\n\n    function remove() {\n        lock(mutex)\n        if (buffer == 0) wait(empty, mutex)\n        buffer = 0\n        signal(full)\n        unlock(mutex)\n    }\n}\n"},"notes/operating-systems/process/interprocess-communication/mutex":{"title":"mutex","links":["tags/mutual-exclusion","tags/semaphores","notes/operating-systems/process/interprocess-communication/mutual-exclusion","notes/operating-systems/process/interprocess-communication/condition-variable"],"tags":["mutual-exclusion","semaphores"],"content":"tags:mutual-exclusionsemaphores\nMutex\nmutex is a special kind of semaphore for a specific purpose to ensure mutual-exclusion. It is similar to lock variable. It can only contain one of two values, 0 or 1. 0 means unlocked and 1 means locked.\nImplementation of mutex for threads\nmutex_lock:\n    TSL REG, MUTEX\n    CMP REG, #0\n    JE RET\n    CALL thread_yield\n    JMP mutex_lock\n\nmutex_unlock:\n    MOVE MUTEX, #0\n    RET\n\nThis implementation may seem similar to enter_region and exit_region. However, it is not. There is a subtle difference between them.\nenter_region and exit_region involves busy waiting. This one though doesnâ€™t do that.\nSee also\n\nFutex\nMutex in Pthreads\ncondition-variable\n"},"notes/operating-systems/process/interprocess-communication/mutual-exclusion":{"title":"mutual-exclusion","links":["tags/os","tags/multiprocessing","tags/race-condition","tags/deadlocks"],"tags":["os","multiprocessing","race-condition","deadlocks"],"content":"tags:osmultiprocessingrace-conditiondeadlocks\nMutual Exclusion\nThis is the condition in which two or more process try to access a single shared resources at the same time. The sequence of accessing the resource matters. If the sequence changes, the result can vary and not wanted.\nThere are various proposed solutions to this problem. With each solution, there are sticking problem with them also.\n1. Disabling interrupts\nWhen the process goes into critical region, it stops responding to clock interrupts.\nProblems\n\nGiving the ability to enable/disable interrupts to user space is not a wise idea.\nWhat if process disables the interrupts but never enables it?\n\nIt is better for kernel level operations instead of user space.\n2. Lock variables\nProcess sets the lock variable to let other processes know that the sharing resource is in use.\nIf you notice, that variable is also a shared resource which is getting used to access another shared resource. How would this variable will be managed?\n3. Strict turns\nProcesses takes turns for getting into their critical regions. Before going into critical region, it has to check if the turn has come. If not, process goes into loop until the turns comes.\nturn = 0;\nProcess A\nwhile (TRUE) {\n    while (turn != 0)\n    critical_region();\n    turn = 1;\n    normal_region();\n}\nProcess B\nwhile (TRUE) {\n    while (turn != 1)\n    critical_region();\n    turn = 0;\n    normal_region();\n}\nInitially, turn is 0, so process A executes critical region and changes turn to 1 . Process B starts executing critical region and changes turn to 0. This goes on like this.\nIt has a problem, one process executing non critical region can case another process to block executing its critical region.\n4. Petersonâ€™s solution\nSolution proposed by Peterson in which he defines two functions in C, which are called before and after the execution of critial regions.\nenter_region is called before going into critical region.\nexit_region is called after the critical region.\n5. TSL instruction\nThe problem with lock variable was that a process can have old value of lock variable and can go into critical region at the same time.\nenter_region:\n    LOAD LOCK, REG\n    MOVE LOCK, #1\n    CMP REG, #0\n    JNE enter_region\n    RET\n\nexit_region:\n    MOVE LOCK, #0\n    RET\n\nIt has a problem. Letâ€™s say process A calls enter_region which executes first assembly instruction and gets suspended. Another process calls enter_region  and sets LOCK to 1. Previous process again starts executing but has old value of LOCK.\nSo, we the reading and settings of LOCK variable should be atomic in nature. This can be accomplished using TSL (Test and Set Lock) instruction. It loads value and sets the value in one shot. It blocks the memory bus so that no other process access the LOCK variable.\nenter_region:\n    TSL REG, LOCK\n    CMP REG, #0\n    JNE enter_region\n    RET\n"},"notes/operating-systems/process/interprocess-communication/producer-consumer-problem":{"title":"producer-consumer-problem","links":["tags/multiprocessing","tags/multithreading","tags/threads","tags/deadlocks","tags/race-condition","notes/operating-systems/process/interprocess-communication/semaphores"],"tags":["multiprocessing","multithreading","threads","deadlocks","race-condition"],"content":"tags:multiprocessingmultithreadingthreadsdeadlocksrace-condition\nProducer Consumer Problem\nThis is a synchronization issue in multiprocessing and multithreading. In this problem, there are two types of processes or threads accessing a single resource or buffer.\nThis problem occur when both of the process try to access the buffer at the same time causing data inconsistencies, deadlocks etc.\nTo solve these problem, there are various techniques are used. One of them is semaphores."},"notes/operating-systems/process/interprocess-communication/race-conditions":{"title":"race-conditions","links":["tags/os","tags/parallelism","tags/multiprocessing","tags/process","notes/operating-systems/process/interprocess-communication/mutual-exclusion"],"tags":["os","parallelism","multiprocessing","process"],"content":"tags:osparallelismmultiprocessingprocess\nRace condition\nThis is a situation in which two or more process try to access a shared resources at the same time where sequence of accessing can alter the resulting output.\nCritical Region\nA part of the program where the program access the shared resource.\nConditions to avoid race conditions\n\nNo two process should go to their critical regions at the same time.\nNo process running outside its critical region should block other process\nNo process should wait to enter its critical region\n\nMutual exclusion\nIt is a way to making sure if some process is using a shared resource, some other process should be excluded to access that resource.\nRead more."},"notes/operating-systems/process/interprocess-communication/semaphores":{"title":"semaphores","links":["tags/synchronization","tags/multithreading","tags/multiprocessing","tags/mutual-exclusion","notes/operating-systems/process/interprocess-communication/mutex"],"tags":["synchronization","multithreading","multiprocessing","mutual-exclusion"],"content":"tags:synchronizationmultithreadingmultiprocessingmutual-exclusion\nSemaphores are the mechanism to synchronize operations in multithreading or concurrent programming. It can be used solve race conditions, produce consumer problem, etc.\nSemaphores is a variable that maintains non-negative value and allows only two atomic operations, down and up.\nIn down operation, the value of semaphore decreases. If itâ€™s value is 0, then the process is put to sleep and value remains 0.\nIn up operation, the value of semaphore is increased. If any process is sleeping associated with the semaphore is get awakend.\nSolving producer consumer problem\nThe main problem with producer and consumer is the accessing the shared buffer. So, we have put the process to sleeping state if some other process is currently using it. That is, we have to manage the control regions.\nWe can solve this using enter_region and exit_region mechanism but it starts the infinity loop which is CPU intensive.\nLetâ€™s use semaphore to solve this issue.\n#define N 100\ntypedef int semaphore\n \nsemaphore mutex = 0;\nsemaphore empty = N;\nsemaphore full = 0;\n \nvoid producer() {\n    int item;\n \n    while (TRUE) {\n        item = produce_item();\n        down(&amp;empty);\n        down(&amp;mutex);    // entering critical region\n        insert_item(item);\n        up(&amp;mutex);    // leaving critical region\n        up(&amp;full);\n    }\n}\n \n \nvoid consumer() {\n    while (TRUE) {\n        down(&amp;full);\n        down(&amp;mutex);\n        item = remote_item();\n        up(&amp;mutex);\n        up(&amp;empty);\n        consume_item(item);\n    }\n}\nLetâ€™s understand this pesudo program. There are three semaphores, mutex, empty and full.\nmutex is used for critical region. Before going into critical region, we are doing down(&amp;mutex). If mutex is 0 then the process is put to sleeping state as shared buffer is already is in use. mutex can only contain one of two values, 0 or 1. This is called Binary Semaphore. mutex is used to ensure mutual exclusion.\nSemaphores full and empty are used to make sure the producer stops running if buffer is full and consumer stops running if the buffer is empty.\nMutexes are specially used for mutual exclusion. These are used for entering and exiting the critical region."},"notes/operating-systems/process/introduction":{"title":"introduction","links":["tags/operating-system","tags/os","tags/process","tags/program","notes/operating-systems/process/process"],"tags":["operating-system","os","process","program"],"content":"tags:operating-systemosprocessprogram\nis_child_of: process\nWhat is a process?\nA process is a running program which has allocated system resources.  In more simpler words, it is a container that holds all the information to run a program.\nProcess resources\nA process contains following resources\n\nAddress space\nRegisters\n\nProgram counter\nStack pointer\n\n\nList of open files\nRelated processes\n\nAddress space\nThis is the memory allocated to a process where it stores all its data. An address space defines the limit of memory accessing of the process.\nAddress space consists the program instructions, data, and its stack.\nProcess table\nOperating system maintains a process table having information about all the processes created so far. The table consist information like\n\nList of all the open files and their pointers.\nInformation about related processes or child processes\nEtc.\n\nWhen a process is suspended, operating system has to save process state and all the information in process table. Address space of process is not stored in process table. That is already preserved in the main memory.\nWhen the process is restarted, operating loads the process information from process table and process continues in the state it left."},"notes/operating-systems/process/memory":{"title":"memory","links":["tags/process","tags/memory","tags/memory-management","tags/operating-system","tags/os","notes/operating-systems/process/process"],"tags":["process","memory","memory-management","operating-system","os"],"content":"tags:processmemorymemory-managementoperating-systemos\nis_child_of: process\nMemory Segments\nA process has memory divided into three segments.\n\nProgram text\nData\nand Stack\n\n\nData grows upward and stack grows downward. Growing of stack is automatic while data space has be explicitely grown."},"notes/operating-systems/process/process-creation":{"title":"process-creation","links":["tags/os","tags/operating-system","tags/process","tags/architecture","notes/operating-systems/process/process"],"tags":["os","operating-system","process","architecture"],"content":"tags:osoperating-systemprocessarchitecture\nis_child_of: process\nProcess Creation\nA process is created by following events:\n\nSystem initialisation\nRunning process calling process creation system call\nUser launching a process\nStarting a batch job\n\nA process is created by using system calls provided by operating system. fork is the system call used to create a clone of an existing process.\nTaking the example of shell, when we a run a command following things happen:\n\nIt forks a child process using fork system call\nChild process copies the file descriptors of the parents to manipulate stdin, stdour and stderr\nChild process then execute execve() system call to change the process image to that command. It means that instead of executing the child  process code, the code of that command will be executed\n\nA simple shell using this theory.\nimport os\n \n \ndef read_command() -&gt; tuple:\n    user_input = input(&quot;&gt; &quot;)\n    return tuple(user_input.strip().split(&#039; &#039;))\n \n \nwhile True:\n    command, *argv = read_command()\n    env = {&#039;PATH&#039;: os.environ[&#039;PATH&#039;]}\n    pid = os.fork()\n \n    if pid == 0:\n        os.execve(command, [command, *argv], env)\n    else:\n        chid_pid, status =\\\n         os.waitpid(pid, os.WUNTRACED)\n \nIâ€™m not doing duplication of file descriptors in this example. However this duplication can be done using dup2 system call."},"notes/operating-systems/process/process-table":{"title":"process-table","links":["tags/os","tags/process","tags/scheduler","notes/operating-systems/process/process"],"tags":["os","process","scheduler"],"content":"tags:osprocessscheduler\nis_child_of: process\nProcess table\nA process table is a data structure used by scheduler to store process information and state.\nIt has process information entries, also known as process control block. This process control block contains information about:\n\nProcess management\n\nRegisters\nProgram counter\nProgram status word\nProcess id\nStack\nProcess group\nSignals\nand so on.\n\n\nMemory management\n\nPointer to text segment\nPointer to data segment\nPointer to stack segment\n\n\nFile management\n\nFile descriptors\nRoot directory\nWorking directory\nUser ID\nGroup ID\n\n\n\n\nBonus: /proc directory in Linux machine, we can see information stored about each process in user program space. Commands such as ps uses this information to show the process. However, ps also uses information from process table residing in kernel memory.\n"},"notes/operating-systems/process/process":{"title":"process","links":["notes/operating-systems/operating-systems"],"tags":[],"content":"is_child_of: operating-systems"},"notes/operating-systems/process/threads/hybrid-implementation":{"title":"hybrid-implementation","links":["tags/implementation","tags/threads","notes/operating-systems/process/threads/implementations"],"tags":["implementation","threads"],"content":"tags:implementationthreads\nis_child_of: implementations\nHybrid Thread Implementation\nIn this approach, both user and kernel space threads implementations are incorporated.\nIn this model, user level threads are created and managed in user space unaware of the underlying hardware.\nKernel level threads are associated with each user level threads to provide system resources. Whenever user level threads need to make a system call, kernel level thread makes that call on its behalf.\nThis approach solve the problem of creation and management of several threads in kernel space and system resources access by threads in user space.\nCreation and management is taking place in user space while access to system resources taking place in kernel space."},"notes/operating-systems/process/threads/implementations":{"title":"implementations","links":["tags/threads","tags/thread","tags/implementation","notes/operating-systems/process/threads/thread","notes/operating-systems/process/threads/user-space-threads","notes/operating-systems/process/threads/kernel-space-threads","notes/operating-systems/process/threads/hybrid-implementation"],"tags":["threads","thread","implementation"],"content":"tags:threadsthreadimplementation\nis_child_of: thread\nThere are majorliy three way the threads can be implemented.\n\nUser space threads\nKernel space threads\nHybrid implementation\n"},"notes/operating-systems/process/threads/kernel-space-threads":{"title":"kernel-space-threads","links":["tags/thread","tags/threads","tags/kernel","tags/implementation","notes/operating-systems/process/threads/implementations"],"tags":["thread","threads","kernel","implementation"],"content":"tags:threadthreadskernelimplementation\nis_child_of: implementations\nImplementing threads in Kernel\n\nKernel is aware of processes threads.\nIt manages threads table having theads state.\nThreads calls blocking system calls and kernel switches the threads.\n\nSome problems\n\nSystem calls are costlier to switch and manage threads.\nSignal issue. If a signal is sent to process, which thread should handle it?\n"},"notes/operating-systems/process/threads/scheduler-activation":{"title":"scheduler-activation","links":["tags/threads","tags/scheduler","tags/synchronization","notes/operating-systems/process/threads/thread"],"tags":["threads","scheduler","synchronization"],"content":"tags:threadsschedulersynchronization\nis_child_of: thread\nScheduler Activation\nThis method focuses on mimicking the functionality of kernal threads. The way kernel threads handle the scheduling threads when a thread gets blocked. If all of these process can happen in user space through the implemetation of threads package.\nWhenever a thread calls blocking system call, kernel notifies run time about it by sending some parameters. Run time then manages which thread to run. When the blocking thread becomes activated, kernel again specifies run time to schedule that thread. Now it is upto run time to restart the thread or schedule it for later.\nThe call where kernel notfies run time is called upcall.\nThe main idea behind this method is to avoid the transition between user space and kernel space."},"notes/operating-systems/process/threads/thread":{"title":"thread","links":["tags/os","tags/process","tags/thread","notes/operating-systems/process/process"],"tags":["os","process","thread"],"content":"tags:osprocessthread\nis_child_of: process\nThread\nA thread is a sequence of instructions getting executed independently. A thread is a part of the big process running independently from the parent process.\nWhy do we need threads?\nThere are following reasons to use threads.\n\nProgram having several IO, networks and any blocking operation holds the program execution and CPU remains idle. This leads to resources wastage. So, organizing the program using threads can resolve this issue.\nThreads are lightweight. These are easy to create and destroy. Threads donâ€™t require much of the resources like parent.\nOperations require parent process resource sharing. Different process canâ€™t be created for the same resource. However, threads can share same parent resource.\nThreads can help gaining performance if the program is composed of several blocking operations.\n\nThreads complications\n\nMakes programming model complicated.\nIf child processes get threads from parent, blocking thread\n"},"notes/operating-systems/process/threads/user-space-threads":{"title":"user-space-threads","links":["tags/threads","tags/thread","tags/user-space","tags/implementation","notes/operating-systems/process/threads/implementations","notes/operating-systems/memory-management/page-fault"],"tags":["threads","thread","user-space","implementation"],"content":"tags:threadsthreaduser-spaceimplementation\nis_child_of: implementations\nImplementing Threads in User space\n\nLibraries in user space are handling creation and management of threads.\nThere is a run-time maintaining a thread table similar to process table managed in kernel space.\nThread table stores the state of each thread which is used in threads switching.\n\nAdvantages over Kernel space\n\nThere is no system call invoked for thread switching leading to performance gain.\nThere is no kernel trap.\nEach process can have its own customized scheduling algorithm.\nIt is scalable as memory wonâ€™t be a problem for threads table when the threads numbers increase.\n\nDisadvantages\n\nThreads using blocking calls such as for I/O or networking, it blocks the whole process, so blocks all threads.\nThis can be avoided using non-blocking calls. However, it would require to change system calls of operating system which is unwanted.\nIf a thread starts running, no other thread in that process will ever run unless the first thread voluntarily gives up the CPU.\nIf a thread causes page-fault, the kernel naturally blocks the entire process until the respective instructions are fetched from the disk.\n\n"},"notes/operating-systems/registers":{"title":"registers","links":["tags/operating-system","tags/os","tags/architecture","notes/operating-systems/operating-systems"],"tags":["operating-system","os","architecture"],"content":"tag:operating-systemosarchitecture\nis_child_of: operating-systems\nCPU contains several registers to store intermediatery variables and other data which is frequently used. Some of the examples are,\n\nGeneral registers that a programmer doesnâ€™t have access to.\nProgram counter which stores the address of next instruction to fetch.\nStack pointer which points to the top of the stack in memory. Stack is used to store the procedure data such as arguments, what is the return address etc.\nPSW (Program Status Word) to store result of conditional instructions and some other information.\n\n"},"notes/operating-systems/resource-multiplexing":{"title":"resource-multiplexing","links":[],"tags":[],"content":""},"notes/operating-systems/system-calls":{"title":"system-calls","links":["tags/os","tags/operating-system","tags/computer","tags/architecture","notes/operating-systems/operating-systems"],"tags":["os","operating-system","computer","architecture"],"content":"tags:osoperating-systemcomputerarchitecture\nis_child_of: operating-systems\nWhat are System calls?\nSystem calls are the abstractions provided by operating system to access low level resources such as IO handling, read/write files, etc.\nThese are the way in which a user program can request service from the kernel of the operating system.\nThere are libraries provided which lets user program make use of system calls.\nFor example, in python we have os package providing system calls to read/write files and some other features.\nimport os\n \nfd = os.open(&#039;file.txt&#039;, os.O_RDONLY)\n \ncontent = os.read(fd, 100)\n \nprint(content.decode())\nThere are library procedures which make use of system calls. We hardly use system calls directly. We use library procedures to deal with system calls.\nRunning a system call\nWhen a system call is invoked, following things happens in a sequence.\n\nUser program calls library procedure to make a system call.\nVariables are pushed on to the stack as the control is going to procedure call.\nProcedure sets a value at a specified register or location the operating system expects for some system call.\nProcedure executes a TRAP instruction which in turn executes the TRAP handler in the kernel mode.\nOnce the TRAP handler done with its work, the control is returned back to procedure call and in return the control goes back to the user program.\n\nProcedure call Vs Trap call\nBoth of these are instruction calls. However, they differ in following two ways.\n\nExecution of TRAP instruction changes mode from user to kernel.\nProcedure call instruction contains the address where to find the instruction. This address is dynamic. Whereas, TRAP instruction as a 8 bit location which points to a table in memory that has the location of TRAP handler to execute. And this is fixed by the system architecture.\n"},"notes/operating-systems/users-and-groups":{"title":"users-and-groups","links":["tags/os","tags/operating-system","tags/computer","tags/linux","tags/unix","notes/operating-systems/operating-systems"],"tags":["os","operating-system","computer","linux","unix"],"content":"tags:osoperating-systemcomputerlinuxunix\nis_child_of: operating-systems\nUsers\nAdding users\nWe can add users using command useradd\nuseradd nitin\nWe have to create a directory of this user and user permissions to that directory.\nmkdir /home/nitin\n \nchown nitin /home/nitin\nNow if we see the permissions we will see,\nroot@515d32d1d259:/home/hemant# ls -alrt /home\ntotal 12\ndrwxr-xr-x 1 nitin root 4096 Apr  6 11:27 nitin\ndrwxr-xr-x 1 root  root 4096 Apr  6 11:27 ..\ndrwxr-xr-x 1 root  root 4096 Apr  6 11:39 .\nThere is one more way to create  a user.\nadduser nitin\nThis does everything for you.\nDeleting user\nuserdel nitin\nrm -rf /home/nitin\nor\ndeluser nitin\nSwitching user\nsu nitin\nsu is switch user command.\nChanging the password\npasswd nitin\n \nNew Password:\nRetype New Password:\nGroups\nA group is a collection of users with a set of permissions.\nCreating a group\ngroupadd bitphile\nAdding group to directory and file\nchgrp nitin /home/nitin\nAdding user to the group\nusermod -aG bitphile nitin\n-aG means add group.\nDelete user from the group\ngpasswd -d bitphile nitin\nDelete group\ngroupdel bitphile\nSee also\n\nsudoers file\n"},"notes/operating-systems/what-is-os":{"title":"what-is-os","links":["tags/operating-system","tags/os","tags/computer","tags/applications","tags/architecture","notes/operating-systems/operating-systems","notes/operating-systems/modes","notes/operating-systems/resource-multiplexing"],"tags":["operating-system","os","computer","applications","architecture"],"content":"tags:operating-systemoscomputerapplicationsarchitecture\nis_child_of: operating-systems\nWhat is an Operating system?\nThis is the program that provides abstractions for user programs to deal with computer hardwares and other resources. It works in kernel modes.\nResource Manager\nOperating system is a resource manager which manages complex system resources among various programs.\nThere are two ways in which an OS multiplexes (shares) the resources among programs.\n\nTime multiplexing\nSpace multiplexing\n\nIn time multiplexing, user programs take turns to use resources. Whoever comes first uses the resource first.\nIn Space multiplexing, defined space is allocated to the programs. For example, space on hard disk and RAM.\nSee resource-multiplexing."},"notes/tech/CPython/Asyc-Server":{"title":"Asyc Server","links":[],"tags":[],"content":"Asyc Server\nDescription\nThis article goes through the thorogh theory and practical information of creating an async server that can handle multiple clients at the same time. We would start with simple socket server that handles only one client and then gradually improve our server.\nLetâ€™s get startedâ€¦\nSimple echo server\n# server.py\n \nimport socket\n \ndef handle_connection(sock: socket.socket):\n    while True:\n        received_data = sock.recv(4096)\n        if not received_data:\n            break\n        sock.sendall(received_data)\n \n    print(&#039;Client disconnected&#039;, sock.getpeername())\n    sock.close()\n \ndef run_server(host: str, port: int) -&gt; None:\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&#039;Server is listening on {}:{}&#039;.format(host, port))\n    while True:\n        client_sock, addr = sock.accept()\n        print(&#039;Connetion from&#039;, addr)\n        handle_connection(client_sock)\n \nif __name__ == &#039;__main__&#039;:\n    run_server(&#039;localhost&#039;, 3000)\nNow, when we run it and connect with it, only one connection can be made at single point of time. For next connection the already existing connection has to end first.\npython3 server.py\n \n# client\nnc localhost 3000\nhello\nhello\nIntroducing Threads\nLetâ€™s use threads for multiple connections:\n \n# server.py\n \nimport socket\nimport threading\n \ndef handle_connection(sock: socket.socket):\n    while True:\n        received_data = sock.recv(4096)\n        if not received_data:\n            break\n        sock.sendall(received_data)\n \n    print(&#039;Client disconnected&#039;, sock.getpeername())\n    sock.close()\n \ndef run_server(host: str, port: int) -&gt; None:\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&#039;Server is listening on {}:{}&#039;.format(host, port))\n    while True:\n        client_sock, addr = sock.accept()\n        print(&#039;Connetion from&#039;, addr)\n\t\t\t\tthread = threading.Thread(target=handle_connection, args=[client_sock])\n\t      thread.start()\n \nif __name__ == &#039;__main__&#039;:\n    run_server(&#039;localhost&#039;, 3000)\n \nSo, this letâ€™s us connect multiple clients but still for each connection you need to have a thread which is expensive.\nUsing Thread pools\nTo control the number of threads being used, we can use thread pools\nfrom concurrent.futures import ThreadPoolExecutor\n \nsockets = set()\n \npool = ThreadPoolExecutor(max_workers=20)\n \ndef handle_connection(sock: socket.socket):\n    while True:\n        received_data = sock.recv(4096)\n        if not received_data:\n            break\n        sock.sendall(received_data)\n \n    print(&#039;Client disconnected&#039;, sock.getpeername())\n    sock.close()\n \ndef run_server(host: str, port: int) -&gt; None:\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&#039;Server is listening on {}:{}&#039;.format(host, port))\n    while True:\n        client_sock, addr = sock.accept()\n        print(&#039;Connetion from&#039;, addr)\n\t\t\t\tpool.submit(handle_connection, client_sock)\n \nif __name__ == &#039;__main__&#039;:\n    run_server(&#039;localhost&#039;, 3000)\nIf you create 20 connections, to make one more connection, you need to wait for the atleast one to get end.\nLetâ€™s use IO multiplexing\nThe major problem with our socket is that it has to wait for any read or write that blocks the program execution. How cool that would be if someone notifies us that there is some update on the socket and run a callback? Cool huhâ€¦\nPython provides IO multiplexing module selectors  that we can use. It provides different selectors and one of them is DefaultSelector .\nTo register a socket, we do\nsel = selectors.DefaultSelector()\n \nsel.register(sock, selectors.EVENT_READ, data)\nsel.select() returns a list of (key, events) tuple and each tuple describes a readu socket.\n\nkey is an object with socket object key.filobj and auxiliary data key.data.\nevents are bitmasks of events ready on the socket.\n\nSo, [server.py](server.py) with selectors\n# server.py\n \nimport selectors\nimport socket\n \nsockets = set()\n \ndef broadcast(broadcaster, message):\n    for sock in sockets:\n        if broadcaster is not sock:\n            sock.sendall(message)\n \ndef handle_connection(sock: socket.socket):\n    received_data = sock.recv(4096)\n    if received_data:\n        broadcast(sock, received_data)\n    else:\n        print(&#039;Client disconnected:&#039;, sock.getpeername())\n        sel.unregister(sock)\n        sock.close()\n \nsel = selectors.DefaultSelector()\n \ndef accept(sock: socket.socket):\n    client_sock, addr = sock.accept()\n    print(&#039;Client connected from {}&#039;.format(addr))\n    sockets.add(client_sock)\n    sel.register(client_sock, selectors.EVENT_READ, handle_connection)\n \ndef setup_server_socket(host: str, port: int):\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&#039;Server is listening on {}:{}&#039;.format(host, port))\n    sel.register(sock, selectors.EVENT_READ, accept)\n \ndef run_event_loop():\n    while True:\n        for key, _ in sel.select():\n            cb = key.data\n            sock = key.fileobj\n            cb(sock)\n \nif __name__ == &#039;__main__&#039;:\n    setup_server_socket(&#039;localhost&#039;, 3000)\n    run_event_loop()\nThis approach also has  a problem. We are writing to the socket and if the writing queue is full, it will block the execution. Which means, handle_connection should be broken into two functions to write and read from the socket.\nThis works for the simple applications but for the larger applications this doesnâ€™t work.\nGenerators as Coroutines\nWe can use yield return the control back to the caller. This way we can achieve context switching as run code irrespective of blocking statements. The easiest thing we can do it to put yield above every blocking statements.\nAbout yield\nThis is used to create generators. What it does is returns the control back to the caller if yield gets executed. For example,\n1. def create_counter():\n2. \t\tfor i in range(10):\n3. \t\t\tyield i\n4.       \n \ncounter = create_counter()\n \nnext(counter) # gives 0\nnext(counter) # gives 1\n#...\nSo, it returns the control back to the caller if yield gets executed. In the context of generator, if generator is called again, it resumes where it left of. For examle, next(counter) returns 0 and controls goes back to calling location. When next(counter) called again generator start executing from line number 4 . It goes through the loop, i becomes 1 and again yield happens and it returns 1.\nLetâ€™s look another example for more details\ndef generator():\n\t\tprint(&#039;started&#039;)\n\t\tprint(&#039;before yielding 1&#039;)\n\t\tyield 1\n\t\tprint(&#039;before yielding 2&#039;)\n\t\tyield 2\n \ng = generator()\nnext(g) # prints &#039;started&#039;, &#039;before yielding 1&#039; and returns 1\nnext(g) # prints &#039;before yielding 2&#039; and returns 2\nnext(g) # raises StopIteration exception and there is no more yield statement\nUsing generator in our problem\nSo, we can put yield wherever there is blocking code. Also, we need an event loop the runs all the generators (or coroutines).\n# server.py\nfrom collections import deque\nimport socket\n \nclass EventLoop:\n    def __init__(self):\n        self.tasks_to_run = deque([])\n \n    def create_task(self, coro):\n        self.tasks_to_run.append(coro)\n \n    def run(self):\n        while self.tasks_to_run:\n            task = self.tasks_to_run.popleft()\n            try:\n                print(&quot;running task&quot;, task)\n                next(task)\n            except StopIteration:\n                print(&#039;STOPITERATION FOR ==&gt; &#039;, task)\n                continue\n            self.create_task(task)\n \ndef run_server(host=&#039;127.0.0.1&#039;, port=55555):\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&quot;Server is running on {}:{}&quot;.format(host, port))\n    while True:\n        print(&quot;in run_server while loop&quot;)\n        yield\n        client_sock, addr = sock.accept()\n        print(&#039;Connection from&#039;, addr)\n        loop.create_task(handle_client(client_sock))\n \ndef handle_client(sock):\n    print(sock)\n    while True:\n        yield\n        received_data = sock.recv(4096)\n        print(received_data)\n        if not received_data:\n            break\n        yield\n        sock.sendall(received_data)\n \n    print(&#039;Client disconnected:&#039;, sock.getpeername())\n    sock.close()\n \nloop = EventLoop()\n \nif __name__ == &#039;__main__&#039;:\n    loop.create_task(run_server(&#039;localhost&#039;, 3000))\n    loop.run()\nSo, there is an event loop which has two methods of creating tasks and running all of them sequentially. Itâ€™s using deque .\nFlow\n\nWe create event loop loop and create task where run_server() returns a generator that gets added to the queue. Then we start the loop.\nLoop takes the leftmost task from the queue, runs it using next() that executes the generator until next yield comes. For the first time when run_server generator is getting executed, socket gets setup, we goes into the loop and we encouters our first yield that returns control back to the loop.\nLoop again takes the same task as there is no task, continue executing the task. As before it was yield , now it executes accept() that blocks the execution and waits for the connection.\nOnce a client is connected, control moves forward. One more task is created of connection handler. Execution continues there in run_server loop. It goes to the loop begining and prints in run_server while loop where again yield comes and control goes back to the event loop.\nNext task is then executed which is handler. It goes like that.\n\nDid you see the problem here?\nThere is a problem. Though there is some sort of concorrency but still another task has to wait for the previous task to get completed (or until next yield comes in the task).\nSo, if a client is connected and there is handler task get control, if client writes something on the socket, it wonâ€™t be written back as the\nyield\n        sock.sendall(received_data)\nmakes control go back to the event loop and run_server takes control and it waits for the new connection. When a new connection is made then control goes back to handler and then data is written back.\nSolving this issue with IO multiplexing\nWe can combine both generators and IO multiplexing to improve our solution. We have modify our EventLoop class.\n# event loop class\n \nclass EventLoop():\n    def __init__(self):\n        self.tasks = deque([])\n        self.sel = selectors.DefaultSelector()\n \n    def create_task(self, task):\n        self.tasks.append(task)\n \n    def run(self):\n        while True:\n            print(&#039;tasks&#039;, self.tasks)\n            if self.tasks:\n                task = self.tasks.popleft()\n                try:\n                    op, sock = next(task)\n                except StopIteration:\n                    continue\n \n                if op == &#039;read&#039;:\n                    self.sel.register(sock, selectors.EVENT_READ, task)\n                elif op == &#039;write&#039;:\n                    self.sel.register(sock, selectors.EVENT_WRITE, task)\n                else:\n                    raise ValueError(&#039;Invalid event loop operation type&#039;, op)\n            else:\n                for key, _ in self.sel.select():\n                    sock = key.fileobj\n                    task = key.data\n                    self.sel.unregister(sock)\n                    self.create_task(task)\nSo, what change we made in EventLoop class is that we get the task, run next. Now, yield in generator returns some information. It returns the operation ( read or write) and the particular socket.\nWhat we do is, according the operation code, we register the socket with selectors. So, if there is no task in the queue, we wait for some update on the socket using sel.select() method. We get the socket, and the task, unregister the socket and put back the same task into the queue where it gets executed.\ndef run_server(host=&#039;127.0.0.1&#039;, port=55555):\n    sock = socket.socket()\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind((host, port))\n    sock.listen()\n    print(&quot;Server is running on {}:{}&quot;.format(host, port))\n    while True:\n        print(&quot;in run_server while loop&quot;)\n        yield &#039;read&#039;, sock\n        client_sock, addr = sock.accept()\n        print(&#039;Connection from&#039;, addr)\n        sockets.add(client_sock)\n        loop.create_task(handle_client(client_sock))\n \ndef broadcast(sender_sock, message):\n    for sock in sockets:\n        if sock is not sender_sock:\n            sock.sendall(message)\n \ndef handle_client(sock):\n    print(sock)\n    while True:\n        yield &#039;read&#039;, sock\n        received_data = sock.recv(4096)\n        print(received_data)\n        if not received_data:\n            break\n        yield &#039;write&#039;, sock\n        broadcast(sock, received_data)\n        # sock.sendall(received_data)\n \n    print(&#039;Client disconnected:&#039;, sock.getpeername())\n    sock.close()\n \nloop = EventLoop()\n \nif __name__ == &#039;__main__&#039;:\n    loop.create_task(run_server(&#039;localhost&#039;, 3000))\n    loop.run()"},"notes/tech/CPython/Async-IO":{"title":"Async IO","links":[],"tags":[],"content":"Async IO\nItâ€™s single threaded, single process design. It uses cooperative multitasking.\nCoroutines\nCoroutines are function that can suspend its execution and pass control to another coroutine.\nRead More"},"notes/tech/CPython/Pandas":{"title":"Pandas","links":[],"tags":[],"content":"Pandas\nPandas is a python library used for data structures and data analysis.\nLoading CSV files\nimport pandas\ndf1 = pandas.read_csv(&quot;filename.csv&quot;)\ndf1\npandas.read_csv() returns a DataFrame.\nCSV file without header\ndf = pandas.read_csv(&quot;csv_without_header.csv&quot;, header=None)\ndf.columns = [&quot;col1&quot;, &quot;col2&quot;,...]\ndf\nSetting index\ndf.set_index(&quot;ID&quot;)\n# this will create a new data frame with ID set as index\n \ndf.set_index(&quot;ID&quot;, inplace=True)\n# this won&#039;t create new data frame. It will modify the same frame\nBut the second statement has a problem. If another index is set of it, the old index column would be deleted i.e. ID column be will be dropped. This can be solved using\ndf.set_index(&quot;ID&quot;, inplace=True, drop=False)\nAccessing DataFrames\nUsing labels\ndf.loc([row_start:row_end, col_start:col_end])\n \ndf1.loc[2:3, &quot;Address&quot;:&quot;City&quot;]\nUsing indexing\n# syntax df1.iloc[row_index_start: row_index_end, col_index_start: col_index_end]\n \ndf1.loc[2:3, 3:5]"},"notes/tech/CPython/Python":{"title":"Python","links":["projects/tech/CPython/Python","notes/tech/CPython/multi-threading","notes/tech/CPython/Async-IO","projects/tech/CPython/References","notes/tech/CPython/Pandas","notes/tech/CPython/opencv-python","notes/tech/CPython/Asyc-Server","notes/tech/CPython/poetry"],"tags":[],"content":"Python\nTable Of contents\nCpython\nA type of implementation of python programming language. Itâ€™s written in\nDeclaring and initializing variable\nname = &#039;nitin&#039;;\nname, age = &#039;nitin&#039;, 21;\n&gt;&gt;&gt; name, age = &#039;nitin&#039;, 21, 23\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nValueError: too many values to unpack (expected 2)\n&gt;&gt;&gt; name, age, status = &#039;nitin&#039;, 21\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nValueError: not enough values to unpack (expected 3, got 2)\n&gt;&gt;&gt;\nOperators\n\n// : floor division\n** : power\n\nData types\n\nint\nfloat\ndecimal\nfraction\ncomplex number\nstrings\n\nprint()\nr for printing raw string\nprint(&#039;hello world\\nbye world&#039;); # prints with new line\nprint(r&#039;hello world\\nbye world&#039;); # prints in single line with &#039;\\n&#039;\nend keyword for specifying what to insert at the end\nprint(&#039;hello world&#039;, end=&#039;,&#039;); #prints &#039;hello world,&#039; without new line\nString\ngreeting = &#039;good morning&#039;;\ngreeting = &quot;good morning&quot;;\nMulti-line string\ngreetings = &#039;&#039;&#039;\n\tGood morning\n\tWelcome everyone\n&#039;&#039;&#039;;\n \ngreetings = &quot;&quot;&quot;\n\tGood morning\n\tWelcome everyone\n&quot;&quot;&quot;;\n \ngreetings = &quot;&quot;&quot;\\\n\tGood morning\n\tWelcome everyone\n&quot;&quot;&quot;; # &#039;\\&#039; prevents leading new line\nTwo or more string literals are concated automatically\n&gt;&gt;&gt; &#039;good&#039; &#039; morning&#039;;\n&#039;good morning&#039;\nStrings can be indexed\nproject[0];   # returns &#039;y&#039;\nproject[-1];  # returns &#039;a&#039;\n\nIndices can be negative to start from the right.\nStrings are immutable here also\n\nSlicing\n&#039;&#039;&#039; Sytax :\n\tstring[startIndex:endIndex];\n&#039;&#039;&#039;\n \nproject[0:1]; # returns &#039;y&#039;\nproject[0:4]; # returns &#039;yask&#039;\n \nproject[:4]; # returns &#039;yask&#039;. defaults to 0\nproject[1:]; # returns &#039;aska&#039;. defaults to string length\nString length\nlen(project); # returns 5\nString formatting\nMethod 1\nname = &#039;Nitin&#039;\nsurname = &#039;Sharma&#039;\nprint(&quot;Hello %s&quot; % name)\nprint(&quot;Hello %s %s&quot; % (name, surname))\nThis method is used in old versions of pythons.\nMethod 2\nprint(f&quot;Hello {nitin}&quot;)\nMethod 3\nprint(&quot;Hello {}&quot;.format(name))\nList\nList is a collection of elements of same type or different types. Itâ€™s more likely arrays in javascript.\npreferences = [&#039;bengalore&#039;, &#039;pune&#039;, &#039;gurgaon&#039;];\npreferences[0]; # returns &#039;bengalore&#039;\n \nlen(preferences); #returns list length i.e. 3\n \n# elements can be changed using indices\npreferences[0] = &#039;pune&#039;;\npreferences[1] = &#039;bengalore&#039;;\n \n# Assignment to slices is also possible\n \npreferences[0:1] = &#039;Pune&#039;;\npreferences; # returns [&#039;Pune&#039;, &#039;bengalore&#039;, &#039;gurgaon&#039;]\n \npreferences[:] = []; # clears the list\nList in a list\ninterests = [&#039;typing&#039;, &#039;reading&#039;];\nperson = [&#039;nitin&#039;, 21, interests];\n \ninterests.append(&#039;traveling&#039;);\nperson[2];  # it also gets &#039;traveling&#039;, i.e. list reference is stored\n\nNote : for equal to operator, the list items are checked. Itâ€™s not javascript where references are compared\n\ninterests = [&#039;typing&#039;, &#039;reading&#039;];\n_interests = [&#039;typing&#039;, &#039;reading&#039;];\n \ninterests == _interests; # True\n \n_interests.append(&#039;travel&#039;);\ninterests == _interests; # False\nList comprehension\nList comprehension provides concise way to create lists\nnums = [1, 2, 3, 4]\nevens = [x for x in nums if x % 2 == 0]\n \nprint(evens) # [2, 4]\nLook, how concise it is.\nIt starts with an expression followed by for statement. That expression is compulsory.\nWith if-else\ndef _filter(items: list) -&gt; list:\n    return [item if type(item) is int else 0 for item in items]\nSo, for if-else, the position of the components has to change\nTuples\nTuples are similar to lists but these are immutable.\nnames = &#039;nitin&#039;, &#039;hemant&#039;\nprint(names) #(&#039;nitin&#039;, &#039;hemant&#039;)\n \nnames = () # for empty tuple\n \nnames = &#039;nitin&#039;, # for single value in tuple. Trailing , is needed\n \n# unpacking the tuple\nname1, = names # name1 contains &#039;nitin&#039;\nNumbers of variables on the left hand side should be equal to size of tuple.\nnames = &#039;nitin&#039;, &#039;hemant&#039;\nmy_name, = names # won&#039;t work\n \nname1, name2 = names; # will work\nThis is called sequence unpacking.\n\nNote that multiple assignment is really just a combination of tuple packing and sequence unpacking\n\nSet\nItâ€™s an unordered collection of items.\nnames = {&#039;nitin&#039;, &#039;hemant&#039;}\n# Or\nnames = set([&#039;nitin&#039;, &#039;hemant&#039;])\n \nnames = set([&#039;nitin&#039;, &#039;hemant&#039;, &#039;nitin&#039;])\nprint(names) # {&#039;nitin&#039;, &#039;hemant&#039;}\nSet operations\nIt also supports set operations\nset1 = {1, 2, 3, 4}\nset2 = {1, 4, 6, 9}\n \nset1 - set2 # {2, 3}, in set1, not in set2\nset1 &amp; set2 # {1, 4} in both\n \nFunction\ndef greet():\n\tprint(&#039;Good morning!&#039;)\nKeyword Arguments\ndef greet(age, name):\n\tprint(&#039;Good morning&#039;, name)\n \ngreet(12, name=&#039;nitin&#039;);\n\nKeyword argument should come after positional argument. If positional argument comes after keyword argument, interepreter throws error.\n\ngreet(name=&#039;nitin&#039;, 12) # won&#039;t work\ngreet(12, name=&#039;nitin&#039;)\n*args and **keywordArgs\ndef function(arg, *args, **keywordArgs):\n\tprint(arg)\n\tprint(args)\n\tprint(keywordArgs)\n\narg is a positional argument,\nargs is a tuple having other dynamic arguments,\nkeywordArgs is a dict having keyword and value\n\nfunction(&#039;hello&#039;, &#039;world&#039;,&#039;bye world&#039;, name=&#039;nitin&#039;, surname=&#039;sharma&#039;);\n# output\nhello\n(&#039;world&#039;, &#039;bye world&#039;)\n{&#039;name&#039;: &#039;nitin&#039;, &#039;surname&#039;: &#039;sharma&#039;}\n\n*args should be before **keywordArgs .\n\nSpecial parameters\ndef f(pos1, pos2, /, pos_or_kwd, *, kwd1, kwd2):\n      -----------    ----------     ----------\n        |             |                  |\n        |        Positional or keyword   |\n        |                                - Keyword only\n         -- Positional only\nNames of positional only parameter can be used in keyword only parameters.\ndef foo(name, /, **kwds):\n\treturn &#039;name&#039; in kwds\n \nfoo(1, name=&#039;nitin&#039;) # return True\nfoo(1, **{name: &#039;niitn&#039;}) # return True\nUnpacking Argument list\n# unpacks list\nargs = [1, 2]\nlist(range(*args))\n \n# unpacks dictionary\ngreet(**{&#039;name&#039;:&#039;nitin&#039;, &#039;age&#039;: 21});\nLambda Expressions\nis_even = lambda number: number % 2 == 0\n \ndef filter(array: list, predicate):\n  resArray = []\n  for item in array:\n    predicate(item) and resArray.append(item)\n  \n  return resArray\n \nprint(filter([1, 2, 3, 4], is_even))\nModule\nfunction.py\ndef greet():\n\tprint(&#039;Good morning&#039;)\nimport function\n \nfunction.greet() # Good morning\nfunction module is in global space where it is imported.\nItâ€™s like require in node. The module file is evaluated and all the functions and variables can be imported.\ndef fun():\n\timport function\n\tfunction.greet()\n \nfun() # import function in it&#039;s space and calls function.greet\n \nfunction.greet() # won&#039;t work as it&#039;s not in global space of the file\n# different types of import\n \nfrom function import greet\n \nfrom function import *\nAll of these imports get the imported function or variables from the module into the global scope of the file.\nimport function as fun\n \n# or \nfrom function import greet as printGreet\n \nfun.greet()\nprintGreet()\nPackages\nA package is a directory having modules or python files.\napp\nâ”œâ”€â”€ app.py\nâ””â”€â”€ src\n    â”œâ”€â”€ components\n    â”‚Â Â  â”œâ”€â”€ __init__.py\n    â”‚Â Â  â””â”€â”€ header.py\n    â””â”€â”€ layout.py\n\nimport app.app\n \napp.App()\nThis is one way to import module from package.\nfrom app.app import App\n \nApp()\nThis is another way to import module. It loads the module and puts the function in the namespace.\nImporting * from package\nfrom app import *\nThis doesnâ€™t work until we put __init__.py in the package.\napp\nâ”œâ”€â”€ __init__.py\nâ”œâ”€â”€ app.py\nâ””â”€â”€ src\n    â”œâ”€â”€ components\n    â”‚Â Â  â”œâ”€â”€ __init__.py\n    â”‚Â Â  â””â”€â”€ header.py\n    â””â”€â”€ layout.py\n\napp/__init__.py\n__all__ = [&quot;app&quot;]\n__all__ is a list having all the modules to make avaiable for  import * So, app  module gets imported the following works.\nfrom app import *\n \napp.App()\n&quot;&quot;&quot;\nThis is app layout\nThis is a header\nThis is App\n&quot;&quot;&quot;\nAbsolute imports and relative imports\nIn file layout.py\n\nAbsolute import\n\nfrom app.src.components.header import Header;\n \ndef Layout():\n  print(&#039;This is app layout&#039;)\n  Header()\n\nRelative import\n\nfrom .components.header import Header;\n \ndef Layout():\n  print(&#039;This is app layout&#039;)\n  Header()\nPython Namespaces and Scopes\nNamespace\nA namespace is a collection of unique names. Name in a namespace can be same as in another namespace.\nAttributes of a dictionary are also names in dictionary namespace.\n\nNote : The important thing to know about namespaces is that there is absolutely no relation between names in different namespaces\n\nTypes of Namespaces\n\nBuilt-in : python built-in\nGlobal : in a file or module\nLocal: in function or method\n\nClasses\nclass SomeClass:\n\tdef __init__(self, arg1, arg2):\n\t\tself.arg1 = arg1;\n\t\tself.arg2 = arg2;\n \n\tdef do_something(self):\n\t\tpass\n\t\t//do something\n \ninstance = SomeClass(1, 2)\ninstance.arg1\ninstance.arg2\ninstance.do_something()\nself has to pass as first argument to class function.\nClass function and Instance method\nThere is a difference between class function and instance method. An instance method is created using class function.\nWhen instance.do_something() is called, the attribute do_something is searched in the class that appears to be a function. instance is bounded with that class function and called with argument list.\nPrivate variables\nThere is way to define private methods and variables in python. Itâ€™s all convention here. Variable starting with _ are considered as private.\nclass S:\n\tdef __init__(self):\n\t\tself._private_attribute = 2\nCallable class instances\nA class instances can be called if class implements __call__ method.\nclass Counter:\n  def __init__(self, start=0):\n    self.count = start\n  \n  def __call__(self):\n    self.count += 1\n    return self.count\n \ncounter = Counter()\n \ncounter() # returns 1\ncounter() # returns 2\nIterator\nAn iterator has __next__() object method that gives the next value. for does the same thing, it gets iterator of the list and calls this __next__() method.\nTo create iterator, the class should have __next__() and __iter__() functions. for loop calls this __iter__() method to get iterator.\nclass Reverse:\n  def __init__(self, items: list):\n    self.items = items\n    self.index = len(items)\n  \n  def __iter__(self):\n    return self\n  \n  def __next__(self):\n    if self.index == 0:\n      raise StopIteration\n    \n    self.index -= 1\n    return self.items[self.index]\nFile IO\nf = open(&#039;file&#039;, &#039;r&#039;, encoding=&#039;utf-8&#039;)\nf.read()\nJSON in Python\nimport json\n \nnums = [1, 2, 3, 4]\n \njson.dumps(nums) # returns json of nums\n \njson.dump(nums, fileObject) # writes the nums json to file\nLoading Json files\njson.load(fileObject)\nErrors and Exceptions\nBasic Syntax\ntry:\n\t# some code that might raise an exception\nexcept Exception:\n\t# code that handles exception\nelse:\n\t# code that executed when try block gets executed successfully\nelse should follow all except clauses.\nexcept can take multiple type of Exceptions together\nexcept (ExceptionType1, ExceptionType2...):\nThere can be multiple except blocks.\nRaising Exceptions\nExceptions are raised using raise statement.\ntry:\n\traise Exception(&#039;demo&#039;)\nexcept Exception as ex:\n\tprint(ex)\nException Chaining\nAn exception can cause another exception to raised.\ntry:\n\topen(&#039;some-bad-file&#039;, &#039;r&#039;)\nexcept OSError:\n\traise RuntimeError(&#039;unable to process&#039;)\nThis flows like, due to OSError another exception RuntimeError occured.\nTo make an exception direct consquence of another, from clause is used\ntry:\n\topen(&#039;some-bad-file&#039;, &#039;r&#039;)\nexcept OSError as file_error:\n\traise RuntimeError(&#039;unable to process&#039;) from file_error\nNow, file_error is the direct cause of RuntimeError .\nThis chaining can be disabled by using from None\ntry:\n\topen(&#039;some-bad-file&#039;, &#039;r&#039;)\nexcept OSError:\n\traise RuntimeError(&#039;unable to process&#039;) from None\nSo, only RuntimeError will be raised.\nfinally block\ntry:\n\t# some code that might raise an exception\nexcept:\n\t# exception handling code\nfinally:\n\t# code that will run always\nSome points to remember:\n\nReturn value will be taken from the finally even though try returns some value.\nIf try raises some exception, except block handles that. If it doesnâ€™t handle, the finally block will get exectued and the exception will be re-raised.\nIf finally has  break, continue or return, exception wonâ€™t be re-raised.\n\nContext Managers\nContext managers allow allocate and release of resources. A common use case of context managers is to lock and unlock resources.\nA context manager basically has two functions\n\n__enter__\n__exit__\n\nwith statement calls this __enter__ method and assigns to as parameter.\nOn any exception, with calls __exit__ method and passes, type, value, and traceback.\nclass FileManager:\n  def __init__(self, filename):\n    self.filename = filename\n  \n  # context manager implementation\n  def __enter__(self):\n    self.fileobject = open(self.filename, encoding=&#039;utf-8&#039;)\n    return self.fileobject\n  \n  def __exit__(self, type, value, traceback):\n    print(&#039;Error:&#039;, type, value, traceback)\n    self.fileobject.close()\n \nwith FileManager(&#039;some-file&#039;) as file:\n\tfile.read()\n \nwith FileManager(&#039;some-file&#039;) as file:\n\tfile.some_undefined_method() # with will call __exit__ method\n \nIf __exit__ method returns True then the exception is handled by __exit__ otherwise with raises exception\nFor more information, read this.\nDecorators\nDecorators are function that wraps a function.\ndef decorator(function):\n  def wrapper():\n    print(&quot;doing something before calling actual function&quot;)\n    function()\n    print(&quot;doing something after calling actual function&quot;)\n  \n  return wrapper\n \ndef greet():\n\tprint(&quot;Good morning peeps!&quot;)\n \ngreet = decorator(greet)\n \ngreet()\nThe same thing can be done using decorator\n@decorator\ndef greet():\n\tprint(&quot;Good morning peeps!&quot;)\n \ngreet()\nDecorating function accepting argument\ndef decorator_with_args(function):\n  def wrapper(*args, **kwargs):\n    print(&quot;before calling actual function&quot;)\n    function(*args, **kwargs)\n    print(&quot;after calling actual function&quot;)\n  return wrapper\nIt is not always necessary that decorate should have wrapper function inside it. The main focus should be on that the decorator takes a function as an argument and returns a function.\n\nNote : With decorator, we are losing information about the actual function. We canâ€™t inspect that function being decorated. For example,\n\n@decorator\ndef greet():\n\tprint(&quot;hi&quot;)\n \ngreet.name # would give wrapper\ngreat.__doc__ # would give doc of wrapper\nTo solve this issue, we use python utility functools package.\nimport functools\n \ndef decorator(function):\n\t@functools.wraps(function)\n  def wrapper():\n    print(&quot;doing something before calling actual function&quot;)\n    function()\n    print(&quot;doing something after calling actual function&quot;)\n  \n  return wrapper\n \ndef greet():\n\tprint(&quot;Good morning peeps!&quot;)\n \ngreet.__name__ # would give greet\nfunctools.wraps is a decorator that takes the function being decorated. What it does is, copies the __doc__ and __name__ information of decorated function given to the wrapper function.\nDecorators with arguments\nA decorator is a function that takes function as a argument. To pass argument to the decorator, we need to make a function that returns decorator.\nimport functools\n \ndef repeat(times):\n  def decorator_repeat(fn):\n    @functools.wraps(fn)\n    def repeat_wrapper(*args, **kwargs):\n      for i in range(times):\n        fn(*args, **kwargs)\n    \n    return repeat_wrapper\n  return decorator_repeat\n \n@repeat(2)\ndef greet():\n\tprint(&quot;Good morning&quot;)\nClass decorators\nClass can be used to make stateful decorators. If we know how decorator works, it would be easy to understand how can be a class used to make decorators.\n@decorator\ndef fn():\n\tpass\nSo, decorator is actually a function which takes fn as argument, wraps this function in a wrapper and returns that wrapper and assigns to same variable fn.\nScenerio with class\n@SomeClassDecorator\ndef fn():\n\tpass\nSo, with this, an instance of the class is created. That instance is assigned to the variable fn. Ofcourse that instance should be callable. And we know how to make class callable.\nclass Countable:\n  def __init__(self, fun):\n    self.fun = fun\n    self.count = 0\n    functools.update_wrapper(self, fun)\n  \n  def __call__(self, *args, **kwargs):\n    self.count += 1\n    self.fun(*args, **kwargs)\n    print(f&quot;Called {self.fun.__name__} {self.count} times&quot;)\n@Countable\ndef greet():\n\tprint(&quot;hi&quot;)\n \n# is equivalent to\ndef greet():\n\tprint(&quot;hi&quot;)\n \ngreet = Countable(greet)\n \nfunctools.update_wrapper is used to update the introspetion details of the instance.\nWhy do we need Decorators\nRead more\nTo look\n\n  Decorators\n  Documentation Strings. Conventions to document.\n  sorted() or .sort() method\n  Inheritance\n  â€œCompiledâ€ Python files\n  Make interfaces.\n\n  Try to pass classes to function.\n  Try passing drived class to function accepting base class and access drive class methods\n\n\n  Decoratos\n\n  Caching values using decorators\n  Adding Information About Units\n  Validating JSON\n\n\n  dags and airflow (jobs and pipelines)\n\nPoints Noted\n\n\nPython code is synchronous in nature. If there is any blocking code in the infinite loop, the loop will wait for that blocking code to get executed.\nFor example\nwhile True:\n        data = connection.recv(1024)\n        print(&quot;From client: &quot;, data)\n        if not data:\n          print(&quot;Client closed the connection&quot;)\n          break\n        \n        connection.sendall(data)\n\n\nReferences\n\nStandard types\nPython 3 contents\nVariables\nNaming Convenstions\nWalrus operator\nGlob\nPathlib\nSocket Real Python\nPEP\n\n\nmulti-threading\nAsync IO\nReferences\nPandas\nopencv-python\nAsyc Server\npoetry"},"notes/tech/CPython/References":{"title":"References","links":[],"tags":[],"content":"References\ngithub.com/Pierian-Data/Complete-Python-3-Bootcamp"},"notes/tech/CPython/asyncio":{"title":"asyncio","links":[],"tags":[],"content":"It is a library in python to carry out asynchronous tasks. Python is asynchronous by nature.\nCoroutines\nA couroutine is a function that can suspended and resumed.\nFor example,\nasync def fetch(url: str):\n\tprint(&#039;sending request at &#039; + url)\n\tawait asyncio.sleep(2)\n\tprint(&#039;got response from  + url&#039;)\n\treturn url + &#039;?authToken=sdfsd&#039;\nfetch is a couroutine. If call coroutine like,\nfetch(&#039;tw.com&#039;)\n&lt;coroutine object fetch at 0x7f8a6dafba70&gt;\nIt is not called like an ordinary function. We need to run this coroutine using asyncio runner.\nasyncio.run(fetch(&#039;tw.com&#039;))\n&#039;sending request at tw.com&#039;\n&#039;got response from  + url&#039;\n&#039;tw.com#039;\nCoroutine function and coroutine object\nA coroutine function is a function where def is followed by async keyword.\nA coroutine object is an object which is generated when a coroutine function is invoked.\nTask\nA task is a wrapper around coroutine. A task schedules the coroutine concurrently.\nasync def main():\n\ttask = asyncio.create_task(fetch(&#039;tw.io&#039;))\n\tresult = await task\n\treturn result\n.create_task() method takes the coroutine and creates a task. It schedules the task immediately and task can only be completed once it is awaited.\n\nAwaiting a task runs all the tasks which can be finished within the finishing time of that task.  Look following example for that.\n\nasync def main():\n \n    tasks = []\n \n    urls = [{&#039;url&#039;: &#039;tw.com&#039;, &#039;time&#039;: 2}, {&#039;url&#039;: &#039;goo.com&#039;, &#039;time&#039;: 2}, { &#039;url&#039;: &#039;bitphile.com&#039;, &#039;time&#039;: 4}, {&#039;url&#039;: &#039;twi.com&#039;, &#039;time&#039;: 2}]\n \n    print(time.asctime())\n    \n    for url in urls:\n        print(url)\n        tasks.append(\n            asyncio.create_task(fetch(url[&#039;url&#039;], \n        url[&#039;time&#039;])))\n \n        print(&#039;--------&#039;)\n \n    task = tasks[0]\n \n    print(task)\n \n    await task\n \n    print(&#039;--------&#039;)\n \n    print(time.asctime())\nSo, awaiting first task will complete all the tasks having time of 2. Task of time 4 wonâ€™t get complete.\nIf we await tasks[3] then all the tasks will be completed getting that task be completed in the last.\nadd_done_callback\nThis is the method on task that gets invoked when the task is completed.\nasync def main():\n    task = asyncio.create_task(fetch(&#039;tw.io&#039;, 2))\n    task.add_done_callback(lambda arg: print(arg))\n    result = await task\n    return result\narg passed to lambda function is the task itself.\nAwaitables\nAnything that can be followed by await keyword.\nFor example,\n\ncoroutines\ntasks\nfutures\n"},"notes/tech/CPython/dis":{"title":"dis","links":[],"tags":["flash-card","python"],"content":"dis\nDisassembles methods, functions, classes and other objects in python.\n \nimport dis\n \ndef inc(x):\n    x += 1\n    return x\n \ndis.dis(inc)\n \n#  2           0 LOAD_FAST                0 (x)\n#              2 LOAD_CONST               1 (1)\n#             4 INPLACE_ADD\n#             6 STORE_FAST               0 (x)\n#\n#  3           8 LOAD_FAST                0 (x)\n#             10 RETURN_VALUE"},"notes/tech/CPython/event-loop":{"title":"event-loop","links":["tags/language","tags/programming-language","tags/python","tags/concurrent","tags/parallelism","tags/asyncio","tags/loop","tags/event-loop"],"tags":["language","programming-language","python","concurrent","parallelism","asyncio","loop","event-loop"],"content":"tags:languageprogramming-languagepythonconcurrentparallelismasyncioloopevent-loop\nEvent Loop\nWhat is an Event loop?\nAn event loop is a loop which has a list of tasks and it attempts to progress them in a sequence in each iteration of the loop. Along with the execute of the tasks, it also executes callbacks and IO handlings.\nAn event loop is the driver code that manages the cooperative multitasking.\nThis is the core part of asyncio library to create and run asynchronuous code in python.\nEvent loop implementations\nEvent loop is an abstract class which has to be implemented explicility according the requirements. There are several event loop implentations are avaiable.\n\nasyncio.DefaultEventLoopPolicy\nasyncio.ProactorEventLoop\nuvloop a high performance event loop implementation based on libuv library which is used in node.js for its event loop.\nasyncio.SelectorEventLoop\n\nLetâ€™s use uvloop implementation.\npip install uvloop\n \nimport asyncio\nimport uvloop\n \nuvloop.install()\n \nloop = asyncio.new_event_loop()\n \n#&lt;uvloop.Loop running=False closed=False debug=False&gt;\nEvent loop APIs\ncall_soon\nThis is the method of event loop to schedule a callback.\nFor instance,\ndef greet():\n    print(&quot;greetings&quot;)\n \nloop = asyncio.new_event_loop()\n \nloop.call_soon(greet)\n \nloop.run_forever()\nIt should print greetings.\nThere may times when we required to schedule a coroutine. Howeer, call_soon only takes function, not a coroutine. We can do by using asyncio.ensure_future which a method that creates a schedules a future wrapping the coroutine.\n \nasync def greet():\n    await asyncio.sleep(2)\n    print(&quot;greetings&quot;)\n \nloop.call_soon(asyncio.ensure_future, greet())\n \nloop.run_forever()\nSo, asyncio.ensure_future is scheduled as callbacks that takes greet() coroutine as an argument.\nReferences\n\n# Asyncio (superseded byÂ async)\nAsync IO Event Loop\n\n"},"notes/tech/CPython/executor":{"title":"executor","links":[],"tags":[],"content":"Executor is a class concurrent.futures.Executor which is used of offload function calls asynchronously. It immediately returns a future object which can be used to get the result.\n \ndef blocking_io(label: int, bytes: int):\n    time.sleep(4)\n    print(f&#039;--------&gt; FOR {label} &lt;---------&#039;)\n    \n    with open(&#039;/usr/share/dict/words&#039;, &#039;r&#039;) as f:\n        print(f.read(bytes))\n        \n    print(&#039;-----------------------------&#039;)\n \nexecutor = concurrent.futures.ThreadPoolExecutor()\n \nhandler = executor.submit(blocking_io, 1, 100)\n \nhandler.add_done_callback(\n                    lambda future: print(future))\nThere are different Executor available in concurrent.futures such as ThreadPoolExecutor to execute tasks in Threads pool.\nWe also have ProcessPoolExecutor for cpu bound tasks."},"notes/tech/CPython/generators":{"title":"generators","links":["tags/language","tags/computer","tags/programming-language","tags/program"],"tags":["language","computer","programming-language","program"],"content":"tags:languagecomputerprogramming-languageprogram\nGenerators\nSpecial kind of functions which returns a lazy iterator.\nFor example, a file reader that files lazily.\n&quot;file_reader.py&quot;\n \n \ndef file_reader(filename):\n    print(&#039;above with&#039;)\n    with open(filename, &#039;r&#039;) as file:\n        print(&#039;below with&#039;)\n        for line in file:\n            yield line\n            print(&quot;after yield&quot;)\n \nGenerator expressions\ncounter = (num in range(10))\nyield\nIt suspends the execution of the function and returns the yield value to the caller. On next generator method call, it continues executing after the yield.\nIt saves the state of the function.\nGenerator methods\n\n.send() : yield is not a statement. It is an expression.\n\ndef counter():\n\ta = 0\n\twhile True:\n\t\tb = (yield a)\n\t\tprint(b)\n\t\ta += 1\nif we do,\nc = counter()\nc.send(12)\n \noutputs:\n0\n12\n1\n12 is assigned to variable b.\n\n.throw() used to throw exception in generator.\n.close() to StopIteration\n\nSee Also\n\ncProfile\n"},"notes/tech/CPython/gil":{"title":"gil","links":[],"tags":["python"],"content":"Global Interpreter Lock (GIL)\nIt is a implementation in CPython which puts a lock on the interpreter in multithreading. Only a single thread can access the interpreter at a time.\nIn IO bound multithreading, threads wait for IO and meanwhile another thread can access the interpreter.\nIn the case of CPU bound multithreading, one thread doesnâ€™t leave the interpreter and executes behaves as single threaded as other threads have to wait for the interpreter to get released.\nPros\n\nMakes single threading faster as there is only lock, that is GIL lock, to maintain.\nGood for IO bound multithreading programs.\n\nCons\n\nNot good for CPU bound and combinations of both IO and CPU bound multithreading programs.\n\nReferences\n\nrealpython.com/python-gil/#what-problem-did-the-gil-solve-for-python\n"},"notes/tech/CPython/interned-objects":{"title":"interned-objects","links":[],"tags":["python","programming"],"content":"Interned Objects\nThese are the cached PyObjects  which python does for optimizations and performance.\n\nAll integers -5 to 255 are cached.\nStrings that contain ASCII letters, digits, or underscores only.\n\nFor example,\na = 1\nb = 1\n \na is b # True\n \nsys.getrefcount(a)\nsys.getrefcount(b)\n \n# both gives same result\nBoth a and b points to the same Python Object (PyObject).\nReferences\n\nrealpython.com/pointers-in-python/#a-note-on-intern-objects-in-python\n"},"notes/tech/CPython/itertools":{"title":"itertools","links":[],"tags":[],"content":"itertools\nProvides functions to deal with iterators. Most commonly used functions are,\n\ncount to create infinite sequence of numbers.\ncycle to create a cycling sequence with given collection.\n\nReferences\n\ndocs.python.org/3/howto/functional.html#the-itertools-module\n"},"notes/tech/CPython/logging/basicConfig":{"title":"basicConfig","links":[],"tags":["python","logs"],"content":"basicConfig\nIt is a method on logging module which tests the basic configurations for the root logger.\nIt can be only called once.\nlogging.basicConfig(level=logging.INFO)\nlogging.basicConfig(level=logging.DEBUG)\nSecond config wonâ€™t work.\nAlso, if any of the logging logging method such as .info, .warn etc is called, they implicitly calls this basicConfig with no arguments. So, basic configs canâ€™t be set after calling any of the logging method."},"notes/tech/CPython/logging/logrecord-attributes":{"title":"logrecord-attributes","links":[],"tags":["logs","python"],"content":"LogRecord Attributes\nWhile constructing the format for the log, we can provide LogRecord attributes such asctime, message etc.\nlogging.basicConfig(\n                    format=&quot;%(levelname)s - %(message)s - %(filename)&quot;\n                    )\nReferences\n\ndocs.python.org/3/library/logging.html#logrecord-attributes\n"},"notes/tech/CPython/memory-management":{"title":"memory-management","links":["notes/tech/CPython/pyobject"],"tags":["python","memory-management"],"content":"\nPython code gets converted into bytecode. Then bytecode is interpreted by the interpreter such as Cpython, jython etc.\nWhat is that entity that converts python code to bytecode?\n\nPython Memory Management\nMemory management differs in the implementation of python interpreter (CPython, Jython, etc).\nThis note includes memory management in CPython.\nEverything in Python is an object which has pyobject as internal structure.\nPython Application Memory\nPython application gets memory from OS. It has following roughly structure.\n\nNon-object Memory for python internals objects.\nObject Memory for user objects.\n\n\nMemory structure\nMemory allocated to Object memory is divided into:\n\nArena\nPools\nBlocks\n\nArena are big size memory allocated which has several pools storing same size blocks.\n\nEach pool maintain a double linked list to another pools.\nPool states\n\nusedpools\nfullpools\nfreepools (empty)\n\nusedpools contains references to pools which have some occupied blocks. fullpools has references to fully occupied pools. freepools have references to totally empty pools.\nPool block states\n\nfree\nuntouched\nallocated\n\nReferences\n\nrealpython.com/python-memory-management/#cpythons-memory-management\n"},"notes/tech/CPython/multi-threading":{"title":"multi-threading","links":[],"tags":[],"content":"MultiThreading in Python\nThreadPoolExecutor\nlogging.info(&quot;Submitting thread_function in pool&quot;)\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    executor.map(thread_function, [4])\nlogging.info(&quot;Submitted all the thread_function s &quot;)\nIt holds the control until all the tasks have been completed as we have used context manager with. It does implicit join.\nlogging.info(&quot;Submitting thread_function in pool&quot;)\nexecutor = ThreadPoolExecutor(max_workers=3)\nexecutor.map(thread_function, [4])\nlogging.info(&quot;Submitted all the thread_function s &quot;)\nIt runs the threads and control goes back to main thread. So, first log will come, then thread_function log and immediately last log."},"notes/tech/CPython/mutex-lock":{"title":"mutex-lock","links":[],"tags":["python","operating-system","deadlocks"],"content":"Lock\nTo make threading programming thread safe, we can use locking mechanism python provides. Python provides mutex locking system with threading.Lock.\nlock = threading.Lock()\n \nlock.acquier()\nchange_value()\nupdate_value()\nlock.release()\nIt can be done using with context manager.\nwith lock:\n    change_value()\n    update_value()\nExample\nclass Atom():\n    def __init__(self, value) -&gt; None:\n        self.value = value\n        self._lock = threading.Lock()\n \n    def inc(self, name) -&gt; int:\n        logging.info(&quot;Starting by thread %d&quot;, name)\n        logging.info(&quot;Acquire Lock by thread %d&quot;, name)\n        # with self._lock:\n        self._lock.acquire()\n        logging.info(&quot;Acquired Lock by thread %d&quot;, name)\n        copy = self.value\n        copy += 1\n        time.sleep(0.1)\n        logging.info(&quot;Update by thread %d&quot;, name)\n        self.value = copy\n        self._lock.release()\n        logging.info(&quot;Release Lock by thread %d&quot;, name)\n \n \nif __name__ == &quot;__main__&quot;:\n    format = &quot;%(asctime)s: %(message)s&quot;\n    logging.basicConfig(format=format, level=logging.INFO,\n                        datefmt=&quot;%H:%M:%S&quot;)\n \n    atom = Atom(1)\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        executor.map(atom.inc, list(range(1, 3)))\n \n    logging.info(&quot;MAIN : atom value is %d&quot;, atom.value)\nOutput\n13:48:02: Starting by thread 1\n13:48:02: Acquire Lock by thread 1\n13:48:02: Acquired Lock by thread 1\n13:48:02: Starting by thread 2\n13:48:02: Acquire Lock by thread 2\n13:48:02: Update by thread 1\n13:48:02: Release Lock by thread 1\n13:48:02: Acquired Lock by thread 2\n13:48:02: Update by thread 2\n13:48:02: Release Lock by thread 2\n13:48:02: MAIN : atom value is 3\n"},"notes/tech/CPython/names-not-variables":{"title":"names-not-variables","links":[],"tags":["python","programming"],"content":"Python Names\nPython doesnâ€™t have variables, instead they have names. There is no concept of variables in Python.\n\n\n                  \n                  Note \n                  \n                \n\nThe notion we have of a variable is something which stores value and it can be changed through out the program life cycle. There can be different implementation of in different programming languages. For instance in C, a variable owns a memory location and stores a value in it. If a new value is reassigned, the memory location remains the same but the value is overwritten on the same location.\n\n\nUnderstanding Names\nConsidering the following statement,\na = 1\nFollowing things happen when this statement executes.\n\nA PyObject is created. This PyObject canâ€™t be accessed by user program. It is internal to CPython implementation.\ntypecode is set as integer.\nvalue is set as 1.\nref count is increased by 1.\nname is a is bounded to this PyObject.\n\n\n\n\n                  \n                  Tip\n                  \n                \n\nsys.getrefcount method returns reference count to a PyObject.\n\n\nReferences\n\nrealpython.com/pointers-in-python/\n\n"},"notes/tech/CPython/opencv-python":{"title":"opencv-python","links":[],"tags":[],"content":"opencv-python\nReading images\nimg = cv2.imread(&quot;path/to/image/file.jpg&quot;)\n# img is a numpy array\n \nimg = cv2.imread(&quot;path/to/image/file.jpg&quot;, 0)\n# 0 =&gt; grayscale, 1 =&gt; bgr, -1 =&gt; preserve alpha values\nCreating images\ncv2.imwrite(&quot;path/to/file.jpg&quot;, old_img)\nResizing images\ncv2.resize(image, (width, height))\nDetecting face\nTo detect face, we need some models that will be used by the cv2 to process the image and detect face.\nFor example, haarcascade_frontalface_default.xml is a model\nface_cascade = cv2.cascadeClassifier(&quot;haarcascade_frontalface_default.xml&quot;)\nresults = face_cascade.detectMultiScale(img, scaleFactor=1.05, minNeighbors=5)\nresults is an 2d array of detected positions.\n[\n\t[x, y, width, height]\n]\nDrawing rectangle\nThis positions can be used to draw rectangle around faces\nfor x, y, w, h in results:\n\timg = cv2.rectangle(img, (x, y), (x + w, y + h), 3)\nShowing image\ncv2.imshow(&quot;Image&quot;, img)\ncv2.waitKey(0)  # wait for key &#039;0&#039; to press\ncv2.destroyAllWindows()  # after pressing &#039;0&#039; the image window will get closed\nwaitKey method wait for user key press on the image window. It takes parameter time in milliseconds to wait for the key. If itâ€™s 0 then wait forever.\nIt returns the ascii value key pressed.\nVideo processing from webcam\nvideo = cv2.VideoCapture(0) # 0 is the camera number\nhas_frame, frame = vide.read()\n \nvideo.realase() # release the camera control\nBlurring Images\nimg = cv.GaussianBlur(img, (21, 21), 0)\nReferences\n\nCascade Classifier\nopencv Haarcascades\n"},"notes/tech/CPython/package":{"title":"package","links":[],"tags":["python","programming"],"content":"Python Package\nA package is a directory containing python modules (or files) containing related set of code.\n__init__.py\n__init__.py is a file in a package which is added to make a directory a package. With old versions of python, it was mandatory to add this file. With newer versions, it is optional. However, for best practices, we should add it.\nFeatures\n\nIt can used to execute some code when the module is imported.\nWhen importing everything using from package import *, __init__.py can decide what all to import. By default nothing is imported when does like it. We need to explicitly add __all__ global variable in __init__.py.\n\n__init__.py\n__all__ = [&quot;format&quot;, &quot;my_math&quot;]\n\n\n                  \n                  Tip\n                  \n                \n\nAlthough, above can be accomplished using from .format import *.\n\n\n__init__.py\nfrom .format import *\nfrom .math import *\n\nfrom lib import *\n \nline(10)\n# &#039;----------&#039;"},"notes/tech/CPython/pickle":{"title":"pickle","links":[],"tags":["python","programming-language"],"content":"Pickle and Unpickle\nIt is used to serialize python objects into binary.\nimport pickle\n \npickle.dump(&quot;{&#039;a&#039;: &#039;b&#039;}&quot;, file) # file is file object\nwith open(&#039;./obj&#039;, &#039;rb&#039;) as file:\n    print(pickle.load(file))"},"notes/tech/CPython/poetry":{"title":"poetry","links":[],"tags":[],"content":"poetry\nInstalling dependencies\npoetry add --group test pytest\n"},"notes/tech/CPython/pyobject":{"title":"pyobject","links":["notes/tech/CPython/gil"],"tags":["python","programming"],"content":"PyObject\nA PyObject is a data structure used by CPython to store data objects in the memory. Everything in python is an object.\nPyObject structure\nCPython code has PyObject which is a grand object. It stores following information:\n\nReference Count: Total number of references for the actual data object.\nPointer to the actual data object.\n\nstruct {\n    ob_refcnt,\n    ob_type\n}\nReference count is used in freeing memory used by unused data objects. If reference counts becomes 0, the memory is freed.\nIn multithreading applications, multiple threads may try to access the same object which can cause inconsistencies in data and eventually program crash. To stop this, python has gil which locks the interpreter for a thread and other threads wait for the lock to get released.\nReference\n\nrealpython.com/python-memory-management\n"},"notes/tech/CPython/python-bindings/marshalling":{"title":"marshalling","links":[],"tags":["python"],"content":"Marshalling\nIt is a processing of converting a data residing in memory to a form that can be transmitted or stored. Mostly used with passing the data from one system to another system for remote calls. In this case, function call in another language.\nMarshalling vs Serialisation\n\nMarshalling is serialisation plus information about data type and codebase.\n\n\n\n                  \n                  Note: \n                  \n                \n\nCodebase: It stores the information about the implementation of the serialised object from where the receiver can know where to find the code to implement the object.\n\n\nWhy Marshalling in Python (Need to move into another file)\nTo create python bindings for c/c++, marshalling is used to move data between python and c/c++ because both languages use different mechanism to store data.\nReferences\n\nrealpython.com/python-bindings-overview/#marshalling-data-types\nMarshalling vs Serialisation at Stackoverflow\n"},"notes/tech/CPython/python-bindings/using-ctype":{"title":"using-ctype","links":[],"tags":["python"],"content":"Python binding using ctype\nAssuming there is already a shared c library, we can load this in python using following statement.\n \nshared_lib = &quot;path to shared library&quot;\nc_lib = ctype.CDDL(shared_lib)\nc_lib has all the function present in shared library as attributes.\nLetâ€™s see there is a function mul which takes an integer and a float and returns the product of it as float.\nBy default, ctype marshalling process passes all the data as integer so we need to explicitly tell ctype to use float.\nc_lib.mul(2, ctype.c_float(2.3)) ## FAILS\nThe above statement fails. The reason being the return type is float and marshalling is not happening correctly from ctype end. Again we need to specify it.\nc_lib.mul.restype = ctype.c_float\nNow, it will work."},"notes/tech/CPython/python-loops":{"title":"python-loops","links":[],"tags":["python"],"content":"Why python loops are slow?\n\nPythonâ€™s dynamically types nature.\n"},"notes/tech/CPython/re":{"title":"re","links":[],"tags":["python","programming"],"content":"re module\ngroup\nReturns group by the number.\nm = re.match(r&quot;(\\w+).(\\d+)&quot;, &quot;www.222&quot;) \n \nm.group(1) # &#039;www&#039;\nm.group(2) # &#039;222&#039;\ngroups\nReturns a tuple of groups found.\nm = re.match(r&quot;(\\w+).(\\d+)&quot;, &quot;www.222&quot;) # gives matches\nm.groups() # (&#039;www&#039;, &#039;222&#039;)\ngroupdict\nReturns a dictionary of named groups.\nm = re.match(r&quot;(?P&lt;words&gt;\\w+).(?P&lt;numbers&gt;\\d+)&quot;, &quot;www.222&quot;) \nm.groupdict()\n# {&#039;word&#039;: &#039;www&#039;, &#039;number&#039;: &#039;222&#039;}"},"notes/tech/aws/aws-cli-setup-sso":{"title":"aws-cli-setup-sso","links":[],"tags":[],"content":"Setting AWS CLI With SSO\nCreating SSO User\nAWS provides IAM Identity Center to create SSO User. Following are the steps to create SSO User.\n\nEnable IAM Identity Center if now enabled in the region.\n\n\n\n                  \n                  Only one IAM Identity Center is allowed in any region. \n                  \n                \n\n\nCreate User by following the steps.\nCreate a group and add the user into the group.\n\n\n\n                  \n                  it is always advisable to attach permissions to the groups instead of individuals users. \n                  \n                \n\n\nCreate a permission set. For now, keep PowerUserAccess for all access except user management.\nAdd a group/user to an account. Multi-account permissions &gt; AWS Accounts.\n\nClick button Assign users or groups.\nAdd user/group\nAttach Permission set just created.\n\n\n\nInstalling AWS CLI\nThis section describes about installing AWS CLI using command line for current user.\nEither you can use user folder or can create new folder for the installation. Assuming a new folder for installation aws-installer.\n\nSetup choices.xml file which tells where to get the aws files.\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;\n&lt;plist version=&quot;1.0&quot;&gt;\n  &lt;array&gt;\n    &lt;dict&gt;\n      &lt;key&gt;choiceAttribute&lt;/key&gt;\n      &lt;string&gt;customLocation&lt;/string&gt;\n      &lt;key&gt;attributeSetting&lt;/key&gt;\n      &lt;string&gt;/Users/username/aws-installer&lt;/string&gt;\n      &lt;key&gt;choiceIdentifier&lt;/key&gt;\n      &lt;string&gt;default&lt;/string&gt;\n    &lt;/dict&gt;\n  &lt;/array&gt;\n&lt;/plist&gt;\n\nDownload the .pkg file.\n\ncurl &quot;awscli.amazonaws.com/AWSCLIV2.pkg&quot; -o &quot;AWSCLIV2.pkg&quot;\n\n\nInstall the downloaded .pkg.\n\ninstaller -pkg AWSCLIV2.pkg \\\n            -target CurrentUserHomeDirectory \\\n            -applyChoiceChangesXML choices.xml\n\n\nLinking the binaries. By default installation doesnâ€™t modify the PATH to set aws binary location. We have to do it manually.\n\nsudo ln -s /Users/username/aws-installer/aws-cli/aws /usr/local/bin/aws\nsudo ln -s /Users/username/aws-installer/aws-cli/aws_completer /usr/local/bin/aws_completer\n\n\nVerify the installation.\n\n~/aws-installer\nâŸ© aws --version\naws-cli/2.15.20 Python/3.11.6 Darwin/23.3.0 exe/x86_64 prompt/off\n\nAWS CLI SSO Setup\nThis setup results in the creation of profile that will be used to access aws resources.\n\nRun the command aws configure sso.\nIt asks for few details such as name, region name, start url that can be fetched from IAM Identity Center.\n"},"notes/tech/aws/email-notification-lambda":{"title":"email-notification-lambda","links":[],"tags":["data-engineering","amazon","lambda","aws"],"content":"Sending Email from Lambda\nA lambda function can be triggered whenever something changes in S3 bucket. This note describes how to setup the lambda to send email on S3 Bucket update.\nCreate Lambda\nCreate lambda with following code.\n&#039;use strict&#039;;\n \nconst AWS = require(&#039;aws-sdk&#039;);\n \nconst sns = new AWS.SNS()\n \nexports.handler = (event, context, callback) =&gt; {\n    console.log(process.env);\n    // This reads the environment variable &#039;sns_topic_arn&#039;\n    var topic_arn = process.env.sns_topic_arn\n    var publishParams = {\n        TopicArn: topic_arn,\n        Message: JSON.stringify(event, null, 2)\n    };\n    sns.publish(publishParams, (err, data) =&gt; {\n        if (err) console.log(err)\n        else callback(null, &quot;Completed&quot;);\n    })\n};\n\n\n                  \n                  Note: Please use Node version 14.x \n                  \n                \n\nSet Trigger Source\nWhile creating lambda function, it provides the trigger source. Select S3 Bucket there.\nAmazon provides SNS (Simple Notification Service) resource for sending email notification. Letâ€™s setup SNS.\nSetting up SNS\nCreate a SNS topic with the name onS3BucketChange (name can be different).\nCreate a subscription where you need to add email address where to send the notification to.\nOnce the email address is added, a mail will be send to that email for subscription confirmation.\nThis should be enough for SNS. Letâ€™s go back to Lambda. As we are using SNS in lambda, we have to provide permission to lambda to use SNS feature.\n\n\n                  \n                  When you create lambda function, you get a role for that lambda created. \n                  \n                \n\nAdd SNS policy for lambda\nGo to roles window and select role for lambda. Add one more policy for lambda role AmazonSNSFullAccess.\nThatâ€™s it. Lambda can access SNS feature.\nConfigure Environment variable\nWe have used sns_topic_arn environment variable in the code. Go to Environment variables tab and add environment variable by putting ARN for SNS topic that we get from Topic dashboard.\nWe are good to go for testing."},"notes/tech/dask/chunking":{"title":"chunking","links":[],"tags":["python","libraries"],"content":"Chunks\n\n\n How chunking across row or column makes processing efficient? (@2024-06-20)\n\n\n What is difference between chunks and partitions? (@2024-05-24)\n\n"},"notes/tech/dask/compute-function":{"title":"compute-function","links":["notes/tech/dask/get-method"],"tags":["libraries"],"content":"compute function\nIt executes the dask collection (dask graph) by the default or given scheduler or get-method function.\ndask.compute(da_1, da_2, scheduler=&quot;threads&quot;)\nAs it takes multiple dask collections, it reuses the nodes in both graphs. For instance,\nx = da.arange(10)\ny = (x + 1).sum()\nz = (x + 1).mean()\n \ndask.compute(y, z)\nSo, node (x + 1) will be reused."},"notes/tech/dask/dask-delayed":{"title":"dask-delayed","links":[],"tags":["python","libraries"],"content":"Dask Delayed\nIt is a feature of dask where it makes the function call lazy.\nimport dask\n \n@dask.delayed\ndef f():\n    return 1\nf becomes lazy. Calling it wonâ€™t give back the result. We need to explicitly call compute .\ndelayed = f()\ndelayed.compute()\nDask actually creates a task graph of function calls which can be chained together."},"notes/tech/dask/dask-graph":{"title":"dask-graph","links":["notes/tech/dask/get-method"],"tags":["libraries","distributed-computing"],"content":"Dask Graph\nA dask graph is a DAG of tasks as nodes and edges as passing intermediatery data.\nA dask graph is represented by using python basic building blocks such as tuple, list, dict etc.\ngraph = {\n         &#039;x&#039;: 2,\n         &#039;y&#039;: (add, &#039;x&#039;, 2)\n}\nThese dask graphs are submitted to scheduler to for the execution. Different scheduler may execute these task graphs in differently based on the optimization techniques they have.\nA dask graph can be evaluated/executed using get method."},"notes/tech/dask/dask-so-far":{"title":"dask-so-far","links":[],"tags":["python","libraries"],"content":"Dask So Far\n\nMakes dealing with big data in distributed and parallel manner.\nUses underlying numpy arrays for dask arrays.\n\n\n\n                  \n                  Note: \n                  \n                \n\nFor dataframes and other data structures, some different underlying data structure can be there.\n\n\n\nAll the computations are lazy. Applying computation methods such as mean, max etc wonâ€™t get executed immediately until explicitly computation is called. (using compute)\n\nd_array.max().compute()\n\nCreates a task graph for the computation operations like spark.\n\nd_array.visualize()\n"},"notes/tech/dask/get-method":{"title":"get-method","links":[],"tags":["distributed-computing"],"content":"get\nA get is used to execute a task graph. This is provided by different schedulers.\n\ndask.threaded.get\ndask.multiprocessing.get\ndask.distributed.Client.get\n\ndsk = {\n       &quot;x&quot;: 1,\n       &quot;y&quot;: (operator.add, &#039;x&#039;, 1)\n}\ndask.threaded.get(dsk, &#039;y&#039;)\ndask.threaded.get(dsk, &#039;x&#039;)\nReferences\n\ndocs.dask.org/en/stable/scheduler-overview.html\n"},"notes/tech/dask/local-cluster":{"title":"local-cluster","links":[],"tags":["libraries","python","distributed-computing"],"content":"LocalCluster\nDask is designed to do no difference in running it on single machine or a distributed cluster.\nLocalCluster is designed to imitate a distributed cluster on a single machine.\nfrom dask.distributed import Client, LocalCluster\n \ncluster = LocalCluster(n_workers=8, threads_per_worker=8)\n \nclient = Client(cluster)"},"notes/tech/dask/map-blocks":{"title":"map-blocks","links":[],"tags":["libraries"],"content":"map_blocks\nThis function/method maps through the blocks in a dask array and apply a customized function on it.\nmap_blocks function from straight dask.array can take multiple dask arrays.\n \nda.map_blocks(lambda x: x * 2, data)\n \n# or\n \ndata.map_blocks(lambda x: x * 2)\nlambda gets numpy.ndarray as chunks.\n\n\n                  \n                  Note\n                  \n                \n\nChunks passed to the lambda are not fully evaluated. For instance, accessing any other element rather than first one will give error. chunk[2] will give error whilst chunk[0] wonâ€™t.\n\n\nExamples\n\nMap on two dask arrays of different block numbers\n\nd5 = da.from_array(np.random.random(10), chunks=2)\nd6 = da.arange(25, chunks=(5))\n \nres = da.map_blocks(lambda x, y: np.array([x.max(), y.max()]), d5, d6)\n \nres.compute()\nlambda returns chunk with max values from two corresponding chunks. So, res.compute() returns max values from corresponding chunks of two dask arrays."},"notes/tech/dask/scheduler":{"title":"scheduler","links":["notes/tech/dask/local-cluster"],"tags":["python","libraries"],"content":"Scheduler\nA scheduler is something that schedules the tasks graph on parallel hardware. There are two types of scheduler,\n\nSingle machine\nDistributed scheduler\n\nSingle machine\nSingle machine scheduler uses machines threads and processes.\ndask.config.set(scheduler=&quot;processes&quot;) # it uses multiprocessing scheduler inside single machine\n \ndask.config.set(scheduler=&quot;threads&quot;)\nAbove configs are set globally. However, they can be used with context manager.\nwith dask.config.set(scheduler=&quot;threads&quot;):\n    d.compute()\nWhile calling compute, we can provide scheduler.\nd.compute(scheduler=&quot;processes&quot;)\nDistributed Scheduler\nDistributed scheduler uses workers to schedule the job on. It can be setup on local single machine or a multi node cluster.\n\n\n                  \n                  Todo\n                  \n                \n\n\n\n single machine\n\n threads\n processing\n single thread\n\n\n\n single machine\n\n threads\n processing\n single thread\n\n\n\n local cluster on single machine\n\n threads\n processing\n single thread\n\n\n\n kubernetes cluster\n\n"},"notes/tech/elk-stack":{"title":"elk-stack","links":[],"tags":["logs","data"],"content":"ELK Stack\nObservations\n\nLogs management and analytics\nELK stack was first introduced by Elastic and was open source. However, the changed it to licensed from open source\nAWS launched OpenSearch as replacement for ELK which was open source.\nLogstash collects the logs data, applies transformations passes onto elasticsearch which indexes and stores the data. Kibana is then used to visualize the data.\n\nElasticsearch\nText search and analytics engine based on apache Lucene open source search engine.\nLogstash\nLogs aggregator and data collector from various sources, execute different transformations and enhancements and ship the data to various supported destinations.\nKibana\nVisualization tool works on top of elasticsearch."},"notes/tech/environment-modules/modules":{"title":"modules","links":[],"tags":["libraries"],"content":"It setups the libraries and paths. Custom configurations that setups the paths, can be written by the user be used with module load command.\nIt already comes on Linux system and can be initialized using,\nsource /usr/local/Modules/init/&lt;shell&gt;\n\nshell can be any supported shell."},"notes/tech/gitlab/ci":{"title":"ci","links":[],"tags":[],"content":"Gitlab CI\n\nA gitlab ci contains stages and job. Jobs are grouped in stages and run in parallel.\n\nstages:\n  - build\n  - deploy\n \nbuild-job:\n  stage: build\n  image: node\n  script:\n    - npm install\n    - npm run build\n  artifacts:\n    paths:\n      - &quot;build/&quot;\n \npages:\n  stage: deploy\n  script:\n    - mv build/ public/\n  artifacts:\n    paths:\n      - &quot;public/&quot;\n \n\nstages directive defines the sequence of jobs. There are two jobs with their stage mentioned in their definition.\n\nReferences\n\ndocs.gitlab.com/ee/ci/quick_start/tutorial.html\n"},"notes/tech/googleapis/delete-rows":{"title":"delete-rows","links":[],"tags":["spreadsheet","gsheets"],"content":"Rows can be deleted using batchUpdate API.\n \nresource = {&quot;requests&quot;:\n                [\n                    {&quot;deleteDimension&quot;: {\n                        &quot;range&quot;:{\n                            &quot;sheetId&quot;:0,\n                            &quot;dimension&quot;:&quot;ROWS&quot;,\n                            &quot;startIndex&quot;:20,\n                            &quot;endIndex&quot;:21}\n                        }\n                    }\n                ]\n            }\n \nbatchReqBody = {spreadsheetId: spreadSheet, resource: resource}\nclient.spreadsheets.batchUpdate(batchReqBody)\nstartIndex specify the range start. This row is not deleted. For instance, the range is startIndex = 16 and endIndex = 20, it will delete rows 17, 18, 19, and 20."},"notes/tech/jax/jax":{"title":"jax","links":[],"tags":["libraries","dic"],"content":"It uses MLIR."},"notes/tech/linux/accessing-usb-drives":{"title":"accessing-usb-drives","links":[],"tags":["linux","terminal","cli","storage"],"content":"List the drives using command,\n\nlsblk - list block devices which lists all the block devices available in the system.\nblkid - list and print block devices attributes\nfdisk - deals file disk partitions and manages them.\nudiskctl\n\nMount and Unmount\n\nMounting is used to mount the external storage device to the local file system. It makes external storage devices accessible within the system.\n\nmount /dev/sda /storage/ultafit\n\n\n\nSyntax - mount device directory\n\n\nUnmounting remove the external storage mount from the directory and the external storage no longer be accessed inside the file system.\n\n\numount /storage/ultafit\n"},"notes/tech/linux/automount-drives":{"title":"automount-drives","links":[],"tags":["storage","linux"],"content":"Auto mount USB drives\nThis can be done by fstab (filesystem table) system file. It can be found at /etc/fstab. It has all the drive information to mount on boot.\nWe can append our usb drive information to this file.\nPARTUUID=494aac82-01 /home/nitin-remote/storage/hp auto defaults,nofail,uid=nitin-remote,gid=nitin-remote,x-systemd.automount 0 2\n\n\n\nWe can provide usb drive path using /dev/sda etc but to play safe and ignoring where we insert the usb into which port, they have a UUID that we can use to identify them.\n\n\nauto flag is tell auto mount.\n\n\nReferences\n\nwww.baeldung.com/linux/automount-usb-device\nwww.redhat.com/sysadmin/etc-fstab\n\n"},"notes/tech/linux/resources":{"title":"resources","links":[],"tags":["linux","resources"],"content":"\nwww.actualtechmedia.com/wp-content/uploads/2017/12/CUMULUS-NETWORKS-Linux101.pdf\n"},"notes/tech/numpy/broadcasting":{"title":"broadcasting","links":[],"tags":["python","libraries"],"content":"Broadcasting\n\nUsed in arithmetic operations in numpy arrays.\nProvides a means of vectorizing array operations so that the looping happens inside C instead of python.\nApplying arithmetic operations on arrays with different shapes and dimensions, the smaller one has to be stretched to make compatible with larger one.\n\nBroadcasting rules\n\nCompatibility checks starts from right to left.\nTwo dimensions are compatible only when\n\nboth are same\neither one of them is 1\n\n\n\ndims1 = (1,)\ndims2 = (1,)\nBOTH ARE COMPATIBLE\n------------\n\ndims1 = (1,2)\ndims2 = (1,1)\nBOTH ARE COMPATIBLE as there is 1 in last dimension\n------------\n\ndims1 = (1,2)\ndims2 = (1,3)\nNOT COMPATIBLE as dimensions differ and one of them is not 1\n------------\n\ndims1 = (4, 1)\ndims2 = (3,)\nBOTH ARE COMPATIBLE\nas\n       4 x 1\n       1 x 3\nresult 4 x 3\n\n\nitems = np.arange(10).reshape((2, 5))\nitems.shape # (2, 5)\n \nmul = np.array([[2, 3], [2, 3]])\nmul.shape # (2, 2)\n \nitems * mul\n# ValueError: operands could not be broadcast together with shapes (2,5) (2,2)"},"notes/tech/numpy/indexing":{"title":"indexing","links":[],"tags":["libraries","python"],"content":"Indexing on ndarrays\n\nThree types of indexing\n\nbasic\nadvanced\nfield\n\n\n\n"},"notes/tech/numpy/internal-organization":{"title":"internal-organization","links":["notes/tech/numpy/view"],"tags":["python","libraries"],"content":"Internal Organization\nNumpy arrays are organized using a set of two parts.\n\nMetadata\nRaw array data (or data buffer)\n\nMetadata is the information about the data buffer. It has information about shape, size, data type etc. A numpy array object is the metadata which points to the data buffer.\nAny operations on the data buffer results in making new metadata object which points to the same data buffer. This is usually called view of the original array (data buffer). This way, same data buffer can be used for different representation.\nHowever, some numpy operation may return new copy of the data buffer.\nReferences\n\nnumpy.org/doc/stable/dev/internals.html\n\n"},"notes/tech/numpy/interp":{"title":"interp","links":[],"tags":["python","libraries"],"content":"np.interp\nThis method interpolate or determines the value of position for a given function.\nFunction here means two arrays data points of equal length. np.interp is then determines the corresponding value of given position with respect to the function (two arrays).\nx = np.array([1, 2, 3, 4])\ny = np.array([1, 2, 3, 4])\n \nnp.interp(0, x, y)\n# 1.0\n# obiuously \n \n# calculating 100 points between x min and max\nx_interp = np.linspace(x.min(), x.max(), 100)\n \ny_interp = np.interp(x_interp, x, y)\n \n# y_interp will be values corresponding to x_interp value with respect to function x, y"},"notes/tech/numpy/newaxis":{"title":"newaxis","links":[],"tags":["python","libraries"],"content":"np.newaxis\nAdds new dimension to the array.\na = np.array([1, 2])\n \na[np.newaxis].shape\n# (1, 2)\nThe same can be done using None. None and np.newaxis are same."},"notes/tech/numpy/pad":{"title":"pad","links":[],"tags":["python","libraries"],"content":"np.pad\nPads the numpy array. By defaults it pads with constant value as shown below.\na = np.arange(5)\n \n# array([0, 1, 2, 3, 4])\nnp.pad(a, (1, 1))\n \n# array([0, 0, 1, 2, 3, 4, 0])\nLooking at the tuple (1, 1) says 1 left and 1 right.\nFor 2d array,\na = np.arange(4).reshape(2, 2)\n \n#array([[0, 1],\n#       [2, 3]])\n \nnp.pad(a, ((1, 1), (1, 1)))\n# array([[0, 0, 0, 0],\n#      [0, 0, 1, 0],\n#      [0, 2, 3, 0],\n#      [0, 0, 0, 0]])\nAgain looking at the tuple for each dimension\n\nfor dimension 0, it is one top and one bottom\nfor dimension 1, it one left and one right.\n"},"notes/tech/numpy/see-also":{"title":"see-also","links":[],"tags":["later","misc"],"content":"See also\n\n C-order indexing\n"},"notes/tech/numpy/transpose":{"title":"transpose","links":["notes/tech/xarray/xarray-so-far"],"tags":["libraries","python"],"content":"Transpose\nSwitch the axis in array. Or switch the dimension for xarray.\nxd = xr.DataArray(np.arange(4).reshape((2,2,1)), dims=(&quot;time&quot;, &quot;baseline&quot;, &quot;pol&quot;))\n \nprint(xd)\nprint(xd.T)\n&lt;xarray.DataArray (time: 2, baseline: 2, pol: 1)&gt; Size: 32B\narray([[[0],\n        [1]],\n\n       [[2],\n        [3]]])\nDimensions without coordinates: time, baseline, pol\n&lt;xarray.DataArray (pol: 1, baseline: 2, time: 2)&gt; Size: 32B\narray([[[0, 2],\n        [1, 3]]])\nDimensions without coordinates: pol, baseline, time\n\nOnly the dimensions are switched. So, accessing the element will be different after the transpose."},"notes/tech/numpy/vectorize":{"title":"vectorize","links":[],"tags":["python","libraries"],"content":"Vectorize\nIt is used to create a vectorize function on the numpy array.\nBasic syntax\nnp.vectorize(user_def_fn)\nIt returns a vectorize function which access numpy array. Numpy then maps the array elements over the function.\nnums = np.arange(10)\nsquare = np.vectorize(lambda x: x * x)\nsquare(nums)\n# array([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])\nsignature\nBy default function gets scaler value from the array and returns the scaler value as output. However, we can change this behavior using optional parameter signature.\nA signature specifies what nd array the function would accept and returns as output.\nnums = np.arange(4).reshape((2, 2))\n# array([[0, 1],\n#       [2, 3]])\n \nsquare_array = np.vectorize(lambda sub_array: sub_array ** 2, signature=&quot;(n)-&gt;(n)&quot;)\nsquare_array(nums)\nSignature (n) -&gt; (n) specifies that the function would take 1d array as input and returns 1d array as output. Signature specifies the shape of input and output array.\n(n, k) -&gt; (n) means the function takes 2d array as input and returns 1d array as output. If the function returns 2d array as output, the signature should be (n, k) -&gt; (x, y).\n\n\n                  \n                  Note: \n                  \n                \n\nThe variables used in input and output signature has not to be same by names.\n\n\nSignature (n),(n)-&gt;(),() specifies that the vectorized function takes two 1d arrays and return two scaler values"},"notes/tech/numpy/view":{"title":"view","links":[],"tags":["libraries","python","programming"],"content":"View\nWhen a new array is created after some operation on existing numpy array, a view is created which points to the same raw array data (data buffer).\nSome operation returns view and some returns array pointing to whole new data buffer.\nIt increase the performance of numpy as no new data is getting created.\nHowever, it has some implications as shown below.\na = np.arange(5)\n#array([0, 1, 2, 3, 4])\n \nb = a[:2]\n#array([0, 1])\n \na[0] = 133\n \n#array([133,   1,   2,   3,   4])\n \nb\n# array([133,   1])\nSo, changing the view modifies the original data buffer.\nReferences\n\nnumpy.org/doc/stable/glossary.html#term-view\n"},"notes/tech/singularity/container-fs":{"title":"container-fs","links":[],"tags":["inferences","docker"],"content":"Container File System\n\n$SINGULARITY_ROOTFS environment variables has the path to the container file system.\n"},"notes/tech/singularity/definition-file-sections":{"title":"definition-file-sections","links":[],"tags":["infrastructure","docker"],"content":"Sections\n\nmultiple sections with the same name can be included and will be appended to one another during the build process.\n\n%setup\n\nExecutes the commands on the host system after base operating system is installed in the container.\nThis section is usually not used as it is dangerous.\n\n%files\n\nThis section is used to copy files from host system to container. Also, from previous stage specified.\n%files section is executed before %post so that files are available while build process in %post.\nsource can be a path from host system or path in previous stage if specified. destination is the path in the container.\n\nif destination is not specified, source path is used for destination.\n\n\n\n# copying file from host to container\n%files\n/from-host /to-container\n\n# copying file from previous stage to current container\n%files prev_stage\n/from /to-container\n"},"notes/tech/singularity/defintion-file-header":{"title":"defintion-file-header","links":[],"tags":["infrastructure","docker"],"content":"Header\nContains information about the base operating system to be used to build container. It is written on the top of def file.\nBootstrap key needs to have a bootstrap agent that will be used to create base operating system.\nâ€œ"},"notes/tech/xarray-cuda/xarray-cuda":{"title":"xarray-cuda","links":[],"tags":["libraries","gpu","python"],"content":"Observations\n\nCuda Array works same as numpy array.\nHowever, some of the functionality where xarray more specific to numpy array, operations on cuda arrays give error.\n\nFor example, xarray.apply_ufunc with vectorize param give error when operated on cuda arrays as internally xarray.apply_ufunc -&gt; dask.array.gufunc.apply_gufunc uses np.vectorize\nAlso, passing cuda vectorize function to xarray.apply_ufunc doesnâ€™t work. Looks like they have not implemented it yet.\npassing signature is not yet implemented for cuda vectorize\n\n\n"},"notes/tech/xarray/apply_ufunc-dask-vectorize":{"title":"apply_ufunc-dask-vectorize","links":["notes/tech/xarray/apply_ufunc-dask"],"tags":["libraries","python"],"content":"Vectorize Dask Xarray\nVectorization for dask xarray works only when we give dask=parallelized parameter.\nFrom the note we saw that dask.array.apply_gufunc provides chunk blocks to custom function. When vectorize is provided, numpy vectorization happens on the custom function.\nSo the flow of execution is as follows,\nxarray.apply_ufunc\n        |\n        V\ndask.array.apply_gufunc\n        |\n        V\nnp.vectorize\n        |\n        V\ncustom function\n    ```\n\nFor example,\n\n```python\ndata\n# &lt;xarray.DataArray &#039;air&#039; (time: 2, lat: 5, lon: 5)&gt; Size: 400B dask.array&lt;rechunk-merge, shape=(2, 5, 5), dtype=float64, chunksize=(1, 5, 5), chunktype=numpy.ndarray&gt;\n\n\ndef add_wrapper(d, value):\n     print(&#039;------------------- START ---------------------&#039;)\n     print(type(d))\n     print(d.shape)\n     print(d)\n     print(&#039;-------------------- END ------------------------&#039;)\n     return d + value\n\n\nxr.apply_ufunc(add_wrapper, \n               data, \n               1, \n               input_core_dims=[[&quot;lon&quot;], []], \n               output_core_dims=[[&quot;lon&quot;]], \n               dask=&quot;parallelized&quot;, \n               vectorize=True).compute()\n\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (1,)\n# [1.]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [242.1  242.7  243.1  243.39 243.6 ]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [243.6 244.1 244.2 244.1 243.7]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [253.2  252.89 252.1  250.8  249.3 ]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [269.7 269.4 268.6 267.4 266. ]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [272.5 271.5 270.4 269.4 268.5]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [241.2 242.5 243.5 244.  244.1]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [243.8  244.5  244.7  244.2  243.39]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [250.   249.8  248.89 247.5  246.  ]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [266.5 267.1 267.1 266.7 265.9]\n# -------------------- END ------------------------\n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (5,)\n# [274.5  274.29 274.1  274.   273.79]\n# -------------------- END ------------------------\n# &lt;xarray.DataArray &#039;air&#039; (time: 2, lat: 5, lon: 5)&gt; Size: 400B\n# array([[[242.2 , 243.5 , 244.5 , 245.  , 245.1 ],\n#         [244.8 , 245.5 , 245.7 , 245.2 , 244.39],\n#         [251.  , 250.8 , 249.89, 248.5 , 247.  ],\n#         [267.5 , 268.1 , 268.1 , 267.7 , 266.9 ],\n#         [275.5 , 275.29, 275.1 , 275.  , 274.79]],\n\n#        [[243.1 , 243.7 , 244.1 , 244.39, 244.6 ],\n#         [244.6 , 245.1 , 245.2 , 245.1 , 244.7 ],\n#         [254.2 , 253.89, 253.1 , 251.8 , 250.3 ],\n#         [270.7 , 270.4 , 269.6 , 268.4 , 267.  ],\n#         [273.5 , 272.5 , 271.4 , 270.4 , 269.5 ]]])\n\nVectorized custom functions is called with 1d array of dim lon 10 times because there are two chunks (1, 5, 5) and (1, 5, 5) and we have broadcasted on dims time and lat. So, function is called with 1d array of 5 elements 10 (1, 5) + (1, 5) = 10\nReferences\n\ntutorial.xarray.dev/advanced/apply_ufunc/dask_apply_ufunc.html#automatic-vectorizing\n"},"notes/tech/xarray/apply_ufunc-dask":{"title":"apply_ufunc-dask","links":[],"tags":["python","libraries"],"content":"apply_ufunc with dask arrays\napply_ufunc handles dask xarray in same manner as it does with numpy xarray. However, it requires an dask argument` to work with dask xarrays.\nxr.apply_ufunc(xda, dask=&#039;allowed&#039;)\ndask parameter\n\ndask=forbidden gives error when dask xarray is given.\ndask=allowed is when given function is able to handle dask array as input.\ndask=parallelized is when function is not able to handle dask array and want xarray.apply_ufunc to make function work with dask arrays.\n\nWorking with non-dask functions\nWhen function doesnâ€™t support dask xarrays, we can use dask=parallelized argument for xr.apply_ufunc.\nHow it works?\n\nxarray.apply_ufunc uses dask.array.apply_gufunc internally.\nunderlying dask array is passed to apply_gufunc.\napply_gufunc then passes each chunk block numpy array to custom function.\ndask stitches back all the output chunk array from custom function.\nxarray then populates meta information to the output array.\n\n\n\n                  \n                  Warning\n                  \n                \n\nIf data is chunked on a core dimension the custom function is working on, executing such operations give error.\n\n\nFor example,\nds.air\n# &lt;xarray.DataArray &#039;air&#039; (time: 2920, lat: 25, lon: 53)&gt; Size: 31MB dask.array&lt;open_dataset-air, shape=(2920, 25, 53), dtype=float64, chunksize=(100, 25, 53), chunktype=numpy.ndarray&gt;\n \n \nxr.apply_ufunc(mean, ds.air, input_core_dims=[[&quot;time&quot;]], kwargs={&quot;axis&quot;: -1}, dask=&quot;parallelized&quot;)\n \n# dimension time on 0th function argument to apply_ufunc with dask=&#039;parallelized&#039; consists of multiple chunks, but is also a core dimension. To fix, either rechunk into a single array chunk along this dimension, i.e., ``.chunk(dict(time=-1))``, or pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` but beware that this may significantly increase memory usage.\nData is chunked across time dimension and providing time as core dimension raises the error. The data has to be re-chunked if we donâ€™t want this error. This can be done by providing argument for dask.apply_gufunc.\n  def mean(d, **kwargs):\n     print(&#039;------------------- START ---------------------&#039;)\n     print(type(d))\n     print(d.shape)\n     print(&#039;-------------------- END ------------------------&#039;)\n     return np.mean(d, **kwargs)\nxr.apply_ufunc(mean,\n               ds.air,\n               input_core_dims=[[&quot;time&quot;]], \n               kwargs={&quot;axis&quot;: -1}, \n               dask_gufunc_kwargs={&quot;allow_rechunk&quot;:True},\n               dask=&quot;parallelized&quot;)\nHowever, this is consumes more memory.\nTo determine the effect of the function on the input, dask first runs the function with dummy data as shown below.\nxr.apply_ufunc(mean, \n               ds.air, \n               input_core_dims=[[&quot;lon&quot;]], \n               kwargs={&quot;axis&quot;: -1}, dask=&quot;parallelized&quot;)\n \n# ------------------- START ---------------------\n# &lt;class &#039;numpy.ndarray&#039;&gt;\n# (1, 1, 1)\n# -------------------- END ------------------------\n#&lt;xarray.DataArray &#039;air&#039; (time: 2920, lat: 25)&gt; Size: 584kB dask.array&lt;transpose, shape=(2920, 25), dtype=float64, chunksize=(100, 25), chunktype=numpy.ndarray&gt;\nReferences\n\ntutorial.xarray.dev/advanced/apply_ufunc/dask_apply_ufunc.html#understanding-what-s-happening\n"},"notes/tech/xarray/apply_ufunc":{"title":"apply_ufunc","links":[],"tags":["libraries","python"],"content":"apply_ufunc\nXarray method that applies custom function on underlying array in xarray. The underlying array can be a numpy or a dask array (need verification).\nHow it works?\n\nWhen xarray DataArray is given to apply_ufunc, it gets the underlying array, and passes to the custom function. Custom function returns the result array which again wrapped by apply_ufunc and returned to us.\nWhen xarray DataSet is given, it loops over the data variables(DataArray), passes each of the data array to the custom function, gets the result array and wraps in Data Array and then in Data set and returns.\nAll the input DataSet/DataArray metadata gets populated on the result by apply_ufunc.\nBy default, attributes are not populated on the result but can retained using keep_attrs=True.\n\n\n\n                  \n                  Note \n                  \n                \n\nvectorize=True doesnâ€™t work when input array is a dask array.\n\n\nSyntax\nIt is the minimal syntax\nxr.apply_ufunct(custom_function, dataset/dataarray, pos_args, kwargs={}, keep_attrs=True)\ninput_core_dims\n\nThis parameter takes the dimensions to which the function would be operated on.\nspecified dimensions are moved to last. for example, input_core_dims=[[&quot;time&quot;]] will be moved to last dimension.\n\nNotebooks\n\n/Users/nitinsharma/TW-Projects/ska/exploration/xarray/apply_ufunc.ipynb\n\n\nReferences\n\ntutorial.xarray.dev/advanced/apply_ufunc/simple_numpy_apply_ufunc.html\n\nincreasing the order by 1 vectorization"},"notes/tech/xarray/core-dimensions":{"title":"core-dimensions","links":["notes/tech/xarray/apply_ufunc"],"tags":["data","python","libraries"],"content":"Core Dimensions\nFundamental dimensions over which an operation is defined.\nWhy core dimensions?\n\napply_ufunc is used to create generalized operations on xarray data.\nCore dimensions specifies which are the main dimensions the function will be operating with.\nCore dimensions are moved to last index (transposed).\n\nFor example,\nda.shape\n# (2, 3, 4)\n \ndef func(d):\n    print(d.shape)\n    return d\n \nxr.apply_ufunc(func, da, input_core_dims=[[&quot;first_dim&quot;]])\n \n# (3, 4, 2)\n\n\n                  \n                  Todo\n                  \n                \n\nUnderstand the use of core dimensions in case of vectorise=True\n\n\nReferences\n\ntutorial.xarray.dev/advanced/apply_ufunc/core-dimensions.html\n"},"notes/tech/xarray/loop-dimensions":{"title":"loop-dimensions","links":["notes/tech/xarray/core-dimensions","notes/tech/xarray/apply_ufunc"],"tags":["python","data","libraries"],"content":"Loop Dimensions\nFunctions that performs operations on data with core-dimensions and taking axis as argument, can be easily used with apply_ufunc.\nHowever, functions which donâ€™t take axis argument and need to work on core-dimensions need loop dimensions. Loop dimension is something where the loop happens and function gets the sub-array as input.\nFor example, a 2d data array with dimensions x and y and square needs to work on only y, we need to loop over x so that function can get only 1d array on dimension y.\ndef square(item):\n    return item ** 2\n    \nxr.apply_ufunc(square,\n               xda,\n               input_core_dims=[[&quot;y&quot;]], \n               output_core_dims=[[&quot;y&quot;]], \n               vectorize=True)\nsquare gets 1d array of dimension y. input_core_dims provides the core dimensions function will take and dimensions which are not provided will be taken as loop dimensions (which is x) here.\nReferences\n\ntutorial.xarray.dev/advanced/apply_ufunc/automatic-vectorizing-numpy.html#core-dimensions-and-looping\n\n"},"notes/tech/xarray/map_blocks":{"title":"map_blocks","links":[],"tags":["python","libraries","parallelism"],"content":"xarray.map_blocks\nIt applies custom function on each chunk block of the DataArray or Dataset.\ndef add_wrapper(d):\n    print(&#039;------------------- START ---------------------&#039;)\n    print(type(d))\n    print(d.shape)\n    print(d)\n    print(&#039;-------------------- END ------------------------&#039;)\n    return d + 1\n \nxda = ds.air[:2, :5, :5].chunk({&quot;time&quot;: 1})\n \n xr.map_blocks(add_wrapper, ds.air).compute()\nOutput\n------------------- START ---------------------\n&lt;class &#039;xarray.core.dataarray.DataArray&#039;&gt;\n(0, 0, 0)\n&lt;xarray.DataArray &#039;air&#039; (time: 0, lat: 0, lon: 0)&gt; Size: 0B\narray([], shape=(0, 0, 0), dtype=float64)\n-------------------- END ------------------------\n------------------- START ---------------------\n&lt;class &#039;xarray.core.dataarray.DataArray&#039;&gt;\n(1, 5, 5)\n&lt;xarray.DataArray &#039;air&#039; (time: 1, lat: 5, lon: 5)&gt; Size: 200B\narray([[[241.2 , 242.5 , 243.5 , 244.  , 244.1 ],\n        [243.8 , 244.5 , 244.7 , 244.2 , 243.39],\n        [250.  , 249.8 , 248.89, 247.5 , 246.  ],\n        [266.5 , 267.1 , 267.1 , 266.7 , 265.9 ],\n        [274.5 , 274.29, 274.1 , 274.  , 273.79]]])\n-------------------- END ------------------------\n------------------- START ---------------------\n&lt;class &#039;xarray.core.dataarray.DataArray&#039;&gt;\n(1, 5, 5)\n&lt;xarray.DataArray &#039;air&#039; (time: 1, lat: 5, lon: 5)&gt; Size: 200B\narray([[[242.1 , 242.7 , 243.1 , 243.39, 243.6 ],\n        [243.6 , 244.1 , 244.2 , 244.1 , 243.7 ],\n        [253.2 , 252.89, 252.1 , 250.8 , 249.3 ],\n        [269.7 , 269.4 , 268.6 , 267.4 , 266.  ],\n        [272.5 , 271.5 , 270.4 , 269.4 , 268.5 ]]])\n-------------------- END ------------------------\nOut[40]:\n&lt;xarray.DataArray &#039;air&#039; (time: 2, lat: 5, lon: 5)&gt; Size: 400B\narray([[[242.2 , 243.5 , 244.5 , 245.  , 245.1 ],\n        [244.8 , 245.5 , 245.7 , 245.2 , 244.39],\n        [251.  , 250.8 , 249.89, 248.5 , 247.  ],\n        [267.5 , 268.1 , 268.1 , 267.7 , 266.9 ],\n        [275.5 , 275.29, 275.1 , 275.  , 274.79]],\n\n       [[243.1 , 243.7 , 244.1 , 244.39, 244.6 ],\n        [244.6 , 245.1 , 245.2 , 245.1 , 244.7 ],\n        [254.2 , 253.89, 253.1 , 251.8 , 250.3 ],\n        [270.7 , 270.4 , 269.6 , 268.4 , 267.  ],\n        [273.5 , 272.5 , 271.4 , 270.4 , 269.5 ]]])\n\nObservations\n\nFunction add_wrapper receives loaded DataArray.\nmap_blocks calls function first time with random data for output type inferencing. If it is not able to determine the type, we need explicitly specify type using template kwarg.\nSo in the above example, the function is called three times.\n\nOne for inference\nFirst chunk\nSecond chunk\n\n\n\n"},"notes/tech/xarray/xarray-so-far":{"title":"xarray-so-far","links":[],"tags":["libraries","python"],"content":"\nxarray is made on top of numpy by adding more information about the nature of data.\nadding lables to the dimensions. Instead of just raw dimension, xarray added name to them such as time, longitude, etc.\n\n"},"notes/tech/xarray/xarray-with-dask":{"title":"xarray-with-dask","links":[],"tags":["libraries","python","data-engineering"],"content":"Xarray with Dask\nXarray is numpy array with metadata and accessing the array with metadata.\nXarray does the computations in memory so has the limitation on data size.\nDask enables xarray to compute with big data with in disk computation like spark.\nDask makes the computation parallel using chunking method.\nXarray fits well with numpy as data array and dask array. Underling array can be a numpy or a dask.\nd = xr.DataArray(np.arange(10)) # numpy array\n \nb = xr.open_dataset(&quot;file.nc&quot;, chunks={&quot;dim_0&quot;: 1}) # dask array\nopen_dataset method reads netCDF files.\nReferences\n\nstephanhoyer.com/2015/06/11/xray-dask-out-of-core-labeled-arrays/\n"}}